# === NAVMAP v1 ===
# {
#   "module": "DocsToKG.DocParsing.core",
#   "purpose": "Shared utilities and CLI glue for DocParsing workflows",
#   "sections": [
#     {
#       "id": "dedupe-preserve-order",
#       "name": "dedupe_preserve_order",
#       "anchor": "function-dedupe-preserve-order",
#       "kind": "function"
#     },
#     {
#       "id": "ensure-str-sequence",
#       "name": "_ensure_str_sequence",
#       "anchor": "function-ensure-str-sequence",
#       "kind": "function"
#     },
#     {
#       "id": "load-yaml-markers",
#       "name": "_load_yaml_markers",
#       "anchor": "function-load-yaml-markers",
#       "kind": "function"
#     },
#     {
#       "id": "load-toml-markers",
#       "name": "_load_toml_markers",
#       "anchor": "function-load-toml-markers",
#       "kind": "function"
#     },
#     {
#       "id": "load-structural-marker-profile",
#       "name": "load_structural_marker_profile",
#       "anchor": "function-load-structural-marker-profile",
#       "kind": "function"
#     },
#     {
#       "id": "load-structural-marker-config",
#       "name": "load_structural_marker_config",
#       "anchor": "function-load-structural-marker-config",
#       "kind": "function"
#     },
#     {
#       "id": "clioption",
#       "name": "CLIOption",
#       "anchor": "class-clioption",
#       "kind": "class"
#     },
#     {
#       "id": "build-subcommand",
#       "name": "build_subcommand",
#       "anchor": "function-build-subcommand",
#       "kind": "function"
#     },
#     {
#       "id": "coerce-path",
#       "name": "_coerce_path",
#       "anchor": "function-coerce-path",
#       "kind": "function"
#     },
#     {
#       "id": "coerce-optional-path",
#       "name": "_coerce_optional_path",
#       "anchor": "function-coerce-optional-path",
#       "kind": "function"
#     },
#     {
#       "id": "coerce-bool",
#       "name": "_coerce_bool",
#       "anchor": "function-coerce-bool",
#       "kind": "function"
#     },
#     {
#       "id": "coerce-int",
#       "name": "_coerce_int",
#       "anchor": "function-coerce-int",
#       "kind": "function"
#     },
#     {
#       "id": "coerce-float",
#       "name": "_coerce_float",
#       "anchor": "function-coerce-float",
#       "kind": "function"
#     },
#     {
#       "id": "coerce-str",
#       "name": "_coerce_str",
#       "anchor": "function-coerce-str",
#       "kind": "function"
#     },
#     {
#       "id": "coerce-str-tuple",
#       "name": "_coerce_str_tuple",
#       "anchor": "function-coerce-str-tuple",
#       "kind": "function"
#     },
#     {
#       "id": "normalize-http-timeout",
#       "name": "normalize_http_timeout",
#       "anchor": "function-normalize-http-timeout",
#       "kind": "function"
#     },
#     {
#       "id": "get-http-session",
#       "name": "get_http_session",
#       "anchor": "function-get-http-session",
#       "kind": "function"
#     },
#     {
#       "id": "manifest-value",
#       "name": "_manifest_value",
#       "anchor": "function-manifest-value",
#       "kind": "function"
#     },
#     {
#       "id": "load-config-mapping",
#       "name": "load_config_mapping",
#       "anchor": "function-load-config-mapping",
#       "kind": "function"
#     },
#     {
#       "id": "stageconfigbase",
#       "name": "StageConfigBase",
#       "anchor": "class-stageconfigbase",
#       "kind": "class"
#     },
#     {
#       "id": "bm25stats",
#       "name": "BM25Stats",
#       "anchor": "class-bm25stats",
#       "kind": "class"
#     },
#     {
#       "id": "spladecfg",
#       "name": "SpladeCfg",
#       "anchor": "class-spladecfg",
#       "kind": "class"
#     },
#     {
#       "id": "qwencfg",
#       "name": "QwenCfg",
#       "anchor": "class-qwencfg",
#       "kind": "class"
#     },
#     {
#       "id": "chunkworkerconfig",
#       "name": "ChunkWorkerConfig",
#       "anchor": "class-chunkworkerconfig",
#       "kind": "class"
#     },
#     {
#       "id": "chunktask",
#       "name": "ChunkTask",
#       "anchor": "class-chunktask",
#       "kind": "class"
#     },
#     {
#       "id": "chunkresult",
#       "name": "ChunkResult",
#       "anchor": "class-chunkresult",
#       "kind": "class"
#     },
#     {
#       "id": "expand-path",
#       "name": "expand_path",
#       "anchor": "function-expand-path",
#       "kind": "function"
#     },
#     {
#       "id": "resolve-hf-home",
#       "name": "resolve_hf_home",
#       "anchor": "function-resolve-hf-home",
#       "kind": "function"
#     },
#     {
#       "id": "resolve-model-root",
#       "name": "resolve_model_root",
#       "anchor": "function-resolve-model-root",
#       "kind": "function"
#     },
#     {
#       "id": "looks-like-filesystem-path",
#       "name": "looks_like_filesystem_path",
#       "anchor": "function-looks-like-filesystem-path",
#       "kind": "function"
#     },
#     {
#       "id": "resolve-pdf-model-path",
#       "name": "resolve_pdf_model_path",
#       "anchor": "function-resolve-pdf-model-path",
#       "kind": "function"
#     },
#     {
#       "id": "init-hf-env",
#       "name": "init_hf_env",
#       "anchor": "function-init-hf-env",
#       "kind": "function"
#     },
#     {
#       "id": "detect-cuda-device",
#       "name": "_detect_cuda_device",
#       "anchor": "function-detect-cuda-device",
#       "kind": "function"
#     },
#     {
#       "id": "ensure-model-environment",
#       "name": "ensure_model_environment",
#       "anchor": "function-ensure-model-environment",
#       "kind": "function"
#     },
#     {
#       "id": "ensure-optional-dependency",
#       "name": "_ensure_optional_dependency",
#       "anchor": "function-ensure-optional-dependency",
#       "kind": "function"
#     },
#     {
#       "id": "ensure-splade-dependencies",
#       "name": "ensure_splade_dependencies",
#       "anchor": "function-ensure-splade-dependencies",
#       "kind": "function"
#     },
#     {
#       "id": "ensure-qwen-dependencies",
#       "name": "ensure_qwen_dependencies",
#       "anchor": "function-ensure-qwen-dependencies",
#       "kind": "function"
#     },
#     {
#       "id": "ensure-splade-environment",
#       "name": "ensure_splade_environment",
#       "anchor": "function-ensure-splade-environment",
#       "kind": "function"
#     },
#     {
#       "id": "ensure-qwen-environment",
#       "name": "ensure_qwen_environment",
#       "anchor": "function-ensure-qwen-environment",
#       "kind": "function"
#     },
#     {
#       "id": "detect-data-root",
#       "name": "detect_data_root",
#       "anchor": "function-detect-data-root",
#       "kind": "function"
#     },
#     {
#       "id": "ensure-dir",
#       "name": "_ensure_dir",
#       "anchor": "function-ensure-dir",
#       "kind": "function"
#     },
#     {
#       "id": "data-doctags",
#       "name": "data_doctags",
#       "anchor": "function-data-doctags",
#       "kind": "function"
#     },
#     {
#       "id": "data-chunks",
#       "name": "data_chunks",
#       "anchor": "function-data-chunks",
#       "kind": "function"
#     },
#     {
#       "id": "data-vectors",
#       "name": "data_vectors",
#       "anchor": "function-data-vectors",
#       "kind": "function"
#     },
#     {
#       "id": "data-manifests",
#       "name": "data_manifests",
#       "anchor": "function-data-manifests",
#       "kind": "function"
#     },
#     {
#       "id": "prepare-data-root",
#       "name": "prepare_data_root",
#       "anchor": "function-prepare-data-root",
#       "kind": "function"
#     },
#     {
#       "id": "resolve-pipeline-path",
#       "name": "resolve_pipeline_path",
#       "anchor": "function-resolve-pipeline-path",
#       "kind": "function"
#     },
#     {
#       "id": "data-pdfs",
#       "name": "data_pdfs",
#       "anchor": "function-data-pdfs",
#       "kind": "function"
#     },
#     {
#       "id": "data-html",
#       "name": "data_html",
#       "anchor": "function-data-html",
#       "kind": "function"
#     },
#     {
#       "id": "derive-doc-id-and-doctags-path",
#       "name": "derive_doc_id_and_doctags_path",
#       "anchor": "function-derive-doc-id-and-doctags-path",
#       "kind": "function"
#     },
#     {
#       "id": "derive-doc-id-and-chunks-path",
#       "name": "derive_doc_id_and_chunks_path",
#       "anchor": "function-derive-doc-id-and-chunks-path",
#       "kind": "function"
#     },
#     {
#       "id": "derive-doc-id-and-vectors-path",
#       "name": "derive_doc_id_and_vectors_path",
#       "anchor": "function-derive-doc-id-and-vectors-path",
#       "kind": "function"
#     },
#     {
#       "id": "compute-relative-doc-id",
#       "name": "compute_relative_doc_id",
#       "anchor": "function-compute-relative-doc-id",
#       "kind": "function"
#     },
#     {
#       "id": "compute-stable-shard",
#       "name": "compute_stable_shard",
#       "anchor": "function-compute-stable-shard",
#       "kind": "function"
#     },
#     {
#       "id": "should-skip-output",
#       "name": "should_skip_output",
#       "anchor": "function-should-skip-output",
#       "kind": "function"
#     },
#     {
#       "id": "resumecontroller",
#       "name": "ResumeController",
#       "anchor": "class-resumecontroller",
#       "kind": "class"
#     },
#     {
#       "id": "stringify-path",
#       "name": "_stringify_path",
#       "anchor": "function-stringify-path",
#       "kind": "function"
#     },
#     {
#       "id": "manifest-log-skip",
#       "name": "manifest_log_skip",
#       "anchor": "function-manifest-log-skip",
#       "kind": "function"
#     },
#     {
#       "id": "manifest-log-success",
#       "name": "manifest_log_success",
#       "anchor": "function-manifest-log-success",
#       "kind": "function"
#     },
#     {
#       "id": "manifest-log-failure",
#       "name": "manifest_log_failure",
#       "anchor": "function-manifest-log-failure",
#       "kind": "function"
#     },
#     {
#       "id": "structuredlogger",
#       "name": "StructuredLogger",
#       "anchor": "class-structuredlogger",
#       "kind": "class"
#     },
#     {
#       "id": "get-logger",
#       "name": "get_logger",
#       "anchor": "function-get-logger",
#       "kind": "function"
#     },
#     {
#       "id": "log-event",
#       "name": "log_event",
#       "anchor": "function-log-event",
#       "kind": "function"
#     },
#     {
#       "id": "find-free-port",
#       "name": "find_free_port",
#       "anchor": "function-find-free-port",
#       "kind": "function"
#     },
#     {
#       "id": "atomic-write",
#       "name": "atomic_write",
#       "anchor": "function-atomic-write",
#       "kind": "function"
#     },
#     {
#       "id": "iter-doctags",
#       "name": "iter_doctags",
#       "anchor": "function-iter-doctags",
#       "kind": "function"
#     },
#     {
#       "id": "iter-chunks",
#       "name": "iter_chunks",
#       "anchor": "function-iter-chunks",
#       "kind": "function"
#     },
#     {
#       "id": "jsonl-load",
#       "name": "jsonl_load",
#       "anchor": "function-jsonl-load",
#       "kind": "function"
#     },
#     {
#       "id": "jsonl-save",
#       "name": "jsonl_save",
#       "anchor": "function-jsonl-save",
#       "kind": "function"
#     },
#     {
#       "id": "jsonl-append-iter",
#       "name": "jsonl_append_iter",
#       "anchor": "function-jsonl-append-iter",
#       "kind": "function"
#     },
#     {
#       "id": "build-jsonl-split-map",
#       "name": "build_jsonl_split_map",
#       "anchor": "function-build-jsonl-split-map",
#       "kind": "function"
#     },
#     {
#       "id": "iter-jsonl-records",
#       "name": "_iter_jsonl_records",
#       "anchor": "function-iter-jsonl-records",
#       "kind": "function"
#     },
#     {
#       "id": "batcher",
#       "name": "Batcher",
#       "anchor": "class-batcher",
#       "kind": "class"
#     },
#     {
#       "id": "manifest-filename",
#       "name": "_manifest_filename",
#       "anchor": "function-manifest-filename",
#       "kind": "function"
#     },
#     {
#       "id": "manifest-append",
#       "name": "manifest_append",
#       "anchor": "function-manifest-append",
#       "kind": "function"
#     },
#     {
#       "id": "resolve-hash-algorithm",
#       "name": "resolve_hash_algorithm",
#       "anchor": "function-resolve-hash-algorithm",
#       "kind": "function"
#     },
#     {
#       "id": "compute-chunk-uuid",
#       "name": "compute_chunk_uuid",
#       "anchor": "function-compute-chunk-uuid",
#       "kind": "function"
#     },
#     {
#       "id": "relative-path",
#       "name": "relative_path",
#       "anchor": "function-relative-path",
#       "kind": "function"
#     },
#     {
#       "id": "quarantine-artifact",
#       "name": "quarantine_artifact",
#       "anchor": "function-quarantine-artifact",
#       "kind": "function"
#     },
#     {
#       "id": "compute-content-hash",
#       "name": "compute_content_hash",
#       "anchor": "function-compute-content-hash",
#       "kind": "function"
#     },
#     {
#       "id": "load-manifest-index",
#       "name": "load_manifest_index",
#       "anchor": "function-load-manifest-index",
#       "kind": "function"
#     },
#     {
#       "id": "iter-manifest-entries",
#       "name": "iter_manifest_entries",
#       "anchor": "function-iter-manifest-entries",
#       "kind": "function"
#     },
#     {
#       "id": "summarize-manifest",
#       "name": "summarize_manifest",
#       "anchor": "function-summarize-manifest",
#       "kind": "function"
#     },
#     {
#       "id": "acquire-lock",
#       "name": "acquire_lock",
#       "anchor": "function-acquire-lock",
#       "kind": "function"
#     },
#     {
#       "id": "pid-is-running",
#       "name": "_pid_is_running",
#       "anchor": "function-pid-is-running",
#       "kind": "function"
#     },
#     {
#       "id": "set-spawn-or-warn",
#       "name": "set_spawn_or_warn",
#       "anchor": "function-set-spawn-or-warn",
#       "kind": "function"
#     },
#     {
#       "id": "run-chunk",
#       "name": "_run_chunk",
#       "anchor": "function-run-chunk",
#       "kind": "function"
#     },
#     {
#       "id": "run-embed",
#       "name": "_run_embed",
#       "anchor": "function-run-embed",
#       "kind": "function"
#     },
#     {
#       "id": "run-token-profiles",
#       "name": "_run_token_profiles",
#       "anchor": "function-run-token-profiles",
#       "kind": "function"
#     },
#     {
#       "id": "run-plan",
#       "name": "_run_plan",
#       "anchor": "function-run-plan",
#       "kind": "function"
#     },
#     {
#       "id": "run-manifest",
#       "name": "_run_manifest",
#       "anchor": "function-run-manifest",
#       "kind": "function"
#     },
#     {
#       "id": "build-doctags-parser",
#       "name": "_build_doctags_parser",
#       "anchor": "function-build-doctags-parser",
#       "kind": "function"
#     },
#     {
#       "id": "scan-pdf-html",
#       "name": "_scan_pdf_html",
#       "anchor": "function-scan-pdf-html",
#       "kind": "function"
#     },
#     {
#       "id": "directory-contains-suffixes",
#       "name": "_directory_contains_suffixes",
#       "anchor": "function-directory-contains-suffixes",
#       "kind": "function"
#     },
#     {
#       "id": "detect-mode",
#       "name": "_detect_mode",
#       "anchor": "function-detect-mode",
#       "kind": "function"
#     },
#     {
#       "id": "merge-args",
#       "name": "_merge_args",
#       "anchor": "function-merge-args",
#       "kind": "function"
#     },
#     {
#       "id": "run-doctags",
#       "name": "_run_doctags",
#       "anchor": "function-run-doctags",
#       "kind": "function"
#     },
#     {
#       "id": "preview-list",
#       "name": "_preview_list",
#       "anchor": "function-preview-list",
#       "kind": "function"
#     },
#     {
#       "id": "plan-doctags",
#       "name": "_plan_doctags",
#       "anchor": "function-plan-doctags",
#       "kind": "function"
#     },
#     {
#       "id": "plan-chunk",
#       "name": "_plan_chunk",
#       "anchor": "function-plan-chunk",
#       "kind": "function"
#     },
#     {
#       "id": "plan-embed",
#       "name": "_plan_embed",
#       "anchor": "function-plan-embed",
#       "kind": "function"
#     },
#     {
#       "id": "display-plan",
#       "name": "_display_plan",
#       "anchor": "function-display-plan",
#       "kind": "function"
#     },
#     {
#       "id": "run-all",
#       "name": "_run_all",
#       "anchor": "function-run-all",
#       "kind": "function"
#     },
#     {
#       "id": "command",
#       "name": "_Command",
#       "anchor": "class-command",
#       "kind": "class"
#     },
#     {
#       "id": "main",
#       "name": "main",
#       "anchor": "function-main",
#       "kind": "function"
#     },
#     {
#       "id": "run-all",
#       "name": "run_all",
#       "anchor": "function-run-all",
#       "kind": "function"
#     },
#     {
#       "id": "chunk",
#       "name": "chunk",
#       "anchor": "function-chunk",
#       "kind": "function"
#     },
#     {
#       "id": "embed",
#       "name": "embed",
#       "anchor": "function-embed",
#       "kind": "function"
#     },
#     {
#       "id": "doctags",
#       "name": "doctags",
#       "anchor": "function-doctags",
#       "kind": "function"
#     },
#     {
#       "id": "token-profiles",
#       "name": "token_profiles",
#       "anchor": "function-token-profiles",
#       "kind": "function"
#     },
#     {
#       "id": "plan",
#       "name": "plan",
#       "anchor": "function-plan",
#       "kind": "function"
#     },
#     {
#       "id": "manifest",
#       "name": "manifest",
#       "anchor": "function-manifest",
#       "kind": "function"
#     }
#   ]
# }
# === /NAVMAP ===

"""
DocParsing Core Utilities

This module centralises lightweight helpers that power multiple DocParsing
pipeline stages. Utilities span path discovery, atomic file writes, JSONL
parsing, manifest bookkeeping, CLI glue, and structured logging so that
chunking, embedding, and conversion scripts can share consistent behaviour
without an additional dependency layer.

Key Features:
- Resolve DocsToKG data directories with environment and ancestor discovery
- Stream JSONL inputs and outputs with validation and error tolerance
- Emit structured JSON logs suited for machine ingestion and dashboards
- Manage pipeline manifests, batching helpers, and advisory file locks

Usage:
    from DocsToKG.DocParsing import core

    chunks_dir = core.data_chunks()
    with core.atomic_write(chunks_dir / \"example.jsonl\") as handle:
        handle.write(\"{}\")

Dependencies:
- json, pathlib, logging: Provide standard I/O and diagnostics primitives.
- typing: Supply type hints consumed by Sphinx documentation tooling and API generators.
- pydantic (optional): Some helpers integrate with schema validation routines.

All helpers are safe to import in multiprocessing contexts and avoid heavy
third-party dependencies beyond the standard library.
"""

from __future__ import annotations

import argparse
import contextlib
import hashlib
import json
import logging
import math
import os
import re
import socket
import threading
import time
import uuid
from dataclasses import dataclass
from pathlib import Path
from typing import (
    Any,
    Callable,
    Dict,
    Iterable,
    Iterator,
    List,
    Mapping,
    Optional,
    Sequence,
    Tuple,
    TypeVar,
)

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from ..OntologyDownload.logging_utils import JSONFormatter
from .config import (
    StageConfigBase,
    _load_toml_markers,
    _load_yaml_markers,
    load_config_mapping,
)
from .env import (
    PDF_MODEL_SUBDIR,
    data_chunks as _data_chunks,
    data_doctags as _data_doctags,
    data_html as _data_html,
    data_manifests as _data_manifests,
    data_pdfs as _data_pdfs,
    data_vectors as _data_vectors,
    detect_data_root as _detect_data_root,
    ensure_model_environment as _ensure_model_environment,
    ensure_qwen_dependencies as _ensure_qwen_dependencies,
    ensure_qwen_environment as _ensure_qwen_environment,
    ensure_splade_dependencies as _ensure_splade_dependencies,
    ensure_splade_environment as _ensure_splade_environment,
    expand_path as _expand_path,
    init_hf_env as _init_hf_env,
    looks_like_filesystem_path as _looks_like_fs_path,
    prepare_data_root as _prepare_data_root,
    resolve_hf_home as _resolve_hf_home,
    resolve_model_root as _resolve_model_root,
    resolve_pdf_model_path as _resolve_pdf_model_path,
    resolve_pipeline_path as _resolve_pipeline_path,
)
from .io import (
    atomic_write as _atomic_write,
    build_jsonl_split_map as _build_jsonl_split_map,
    compute_chunk_uuid as _compute_chunk_uuid,
    compute_content_hash as _compute_content_hash,
    dedupe_preserve_order as _dedupe_preserve_order,
    make_hasher as _make_hasher,
    iter_doctags as _iter_doctags,
    iter_jsonl as _iter_jsonl,
    iter_jsonl_batches as _iter_jsonl_batches,
    iter_manifest_entries as _iter_manifest_entries,
    jsonl_append_iter as _jsonl_append_iter,
    jsonl_load as _jsonl_load,
    jsonl_save as _jsonl_save,
    load_manifest_index as _load_manifest_index,
    manifest_append as _manifest_append,
    quarantine_artifact as _quarantine_artifact,
    relative_path as _relative_path,
    resolve_hash_algorithm as _resolve_hash_algorithm,
)
from .logging import (
    StructuredLogger as _StructuredLogger,
    get_logger as _get_logger,
    log_event as _log_event,
    manifest_log_failure as _manifest_log_failure,
    manifest_log_skip as _manifest_log_skip,
    manifest_log_success as _manifest_log_success,
    summarize_manifest as _summarize_manifest,
)

T = TypeVar("T")

# Re-export helpers from focused modules while keeping legacy import paths.
atomic_write = _atomic_write
build_jsonl_split_map = _build_jsonl_split_map
compute_chunk_uuid = _compute_chunk_uuid
compute_content_hash = _compute_content_hash
make_hasher = _make_hasher
iter_manifest_entries = _iter_manifest_entries
iter_doctags = _iter_doctags
jsonl_append_iter = _jsonl_append_iter
iter_jsonl = _iter_jsonl
iter_jsonl_batches = _iter_jsonl_batches
jsonl_load = _jsonl_load
jsonl_save = _jsonl_save
dedupe_preserve_order = _dedupe_preserve_order
load_manifest_index = _load_manifest_index
manifest_append = _manifest_append
quarantine_artifact = _quarantine_artifact
relative_path = _relative_path
resolve_hash_algorithm = _resolve_hash_algorithm

data_chunks = _data_chunks
data_doctags = _data_doctags
data_html = _data_html
data_manifests = _data_manifests
data_pdfs = _data_pdfs
data_vectors = _data_vectors
detect_data_root = _detect_data_root
ensure_model_environment = _ensure_model_environment
ensure_qwen_dependencies = _ensure_qwen_dependencies
ensure_qwen_environment = _ensure_qwen_environment
ensure_splade_dependencies = _ensure_splade_dependencies
ensure_splade_environment = _ensure_splade_environment
expand_path = _expand_path
init_hf_env = _init_hf_env
looks_like_filesystem_path = _looks_like_fs_path
prepare_data_root = _prepare_data_root
resolve_hf_home = _resolve_hf_home
resolve_model_root = _resolve_model_root
resolve_pdf_model_path = _resolve_pdf_model_path
resolve_pipeline_path = _resolve_pipeline_path

StructuredLogger = _StructuredLogger
get_logger = _get_logger
log_event = _log_event
manifest_log_failure = _manifest_log_failure
manifest_log_skip = _manifest_log_skip
manifest_log_success = _manifest_log_success
summarize_manifest = _summarize_manifest

# Default structural marker configuration shared across stages.
DEFAULT_HEADING_MARKERS: Tuple[str, ...] = ("#",)
DEFAULT_CAPTION_MARKERS: Tuple[str, ...] = (
    "Figure caption:",
    "Table:",
    "Picture description:",
    "<!-- image -->",
)

DEFAULT_HTTP_TIMEOUT: Tuple[float, float] = (5.0, 30.0)

_HTTP_SESSION_LOCK = threading.Lock()
_HTTP_SESSION: Optional[requests.Session] = None
_HTTP_SESSION_TIMEOUT: Tuple[float, float] = DEFAULT_HTTP_TIMEOUT


def _ensure_str_sequence(value: object, label: str) -> List[str]:
    """Normalise structural marker entries into string lists."""

    if value is None:
        return []
    if isinstance(value, str):
        value = [value]
    if not isinstance(value, list) or not all(isinstance(item, str) for item in value):
        raise ValueError(f"Expected a list of strings for '{label}'")
    return [item for item in value if item]


def load_structural_marker_profile(path: Path) -> Tuple[List[str], List[str]]:
    """Load heading/caption marker overrides from JSON, YAML, or TOML files."""

    raw = path.read_text(encoding="utf-8")
    suffix = path.suffix.lower()

    parsers: List[str] = []
    if suffix in {".yaml", ".yml"}:
        parsers = ["yaml"]
    elif suffix == ".toml":
        parsers = ["toml"]
    elif suffix == ".json":
        parsers = ["json"]
    else:
        parsers = ["json", "yaml", "toml"]

    data: object = None
    last_error: Exception | None = None

    for parser in parsers:
        try:
            if parser == "json":
                data = json.loads(raw)
            elif parser == "yaml":
                data = _load_yaml_markers(raw)
            elif parser == "toml":
                data = _load_toml_markers(raw)
            else:  # pragma: no cover - defensive branch
                continue
        except (ValueError, json.JSONDecodeError) as exc:
            last_error = exc
            data = None
            continue
        except Exception as exc:
            last_error = exc
            data = None
            continue

        if data is not None:
            break

    if data is None:
        messages = ", ".join(parsers)
        detail = f"; last error: {last_error}" if last_error else ""
        raise ValueError(
            f"Unable to parse structural marker file {path} using supported formats ({messages}){detail}."
        )

    if isinstance(data, list):
        headings = _ensure_str_sequence(data, "headings")
        captions: List[str] = []
    elif isinstance(data, dict):
        headings = _ensure_str_sequence(data.get("headings"), "headings")
        captions = _ensure_str_sequence(data.get("captions"), "captions")
    else:
        raise ValueError(f"Unsupported structural marker format in {path}")

    return headings, captions


def load_structural_marker_config(path: Path) -> Tuple[List[str], List[str]]:
    """Backward compatible alias for :func:`load_structural_marker_profile`."""

    return load_structural_marker_profile(path)


@dataclass(frozen=True)
class CLIOption:
    """Declarative CLI argument specification used by ``build_subcommand``."""

    flags: Tuple[str, ...]
    kwargs: Dict[str, Any]


def build_subcommand(
    parser: argparse.ArgumentParser, options: Sequence[CLIOption]
) -> argparse.ArgumentParser:
    """Attach CLI options described by ``options`` to ``parser``."""

    for option in options:
        parser.add_argument(*option.flags, **option.kwargs)
    return parser


def normalize_http_timeout(timeout: Optional[object]) -> Tuple[float, float]:
    """Normalize timeout inputs into a ``(connect, read)`` tuple of floats."""

    if timeout is None:
        return DEFAULT_HTTP_TIMEOUT

    def _coerce_pair(values: Sequence[object]) -> Tuple[float, float]:
        extracted = [v for v in values if v is not None]
        if not extracted:
            return DEFAULT_HTTP_TIMEOUT
        if len(extracted) == 1:
            return float(DEFAULT_HTTP_TIMEOUT[0]), float(extracted[0])
        return float(extracted[0]), float(extracted[1])

    if isinstance(timeout, (int, float)):
        return float(DEFAULT_HTTP_TIMEOUT[0]), float(timeout)

    if isinstance(timeout, str):
        parts = [part for part in re.split(r"[;,\\s]+", timeout) if part]
        if not parts:
            return DEFAULT_HTTP_TIMEOUT
        return _coerce_pair(parts)

    if isinstance(timeout, (list, tuple, set)):
        return _coerce_pair(list(timeout))

    if hasattr(timeout, "__iter__"):
        try:
            return _coerce_pair(list(timeout))  # type: ignore[arg-type]
        except TypeError as exc:  # pragma: no cover - defensive
            raise ValueError("Unable to interpret HTTP timeout iterable") from exc

    raise TypeError(f"Unsupported timeout type: {type(timeout)!r}")


def get_http_session(
    *,
    timeout: Optional[object] = None,
    base_headers: Optional[Mapping[str, str]] = None,
    retry_total: int = 5,
    retry_backoff: float = 0.5,
    status_forcelist: Sequence[int] = (429, 500, 502, 503, 504),
    allowed_methods: Sequence[str] = ("GET", "HEAD", "POST", "PUT", "DELETE", "OPTIONS", "PATCH"),
) -> Tuple[requests.Session, Tuple[float, float]]:
    """Return a shared :class:`requests.Session` configured with retries.

    Args:
        timeout: Optional override for the ``(connect, read)`` timeout tuple. Scalars
            override only the read timeout while preserving the default connect value.
        base_headers: Headers merged into the shared session.
        retry_total: Maximum retry attempts applied for connect/read failures.
        retry_backoff: Exponential backoff factor between retries.
        status_forcelist: HTTP status codes that trigger retries.
        allowed_methods: HTTP verbs eligible for retries.

    Returns:
        Tuple containing the shared session and the effective timeout tuple.
    """

    effective_timeout = normalize_http_timeout(timeout)

    with _HTTP_SESSION_LOCK:
        global _HTTP_SESSION, _HTTP_SESSION_TIMEOUT
        if _HTTP_SESSION is None:
            session = requests.Session()
            retry = Retry(
                total=retry_total,
                read=retry_total,
                connect=retry_total,
                backoff_factor=retry_backoff,
                status_forcelist=tuple(int(code) for code in status_forcelist),
                allowed_methods=frozenset(method.upper() for method in allowed_methods),
                raise_on_status=False,
            )
            adapter = HTTPAdapter(max_retries=retry)
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            _HTTP_SESSION = session
            logging.getLogger(__name__).debug(
                "Created shared HTTP session",
                extra={
                    "extra_fields": {
                        "retry_total": retry_total,
                        "status_forcelist": list(status_forcelist),
                        "allowed_methods": [method.upper() for method in allowed_methods],
                    }
                },
            )

        if base_headers:
            _HTTP_SESSION.headers.update(
                {key: value for key, value in base_headers.items() if value is not None}
            )

        _HTTP_SESSION_TIMEOUT = effective_timeout
        return _HTTP_SESSION, _HTTP_SESSION_TIMEOUT


# --- Globals ---

__all__ = [
    "detect_data_root",
    "data_doctags",
    "data_chunks",
    "data_vectors",
    "data_manifests",
    "data_pdfs",
    "data_html",
    "expand_path",
    "resolve_hf_home",
    "resolve_model_root",
    "find_free_port",
    "atomic_write",
    "iter_doctags",
    "iter_chunks",
    "jsonl_load",
    "jsonl_save",
    "jsonl_append_iter",
    "iter_jsonl",
    "iter_jsonl_batches",
    "build_jsonl_split_map",
    "get_logger",
    "log_event",
    "Batcher",
    "compute_chunk_uuid",
    "quarantine_artifact",
    "manifest_append",
    "manifest_log_failure",
    "manifest_log_skip",
    "manifest_log_success",
    "compute_content_hash",
    "make_hasher",
    "resolve_hash_algorithm",
    "load_manifest_index",
    "acquire_lock",
    "set_spawn_or_warn",
    "derive_doc_id_and_vectors_path",
    "derive_doc_id_and_doctags_path",
    "derive_doc_id_and_chunks_path",
    "compute_relative_doc_id",
    "compute_stable_shard",
    "should_skip_output",
    "relative_path",
    "init_hf_env",
    "ensure_model_environment",
    "ensure_splade_dependencies",
    "ensure_qwen_dependencies",
    "ensure_splade_environment",
    "ensure_qwen_environment",
    "DEFAULT_HTTP_TIMEOUT",
    "normalize_http_timeout",
    "get_http_session",
    "iter_manifest_entries",
    "summarize_manifest",
    "ResumeController",
    "UUID_NAMESPACE",
    "BM25Stats",
    "SpladeCfg",
    "QwenCfg",
    "ChunkWorkerConfig",
    "ChunkTask",
    "ChunkResult",
    "DEFAULT_HEADING_MARKERS",
    "DEFAULT_CAPTION_MARKERS",
    "DEFAULT_SERIALIZER_PROVIDER",
    "DEFAULT_TOKENIZER",
    "dedupe_preserve_order",
    "CLIOption",
    "build_subcommand",
    "looks_like_filesystem_path",
    "resolve_pdf_model_path",
    "prepare_data_root",
    "resolve_pipeline_path",
    "load_structural_marker_profile",
    "load_structural_marker_config",
    "CommandHandler",
    "CLI_DESCRIPTION",
    "main",
    "run_all",
    "chunk",
    "embed",
    "doctags",
    "token_profiles",
    "plan",
    "manifest",
]

# --- Data Containers ---

UUID_NAMESPACE = uuid.UUID("00000000-0000-0000-0000-000000000000")
DEFAULT_SERIALIZER_PROVIDER = "DocsToKG.DocParsing.formats:RichSerializerProvider"
DEFAULT_TOKENIZER = "Qwen/Qwen3-Embedding-4B"
PDF_MODEL_SUBDIR = Path("granite-docling-258M")


@dataclass(slots=True)
class BM25Stats:
    """Corpus-level statistics required for BM25 weighting."""

    N: int
    avgdl: float
    df: Dict[str, int]


@dataclass(slots=True)
class SpladeCfg:
    """Runtime configuration for SPLADE sparse encoding."""

    model_dir: Path
    device: str = "cuda"
    batch_size: int = 32
    cache_folder: Optional[Path] = None
    max_active_dims: Optional[int] = None
    attn_impl: Optional[str] = None
    local_files_only: bool = True


@dataclass(slots=True)
class QwenCfg:
    """Configuration for generating dense embeddings with Qwen via vLLM."""

    model_dir: Path
    dtype: str = "bfloat16"
    tp: int = 1
    gpu_mem_util: float = 0.60
    batch_size: int = 32
    quantization: Optional[str] = None
    dim: int = 2560
    cache_enabled: bool = True


@dataclass(slots=True)
class ChunkWorkerConfig:
    """Lightweight configuration shared across chunker worker processes."""

    tokenizer_model: str
    min_tokens: int
    max_tokens: int
    soft_barrier_margin: int
    heading_markers: Tuple[str, ...]
    caption_markers: Tuple[str, ...]
    docling_version: str
    serializer_provider_spec: str = DEFAULT_SERIALIZER_PROVIDER
    inject_anchors: bool = False


@dataclass(slots=True)
class ChunkTask:
    """Work unit describing a single DocTags file to chunk."""

    doc_path: Path
    output_path: Path
    doc_id: str
    doc_stem: str
    input_hash: str
    parse_engine: str
    sanitizer_profile: Optional[str] = None


@dataclass(slots=True)
class ChunkResult:
    """Result envelope emitted by chunker workers."""

    doc_id: str
    doc_stem: str
    status: str
    duration_s: float
    input_path: Path
    output_path: Path
    input_hash: str
    chunk_count: int
    parse_engine: str
    sanitizer_profile: Optional[str] = None
    anchors_injected: bool = False
    error: Optional[str] = None


def derive_doc_id_and_doctags_path(
    source_pdf: Path, pdfs_root: Path, doctags_root: Path
) -> tuple[str, Path]:
    """Return manifest doc identifier and DocTags output path for ``source_pdf``."""

    doc_id = compute_relative_doc_id(source_pdf, pdfs_root)
    relative = Path(doc_id)
    doctags_path = (doctags_root / relative).with_suffix(".doctags")
    return doc_id, doctags_path


def derive_doc_id_and_chunks_path(
    doctags_file: Path, doctags_root: Path, chunks_root: Path
) -> tuple[str, Path]:
    """Return manifest doc identifier and chunk output path for ``doctags_file``."""

    doc_id = compute_relative_doc_id(doctags_file, doctags_root)
    relative = Path(doc_id)
    chunk_path = (chunks_root / relative).with_suffix(".chunks.jsonl")
    return doc_id, chunk_path


def derive_doc_id_and_vectors_path(
    chunk_file: Path, chunks_root: Path, vectors_root: Path
) -> tuple[str, Path]:
    """Return manifest doc identifier and vectors output path for ``chunk_file``.

    Args:
        chunk_file: Path to the chunk JSONL artefact.
        chunks_root: Root directory containing chunk artefacts.
        vectors_root: Root directory where vector outputs should be written.

    Returns:
        Tuple containing the manifest ``doc_id`` and the full vectors output path.
    """

    relative = chunk_file.relative_to(chunks_root)
    base = relative
    if base.suffix == ".jsonl":
        base = base.with_suffix("")
    if base.suffix == ".chunks":
        base = base.with_suffix("")
    doc_id = base.with_suffix(".doctags").as_posix()
    vector_relative = base.with_suffix(".vectors.jsonl")
    return doc_id, vectors_root / vector_relative


def compute_relative_doc_id(path: Path, root: Path) -> str:
    """Return POSIX-style relative identifier for a document path.

    Args:
        path: Absolute path to the document on disk.
        root: Root directory that anchors relative identifiers.

    Returns:
        str: POSIX-style relative path suitable for manifest IDs.
    """

    return path.relative_to(root).as_posix()


def compute_stable_shard(identifier: str, shard_count: int) -> int:
    """Deterministically map ``identifier`` to a shard in ``[0, shard_count)``."""

    if shard_count < 1:
        raise ValueError("shard_count must be >= 1")
    digest = hashlib.sha256(identifier.encode("utf-8")).digest()
    return int.from_bytes(digest[:8], "big") % shard_count


def should_skip_output(
    output_path: Path,
    manifest_entry: Optional[Mapping[str, object]],
    input_hash: str,
    resume: bool,
    force: bool,
) -> bool:
    """Return ``True`` when resume/skip conditions indicate work can be skipped."""

    if not resume or force:
        return False
    if not output_path.exists():
        return False
    if not manifest_entry:
        return False
    stored_hash = manifest_entry.get("input_hash") if isinstance(manifest_entry, Mapping) else None
    return stored_hash == input_hash


@dataclass(slots=True)
class ResumeController:
    """Centralize resume/force decisions using manifest metadata."""

    resume: bool
    force: bool
    manifest_index: Optional[Mapping[str, Mapping[str, object]]] = None

    def entry(self, doc_id: str) -> Optional[Mapping[str, object]]:
        """Return the manifest entry associated with ``doc_id`` when available."""

        if not self.manifest_index:
            return None
        return self.manifest_index.get(doc_id)

    def should_skip(
        self, doc_id: str, output_path: Path, input_hash: str
    ) -> Tuple[bool, Optional[Mapping[str, object]]]:
        """Return ``True`` when work for ``doc_id`` can be safely skipped."""

        entry = self.entry(doc_id)
        skip = should_skip_output(output_path, entry, input_hash, self.resume, self.force)
        return skip, entry

    def should_process(
        self, doc_id: str, output_path: Path, input_hash: str
    ) -> Tuple[bool, Optional[Mapping[str, object]]]:
        """Return ``True`` when ``doc_id`` requires processing."""

        skip, entry = self.should_skip(doc_id, output_path, input_hash)
        return not skip, entry


def find_free_port(start: int = 8000, span: int = 32) -> int:
    """Locate an available TCP port on localhost within a range.

    Args:
        start: Starting port for the scan. Defaults to ``8000``.
        span: Number of sequential ports to check. Defaults to ``32``.

    Returns:
        The first free port number. Falls back to an OS-assigned ephemeral port
        if the requested range is exhausted.

    Examples:
        >>> port = find_free_port(8500, 1)
        >>> isinstance(port, int)
        True
    """

    for port in range(start, start + span):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
            sock.settimeout(0.2)
            if sock.connect_ex(("127.0.0.1", port)) != 0:
                return port

    logger = get_logger(__name__)
    log_event(
        logger,
        "warning",
        "Port scan exhausted",
        stage="core",
        doc_id="__system__",
        input_hash=None,
        error_code="PORT_SCAN_EXHAUSTED",
        start=start,
        span=span,
        action="ephemeral_port",
    )
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:
        sock.bind(("127.0.0.1", 0))
        return sock.getsockname()[1]


# --- Dataset Iterators ---


def iter_chunks(directory: Path) -> Iterator[Path]:
    """Yield chunk JSONL files from ``directory`` and all descendants.

    Args:
        directory: Directory containing chunk artifacts.

    Returns:
        Iterator over absolute ``Path`` objects.

    Yields:
        Absolute paths to files matching ``*.chunks.jsonl`` sorted
        lexicographically.

    Examples:
        >>> next(iter_chunks(Path(".")), None) is None
        True
    """

    seen = set()
    for candidate in directory.rglob("*.chunks.jsonl"):
        if candidate.is_file() and not candidate.name.startswith("."):
            seen.add(candidate.resolve())
    for path in sorted(seen):
        yield path


# --- Collection Utilities ---


class Batcher(Iterable[List[T]]):
    """Yield fixed-size batches from an iterable with optional policies.

    Args:
        iterable: Source iterable providing items to batch.
        batch_size: Maximum number of elements per yielded batch.
        policy: Optional batching policy. When ``"length"`` the iterable is
            bucketed by ``lengths`` before batching.
        lengths: Sequence of integer lengths aligned with ``iterable`` used for
            length-aware batching policies.

    Examples:
        >>> list(Batcher([1, 2, 3, 4, 5], 2))
        [[1, 2], [3, 4], [5]]
    """

    def __init__(
        self,
        iterable: Iterable[T],
        batch_size: int,
        *,
        policy: Optional[str] = None,
        lengths: Optional[Sequence[int]] = None,
    ):
        if batch_size < 1:
            raise ValueError("batch_size must be >= 1")
        self._items: List[T] = list(iterable)
        self._batch_size = batch_size
        self._policy = (policy or "").lower() or None
        if self._policy:
            if self._policy not in {"length"}:
                raise ValueError(f"Unsupported batching policy: {policy}")
            if lengths is None:
                raise ValueError("lengths must be provided when using a policy")
            if len(lengths) != len(self._items):
                raise ValueError("lengths must align with iterable length")
            self._lengths = [int(max(0, length)) for length in lengths]
        else:
            self._lengths = None

    @staticmethod
    def _length_bucket(length: int) -> int:
        """Return the power-of-two bucket for ``length``."""

        if length <= 0:
            return 0
        return 1 << (int(math.log2(length - 1)) + 1)

    def _ordered_indices(self) -> List[int]:
        if not self._lengths:
            return list(range(len(self._items)))
        pairs = [
            (idx, self._length_bucket(self._lengths[idx]))
            for idx in range(len(self._items))
        ]
        pairs.sort(key=lambda pair: (pair[1], pair[0]))
        return [idx for idx, _ in pairs]

    def __iter__(self) -> Iterator[List[T]]:
        if not self._policy:
            for i in range(0, len(self._items), self._batch_size):
                yield self._items[i : i + self._batch_size]
            return

        ordered_indices = self._ordered_indices()
        for i in range(0, len(ordered_indices), self._batch_size):
            batch_indices = ordered_indices[i : i + self._batch_size]
            yield [self._items[idx] for idx in batch_indices]


# --- Concurrency Utilities ---


@contextlib.contextmanager
def acquire_lock(path: Path, timeout: float = 60.0) -> Iterator[bool]:
    """Acquire an advisory lock using ``.lock`` sentinel files.

    Args:
        path: Target file path whose lock should be acquired.
        timeout: Maximum time in seconds to wait for the lock.

    Returns:
        Iterator yielding a boolean when the lock is acquired.

    Yields:
        ``True`` once the lock is acquired.

    Raises:
        TimeoutError: If the lock cannot be obtained within ``timeout``.

    Examples:
        >>> target = Path("/tmp/lock.txt")
        >>> with acquire_lock(target):
        ...     pass
    """

    lock_path = path.with_suffix(path.suffix + ".lock")
    start = time.time()
    while lock_path.exists():
        try:
            pid_text = lock_path.read_text(encoding="utf-8").strip()
            existing_pid = int(pid_text) if pid_text else None
        except (OSError, ValueError):
            existing_pid = None

        if existing_pid and not _pid_is_running(existing_pid):
            lock_path.unlink(missing_ok=True)
            continue

        if time.time() - start > timeout:
            raise TimeoutError(f"Could not acquire lock on {path} after {timeout}s")
        time.sleep(0.1)

    try:
        lock_path.write_text(str(os.getpid()), encoding="utf-8")
        yield True
    finally:
        lock_path.unlink(missing_ok=True)


def _pid_is_running(pid: int) -> bool:
    """Return ``True`` if a process with the given PID appears to be alive."""

    if pid is None or pid <= 0:
        return False
    try:
        os.kill(pid, 0)
    except ProcessLookupError:
        return False
    except PermissionError:  # pragma: no cover - platform specific
        return True
    except OSError:  # pragma: no cover - defensive guard
        return False
    return True


def set_spawn_or_warn(logger: Optional[logging.Logger] = None) -> None:
    """Ensure the multiprocessing start method is set to ``spawn``.

    Args:
        logger: Optional logger that receives diagnostic messages about the start
            method configuration.

    Returns:
        None: The function mutates global multiprocessing state and logs warnings.

    This helper attempts to set the start method to ``spawn`` with ``force=True``.
    If a ``RuntimeError`` occurs (meaning the method was already set), it checks
    if the current method is ``spawn``. If not, it emits a warning about the
    potential CUDA safety risk, logging the current method so callers understand
    the degraded safety state.
    """

    import multiprocessing as mp

    try:
        mp.set_start_method("spawn", force=True)
        if logger is not None:
            logger.debug("Multiprocessing start method set to 'spawn'")
        return
    except RuntimeError:
        current = mp.get_start_method(allow_none=True)
        if current == "spawn":
            if logger is not None:
                logger.debug("Multiprocessing start method already 'spawn'")
            return
        message = "Multiprocessing start method is %s; CUDA workloads require 'spawn'." % (
            current or "unset"
        )
        if logger is not None:
            log_event(
                logger,
                "warning",
                message,
                stage="core",
                doc_id="__system__",
                input_hash=None,
                error_code="MP_SPAWN_REQUIRED",
                current_method=current or "unset",
            )
        else:
            log_event(
                logging.getLogger(__name__),
                "warning",
                message,
                stage="core",
                doc_id="__system__",
                input_hash=None,
                error_code="MP_SPAWN_REQUIRED",
                current_method=current or "unset",
            )


# --- Unified CLI ---


CommandHandler = Callable[[Sequence[str]], int]

CLI_DESCRIPTION = """\
Unified DocParsing CLI

Examples:
  python -m DocsToKG.DocParsing.cli all --resume
  python -m DocsToKG.DocParsing.cli chunk --data-root Data
  python -m DocsToKG.DocParsing.cli embed --resume
  python -m DocsToKG.DocParsing.cli doctags --mode pdf --workers 2
  python -m DocsToKG.DocParsing.cli token-profiles --doctags-dir Data/DocTagsFiles
  python -m DocsToKG.DocParsing.cli manifest --stage chunk --tail 10
  python -m DocsToKG.DocParsing.cli plan --data-root Data --mode auto
"""

_PDF_SUFFIXES: tuple[str, ...] = (".pdf",)
_HTML_SUFFIXES: tuple[str, ...] = (".html", ".htm")


def _run_chunk(argv: Sequence[str]) -> int:
    """Execute the Docling chunker subcommand."""

    from DocsToKG.DocParsing import chunking as chunk_module

    parser = chunk_module.build_parser()
    parser.prog = "docparse chunk"
    args = parser.parse_args(argv)
    return chunk_module.main(args)


def _run_embed(argv: Sequence[str]) -> int:
    """Execute the embedding pipeline subcommand."""

    from DocsToKG.DocParsing import embedding as embedding_module

    parser = embedding_module.build_parser()
    parser.prog = "docparse embed"
    args = parser.parse_args(argv)
    return embedding_module.main(args)


def _run_token_profiles(argv: Sequence[str]) -> int:
    """Execute the tokenizer profiling subcommand."""

    from DocsToKG.DocParsing import token_profiles as token_profiles_module

    parser = token_profiles_module.build_parser()
    parser.prog = "docparse token-profiles"
    args = parser.parse_args(argv)
    return token_profiles_module.main(args)


def _run_plan(argv: Sequence[str]) -> int:
    """Display the doctags → chunk → embed plan without executing."""

    args = list(argv)
    if "--plan" not in args:
        args.append("--plan")
    return _run_all(args)


def _run_manifest(argv: Sequence[str]) -> int:
    """Inspect pipeline manifest artifacts via CLI."""

    parser = argparse.ArgumentParser(
        prog="docparse manifest",
        description="Inspect DocParsing manifest artifacts",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "--stage",
        dest="stages",
        action="append",
        default=None,
        help="Manifest stage to inspect (repeatable). Defaults to doctags, chunk, embeddings.",
    )
    parser.add_argument(
        "--data-root",
        type=Path,
        default=None,
        help="DocsToKG data root override used when resolving manifests",
    )
    parser.add_argument(
        "--tail",
        type=int,
        default=0,
        help="Print the last N manifest entries",
    )
    parser.add_argument(
        "--summarize",
        action="store_true",
        help="Print per-stage status and duration summary",
    )
    parser.add_argument(
        "--raw",
        action="store_true",
        help="Output tail entries as JSON instead of human-readable text",
    )

    args = parser.parse_args(argv)
    manifest_dir = data_manifests(args.data_root)
    if args.stages:
        seen = []
        for stage in args.stages:
            trimmed = stage.strip()
            if trimmed and trimmed not in seen:
                seen.append(trimmed)
        stages = seen
    else:
        discovered = []
        for path in sorted(manifest_dir.glob("docparse.*.manifest.jsonl")):
            parts = path.name.split(".")
            if len(parts) >= 4:
                stage = parts[1]
                if stage not in discovered:
                    discovered.append(stage)
        stages = discovered
    if not stages:
        stages = ["embeddings"]
    logger = get_logger(
        __name__,
        base_fields={"stage": "manifest"},
    )

    entries = list(iter_manifest_entries(stages, args.data_root))
    if not entries:
        log_event(
            logger,
            "warning",
            "No manifest entries located",
            stage="manifest",
            doc_id="__aggregate__",
            input_hash=None,
            error_code="NO_MANIFEST_ENTRIES",
            stages=stages,
        )
        print("No manifest entries found for the requested stages.")
        return 0

    tail_count = max(0, int(args.tail))
    tail_entries = entries[-tail_count:] if tail_count else []

    if tail_count:
        print(f"docparse manifest tail (last {len(tail_entries)} entries)")
        if args.raw:
            for entry in tail_entries:
                print(json.dumps(entry, ensure_ascii=False))
        else:
            for entry in tail_entries:
                timestamp = entry.get("timestamp", "")
                stage = entry.get("stage", "unknown")
                doc_id = entry.get("doc_id", "unknown")
                status = entry.get("status", "unknown")
                duration = entry.get("duration_s")
                line = f"{timestamp} [{stage}] {doc_id} status={status}"
                if duration is not None:
                    line += f" duration={duration}"
                error = entry.get("error")
                if error:
                    line += f" error={error}"
                print(line)

    if args.summarize or not tail_count:
        summary = summarize_manifest(entries)
        print("\nManifest summary")
        for stage in sorted(summary):
            data = summary[stage]
            print(f"- {stage}: total={data['total']} duration_s={data['duration_s']}")
            status_map = data.get("statuses", {})
            if status_map:
                statuses = ", ".join(f"{name}={count}" for name, count in sorted(status_map.items()))
                print(f"  statuses: {statuses}")

    log_event(
        logger,
        "info",
        "Manifest inspection completed",
        stage="manifest",
        doc_id="__aggregate__",
        input_hash=None,
        error_code="MANIFEST_OK",
        tail_count=tail_count,
        summarize=bool(args.summarize or not tail_count),
        stages=stages,
    )
    return 0


def _build_doctags_parser(prog: str = "docparse doctags") -> argparse.ArgumentParser:
    """Create an :mod:`argparse` parser configured for DocTags conversion."""

    from DocsToKG.DocParsing import doctags as doctags_module

    examples = """Examples:
  docparse doctags --input Data/HTML
  docparse doctags --mode pdf --workers 4
  docparse doctags --mode html --overwrite
"""
    parser = argparse.ArgumentParser(
        prog=prog,
        description="Convert HTML or PDF corpora to DocTags using Docling",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=examples,
    )
    parser.add_argument(
        "--mode",
        choices=["auto", "html", "pdf"],
        default="auto",
        help="Select conversion backend; auto infers from input directory",
    )
    doctags_module.add_data_root_option(parser)
    parser.add_argument(
        "--in-dir",
        "--input",
        dest="in_dir",
        type=Path,
        default=None,
        help="Directory containing HTML or PDF sources (defaults vary by mode)",
    )
    parser.add_argument(
        "--out-dir",
        "--output",
        dest="out_dir",
        type=Path,
        default=None,
        help="Destination for generated .doctags files (defaults vary by mode)",
    )
    parser.add_argument(
        "--workers",
        type=int,
        default=None,
        help="Worker processes to launch; backend defaults used when omitted",
    )
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="Override vLLM model path or identifier for PDF conversion",
    )
    parser.add_argument(
        "--served-model-name",
        dest="served_model_names",
        action="append",
        nargs="+",
        default=None,
        help="Model alias to expose from vLLM (repeatable)",
    )
    parser.add_argument(
        "--gpu-memory-utilization",
        type=float,
        default=None,
        help="Fraction of GPU memory allocated to the vLLM server",
    )
    doctags_module.add_resume_force_options(
        parser,
        resume_help="Skip documents whose outputs already exist with matching content hash",
        force_help="Force reprocessing even when resume criteria are satisfied",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite existing DocTags files (HTML mode only)",
    )
    return parser


def _scan_pdf_html(input_dir: Path) -> tuple[bool, bool]:
    """Return booleans indicating whether PDFs or HTML files exist beneath ``input_dir``."""

    has_pdf = False
    has_html = False

    if not input_dir.exists():
        return has_pdf, has_html

    for root, _dirs, files in os.walk(input_dir):
        if not files:
            continue
        for name in files:
            lower = name.lower()
            if not has_pdf and lower.endswith(_PDF_SUFFIXES):
                has_pdf = True
            elif not has_html and lower.endswith(_HTML_SUFFIXES):
                has_html = True
            if has_pdf and has_html:
                return has_pdf, has_html
    return has_pdf, has_html


def _directory_contains_suffixes(directory: Path, suffixes: tuple[str, ...]) -> bool:
    """Return True when ``directory`` contains at least one file ending with ``suffixes``."""

    if not directory.exists():
        return False
    suffixes_lower = tuple(s.lower() for s in suffixes)
    for root, _dirs, files in os.walk(directory):
        if not files:
            continue
        for name in files:
            if name.lower().endswith(suffixes_lower):
                return True
    return False


def _detect_mode(input_dir: Path) -> str:
    """Infer conversion mode based on the contents of ``input_dir``."""

    if not input_dir.exists():
        raise ValueError(f"Cannot auto-detect mode in {input_dir}: directory not found")

    has_pdf, has_html = _scan_pdf_html(input_dir)
    if has_pdf and not has_html:
        return "pdf"
    if has_html and not has_pdf:
        return "html"
    if has_pdf and has_html:
        raise ValueError(f"Cannot auto-detect mode in {input_dir}: found both PDF and HTML files")
    raise ValueError(f"Cannot auto-detect mode in {input_dir}: no PDF or HTML files found")


def _merge_args(parser: argparse.ArgumentParser, overrides: Dict[str, Any]) -> argparse.Namespace:
    """Merge override values into the default parser namespace."""

    base = parser.parse_args([])
    for key, value in overrides.items():
        if value is not None:
            setattr(base, key, value)
    return base


def _run_doctags(argv: Sequence[str]) -> int:
    """Execute the DocTags conversion subcommand."""

    from DocsToKG.DocParsing import doctags as doctags_module

    parser = _build_doctags_parser()
    args = parser.parse_args(argv)
    logger = get_logger(__name__)

    resolved_root = (
        detect_data_root(args.data_root) if args.data_root is not None else detect_data_root()
    )

    html_default_in = data_html(resolved_root)
    pdf_default_in = data_pdfs(resolved_root)
    doctags_default_out = data_doctags(resolved_root)

    mode = args.mode
    if args.in_dir is not None:
        input_dir = args.in_dir.resolve()
        if mode == "auto":
            mode = _detect_mode(input_dir)
    else:
        if mode == "auto":
            html_present = _directory_contains_suffixes(html_default_in, _HTML_SUFFIXES)
            pdf_present = _directory_contains_suffixes(pdf_default_in, _PDF_SUFFIXES)
            if html_present and not pdf_present:
                mode = "html"
            elif pdf_present and not html_present:
                mode = "pdf"
            else:
                raise ValueError("Cannot auto-detect mode: specify --mode or --input explicitly")
        input_dir = html_default_in if mode == "html" else pdf_default_in

    output_dir = args.out_dir.resolve() if args.out_dir is not None else doctags_default_out

    args.in_dir = input_dir
    args.out_dir = output_dir

    logger.info(
        "Unified DocTags conversion",
        extra={
            "extra_fields": {
                "mode": mode,
                "data_root": str(resolved_root),
                "input_dir": str(input_dir),
                "output_dir": str(output_dir),
                "workers": args.workers,
                "resume": args.resume,
                "force": args.force,
                "overwrite": args.overwrite,
                "model": args.model,
                "served_model_names": args.served_model_names,
                "gpu_memory_utilization": args.gpu_memory_utilization,
            }
        },
    )

    base_overrides = {
        "data_root": args.data_root,
        "input": input_dir,
        "output": output_dir,
        "workers": args.workers,
        "resume": args.resume,
        "force": args.force,
    }

    if mode == "html":
        html_overrides = {
            **base_overrides,
            "overwrite": args.overwrite,
        }
        html_args = _merge_args(doctags_module.html_build_parser(), html_overrides)
        return doctags_module.html_main(html_args)

    overrides = {
        **base_overrides,
        "model": args.model,
        "served_model_names": args.served_model_names,
        "gpu_memory_utilization": args.gpu_memory_utilization,
    }
    pdf_args = _merge_args(doctags_module.pdf_build_parser(), overrides)
    return doctags_module.pdf_main(pdf_args)


def _preview_list(items: List[str], limit: int = 5) -> List[str]:
    """Return a truncated preview list with remainder hint."""

    if len(items) <= limit:
        return list(items)
    preview = list(items[:limit])
    preview.append(f"... (+{len(items) - limit} more)")
    return preview


def _plan_doctags(argv: Sequence[str]) -> Dict[str, Any]:
    """Compute which DocTags inputs would be processed."""

    from DocsToKG.DocParsing import doctags as doctags_module

    parser = _build_doctags_parser()

    args, _unknown = parser.parse_known_args(argv)
    resolved_root = (
        detect_data_root(args.data_root) if args.data_root is not None else detect_data_root()
    )

    html_default_in = data_html(resolved_root)
    pdf_default_in = data_pdfs(resolved_root)
    doctags_default_out = data_doctags(resolved_root)

    mode = args.mode
    if args.in_dir is not None:
        input_dir = args.in_dir.resolve()
        if mode == "auto":
            mode = _detect_mode(input_dir)
    else:
        if mode == "auto":
            html_present = _directory_contains_suffixes(html_default_in, _HTML_SUFFIXES)
            pdf_present = _directory_contains_suffixes(pdf_default_in, _PDF_SUFFIXES)
            if html_present and not pdf_present:
                mode = "html"
            elif pdf_present and not html_present:
                mode = "pdf"
            else:
                raise ValueError("Cannot auto-detect mode: specify --mode or --input explicitly")
        input_dir = html_default_in if mode == "html" else pdf_default_in

    output_dir = args.out_dir.resolve() if args.out_dir is not None else doctags_default_out

    if not input_dir.exists():
        return {
            "stage": "doctags",
            "mode": mode,
            "input_dir": str(input_dir),
            "output_dir": str(output_dir),
            "process": [],
            "skip": [],
            "notes": ["Input directory missing"],
        }

    if mode == "html":
        files = doctags_module.list_htmls(input_dir)
        manifest_stage = getattr(doctags_module, "HTML_MANIFEST_STAGE", "doctags-html")
        overwrite = bool(getattr(args, "overwrite", False))
    else:
        files = doctags_module.list_pdfs(input_dir)
        manifest_stage = doctags_module.MANIFEST_STAGE
        overwrite = False

    manifest_index = load_manifest_index(manifest_stage, resolved_root) if args.resume else {}
    resume_controller = ResumeController(args.resume, args.force, manifest_index)
    planned: List[str] = []
    skipped: List[str] = []

    for path in files:
        doc_id, out_path = derive_doc_id_and_doctags_path(path, input_dir, output_dir)
        input_hash = compute_content_hash(path)
        skip, _ = resume_controller.should_skip(doc_id, out_path, input_hash)
        if mode == "html" and overwrite:
            skip = False
        if skip:
            skipped.append(doc_id)
        else:
            planned.append(doc_id)

    return {
        "stage": "doctags",
        "mode": mode,
        "input_dir": str(input_dir),
        "output_dir": str(output_dir),
        "process": planned,
        "skip": skipped,
        "notes": [],
    }


def _plan_chunk(argv: Sequence[str]) -> Dict[str, Any]:
    """Compute which DocTags files the chunk stage would touch."""

    from DocsToKG.DocParsing import chunking as chunk_module
    from DocsToKG.DocParsing import doctags as doctags_module

    parser = chunk_module.build_parser()
    args, _unknown = parser.parse_known_args(argv)
    resolved_root = doctags_module.prepare_data_root(args.data_root, detect_data_root())
    data_root_overridden = args.data_root is not None

    default_in_dir = data_doctags(resolved_root)
    default_out_dir = data_chunks(resolved_root)

    in_dir = doctags_module.resolve_pipeline_path(
        cli_value=args.in_dir,
        default_path=default_in_dir,
        resolved_data_root=resolved_root,
        data_root_overridden=data_root_overridden,
        resolver=data_doctags,
    ).resolve()

    out_dir = doctags_module.resolve_pipeline_path(
        cli_value=args.out_dir,
        default_path=default_out_dir,
        resolved_data_root=resolved_root,
        data_root_overridden=data_root_overridden,
        resolver=data_chunks,
    ).resolve()

    if not in_dir.exists():
        return {
            "stage": "chunk",
            "input_dir": str(in_dir),
            "output_dir": str(out_dir),
            "process": [],
            "skip": [],
            "notes": ["DocTags directory missing"],
        }

    files = list(iter_doctags(in_dir))
    manifest_index = (
        load_manifest_index(chunk_module.MANIFEST_STAGE, resolved_root) if args.resume else {}
    )
    resume_controller = ResumeController(args.resume, args.force, manifest_index)
    planned: List[str] = []
    skipped: List[str] = []

    for path in files:
        rel_id, out_path = derive_doc_id_and_chunks_path(path, in_dir, out_dir)
        input_hash = compute_content_hash(path)
        skip, _ = resume_controller.should_skip(rel_id, out_path, input_hash)
        if skip:
            skipped.append(rel_id)
        else:
            planned.append(rel_id)

    return {
        "stage": "chunk",
        "input_dir": str(in_dir),
        "output_dir": str(out_dir),
        "process": planned,
        "skip": skipped,
        "notes": [],
    }


def _plan_embed(argv: Sequence[str]) -> Dict[str, Any]:
    """Compute which chunk files the embed stage would process or validate."""

    from DocsToKG.DocParsing import doctags as doctags_module
    from DocsToKG.DocParsing import embedding as embedding_module

    parser = embedding_module.build_parser()
    args, _unknown = parser.parse_known_args(argv)
    resolved_root = doctags_module.prepare_data_root(args.data_root, detect_data_root())
    data_root_overridden = args.data_root is not None

    default_chunks_dir = data_chunks(resolved_root)
    default_vectors_dir = data_vectors(resolved_root)

    chunks_dir = doctags_module.resolve_pipeline_path(
        cli_value=args.chunks_dir,
        default_path=default_chunks_dir,
        resolved_data_root=resolved_root,
        data_root_overridden=data_root_overridden,
        resolver=data_chunks,
    ).resolve()

    vectors_dir = doctags_module.resolve_pipeline_path(
        cli_value=args.out_dir,
        default_path=default_vectors_dir,
        resolved_data_root=resolved_root,
        data_root_overridden=data_root_overridden,
        resolver=data_vectors,
    ).resolve()

    if args.validate_only:
        validate: List[str] = []
        missing: List[str] = []
        for chunk_path in iter_chunks(chunks_dir):
            doc_id, vector_path = derive_doc_id_and_vectors_path(
                chunk_path, chunks_dir, vectors_dir
            )
            if vector_path.exists():
                validate.append(doc_id)
            else:
                missing.append(doc_id)
        return {
            "stage": "embed",
            "action": "validate",
            "chunks_dir": str(chunks_dir),
            "vectors_dir": str(vectors_dir),
            "validate": validate,
            "missing": missing,
            "notes": [],
        }

    files = list(iter_chunks(chunks_dir))
    manifest_index = (
        load_manifest_index(embedding_module.MANIFEST_STAGE, resolved_root) if args.resume else {}
    )
    resume_controller = ResumeController(args.resume, args.force, manifest_index)
    planned: List[str] = []
    skipped: List[str] = []

    for chunk_path in files:
        doc_id, vector_path = derive_doc_id_and_vectors_path(chunk_path, chunks_dir, vectors_dir)
        input_hash = compute_content_hash(chunk_path)
        skip, _ = resume_controller.should_skip(doc_id, vector_path, input_hash)
        if skip:
            skipped.append(doc_id)
        else:
            planned.append(doc_id)

    return {
        "stage": "embed",
        "action": "generate",
        "chunks_dir": str(chunks_dir),
        "vectors_dir": str(vectors_dir),
        "process": planned,
        "skip": skipped,
        "notes": [],
    }


def _display_plan(plans: Sequence[Dict[str, Any]]) -> None:
    """Pretty-print plan summaries to stdout."""

    print("docparse all plan")
    for entry in plans:
        stage = entry.get("stage", "unknown")
        notes = entry.get("notes", [])
        if stage == "doctags":
            desc = f"doctags (mode={entry.get('mode')})"
            process = entry.get("process", [])
            skip = entry.get("skip", [])
            print(f"- {desc}: process {len(process)}, skip {len(skip)}")
            print(f"  input:  {entry.get('input_dir')}")
            print(f"  output: {entry.get('output_dir')}")
            if process:
                print("  process preview:", ", ".join(_preview_list(process)))
            if skip:
                print("  skip preview:", ", ".join(_preview_list(skip)))
        elif stage == "chunk":
            process = entry.get("process", [])
            skip = entry.get("skip", [])
            print(f"- chunk: process {len(process)}, skip {len(skip)}")
            print(f"  input:  {entry.get('input_dir')}")
            print(f"  output: {entry.get('output_dir')}")
            if process:
                print("  process preview:", ", ".join(_preview_list(process)))
            if skip:
                print("  skip preview:", ", ".join(_preview_list(skip)))
        elif stage == "embed" and entry.get("action") == "validate":
            validate = entry.get("validate", [])
            missing = entry.get("missing", [])
            print(
                f"- embed (validate-only): validate {len(validate)}, missing vectors {len(missing)}"
            )
            print(f"  chunks:  {entry.get('chunks_dir')}")
            print(f"  vectors: {entry.get('vectors_dir')}")
            if validate:
                print("  validate preview:", ", ".join(_preview_list(validate)))
            if missing:
                print("  missing preview:", ", ".join(_preview_list(missing)))
        elif stage == "embed":
            process = entry.get("process", [])
            skip = entry.get("skip", [])
            print(f"- embed: process {len(process)}, skip {len(skip)}")
            print(f"  chunks:  {entry.get('chunks_dir')}")
            print(f"  vectors: {entry.get('vectors_dir')}")
            if process:
                print("  process preview:", ", ".join(_preview_list(process)))
            if skip:
                print("  skip preview:", ", ".join(_preview_list(skip)))
        else:
            print(f"- {stage}: no actionable items")
        if notes:
            print("  notes:", "; ".join(notes))
    print()


def _run_all(argv: Sequence[str]) -> int:
    """Execute DocTags conversion, chunking, and embedding sequentially."""

    parser = argparse.ArgumentParser(
        description="Run doctags → chunk → embed in sequence",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "--data-root",
        type=Path,
        default=None,
        help="DocsToKG data root override passed to all stages",
    )
    parser.add_argument(
        "--log-level",
        type=lambda value: str(value).upper(),
        default="INFO",
        choices=["CRITICAL", "ERROR", "WARNING", "INFO", "DEBUG"],
        help="Logging verbosity applied to all stages",
    )
    parser.add_argument(
        "--resume",
        action="store_true",
        help="Resume each stage by skipping outputs with matching manifests",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Force regeneration in each stage even when outputs exist",
    )
    parser.add_argument(
        "--mode",
        choices=["auto", "html", "pdf"],
        default="auto",
        help="DocTags conversion mode",
    )
    parser.add_argument(
        "--doctags-in-dir",
        type=Path,
        default=None,
        help="Override DocTags input directory",
    )
    parser.add_argument(
        "--doctags-out-dir",
        type=Path,
        default=None,
        help="Override DocTags output directory",
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="Allow rewriting DocTags outputs (HTML mode only)",
    )
    parser.add_argument(
        "--vllm-wait-timeout",
        type=int,
        default=None,
        help="Seconds to wait for vLLM readiness during the DocTags stage",
    )
    parser.add_argument(
        "--chunk-out-dir",
        type=Path,
        default=None,
        help="Output directory override for chunk JSONL files",
    )
    parser.add_argument(
        "--chunk-workers",
        type=int,
        default=None,
        help="Worker processes for the chunk stage",
    )
    parser.add_argument(
        "--chunk-min-tokens",
        type=int,
        default=None,
        help="Minimum tokens per chunk passed to the chunk stage",
    )
    parser.add_argument(
        "--chunk-max-tokens",
        type=int,
        default=None,
        help="Maximum tokens per chunk passed to the chunk stage",
    )
    parser.add_argument(
        "--structural-markers",
        type=Path,
        default=None,
        help="Structural marker configuration forwarded to the chunk stage",
    )
    parser.add_argument(
        "--chunk-shard-count",
        type=int,
        default=None,
        help="Total number of shards for the chunk stage",
    )
    parser.add_argument(
        "--chunk-shard-index",
        type=int,
        default=None,
        help="Zero-based shard index for the chunk stage",
    )
    parser.add_argument(
        "--embed-out-dir",
        type=Path,
        default=None,
        help="Output directory override for embedding JSONL files",
    )
    parser.add_argument(
        "--embed-offline",
        action="store_true",
        help="Run the embedding stage with TRANSFORMERS_OFFLINE=1",
    )
    parser.add_argument(
        "--embed-validate-only",
        action="store_true",
        help="Skip embedding generation and only validate existing vectors",
    )
    parser.add_argument(
        "--splade-sparsity-warn-pct",
        dest="splade_sparsity_warn_pct",
        type=float,
        default=None,
        help="Override SPLADE sparsity warning threshold for the embed stage",
    )
    parser.add_argument(
        "--splade-zero-pct-warn-threshold",
        dest="splade_sparsity_warn_pct",
        type=float,
        default=None,
        help=argparse.SUPPRESS,
    )
    parser.add_argument(
        "--embed-shard-count",
        type=int,
        default=None,
        help="Total number of shards for the embed stage (defaults to chunk shard count)",
    )
    parser.add_argument(
        "--embed-shard-index",
        type=int,
        default=None,
        help="Zero-based shard index for the embed stage (defaults to chunk shard index)",
    )
    parser.add_argument(
        "--embed-format",
        choices=["jsonl", "parquet"],
        default=None,
        help="Vector output format for the embed stage",
    )
    parser.add_argument(
        "--embed-no-cache",
        action="store_true",
        help="Disable Qwen cache reuse during the embed stage",
    )
    parser.add_argument(
        "--plan",
        action="store_true",
        help="Show a plan of the files each stage would touch instead of running",
    )

    args = parser.parse_args(argv)
    logger = get_logger(__name__, level=args.log_level)

    chunk_shard_count = args.chunk_shard_count
    chunk_shard_index = args.chunk_shard_index
    embed_shard_count = args.embed_shard_count
    if embed_shard_count is None and chunk_shard_count is not None:
        embed_shard_count = chunk_shard_count
    embed_shard_index = args.embed_shard_index
    if embed_shard_index is None and chunk_shard_index is not None:
        embed_shard_index = chunk_shard_index

    extra = {
        "resume": bool(args.resume),
        "force": bool(args.force),
        "mode": args.mode,
        "log_level": args.log_level,
    }
    if args.data_root:
        extra["data_root"] = str(args.data_root)
    if chunk_shard_count is not None:
        extra["chunk_shard_count"] = chunk_shard_count
    if chunk_shard_index is not None:
        extra["chunk_shard_index"] = chunk_shard_index
    if embed_shard_count is not None:
        extra["embed_shard_count"] = embed_shard_count
    if embed_shard_index is not None:
        extra["embed_shard_index"] = embed_shard_index
    if args.embed_format:
        extra["embed_format"] = args.embed_format
    logger.info("docparse all starting", extra={"extra_fields": extra})

    doctags_args: List[str] = []
    doctags_args.extend(["--log-level", args.log_level])
    if args.data_root:
        doctags_args.extend(["--data-root", str(args.data_root)])
    if args.resume:
        doctags_args.append("--resume")
    if args.force:
        doctags_args.append("--force")
    if args.mode != "auto":
        doctags_args.extend(["--mode", args.mode])
    if args.doctags_in_dir:
        doctags_args.extend(["--in-dir", str(args.doctags_in_dir)])
    if args.doctags_out_dir:
        doctags_args.extend(["--out-dir", str(args.doctags_out_dir)])
    if args.overwrite:
        doctags_args.append("--overwrite")
    if args.vllm_wait_timeout is not None:
        doctags_args.extend(["--vllm-wait-timeout", str(args.vllm_wait_timeout)])

    chunk_args: List[str] = []
    chunk_args.extend(["--log-level", args.log_level])
    if args.data_root:
        chunk_args.extend(["--data-root", str(args.data_root)])
    if args.resume:
        chunk_args.append("--resume")
    if args.force:
        chunk_args.append("--force")
    if args.doctags_out_dir:
        chunk_args.extend(["--in-dir", str(args.doctags_out_dir)])
    if args.chunk_out_dir:
        chunk_args.extend(["--out-dir", str(args.chunk_out_dir)])
    if args.chunk_workers:
        chunk_args.extend(["--workers", str(args.chunk_workers)])
    if args.chunk_min_tokens:
        chunk_args.extend(["--min-tokens", str(args.chunk_min_tokens)])
    if args.chunk_max_tokens:
        chunk_args.extend(["--max-tokens", str(args.chunk_max_tokens)])
    if args.structural_markers:
        chunk_args.extend(["--structural-markers", str(args.structural_markers)])
    if chunk_shard_count is not None:
        chunk_args.extend(["--shard-count", str(chunk_shard_count)])
    if chunk_shard_index is not None:
        chunk_args.extend(["--shard-index", str(chunk_shard_index)])

    embed_args: List[str] = []
    embed_args.extend(["--log-level", args.log_level])
    if args.data_root:
        embed_args.extend(["--data-root", str(args.data_root)])
    if args.resume:
        embed_args.append("--resume")
    if args.force:
        embed_args.append("--force")
    if args.chunk_out_dir:
        embed_args.extend(["--chunks-dir", str(args.chunk_out_dir)])
    if args.embed_out_dir:
        embed_args.extend(["--out-dir", str(args.embed_out_dir)])
    if args.embed_offline:
        embed_args.append("--offline")
    if args.embed_validate_only:
        embed_args.append("--validate-only")
    if args.embed_format:
        embed_args.extend(["--format", args.embed_format])
    if args.embed_no_cache:
        embed_args.append("--no-cache")
    if embed_shard_count is not None:
        embed_args.extend(["--shard-count", str(embed_shard_count)])
    if embed_shard_index is not None:
        embed_args.extend(["--shard-index", str(embed_shard_index)])
    if args.splade_sparsity_warn_pct is not None:
        embed_args.extend(["--splade-sparsity-warn-pct", str(args.splade_sparsity_warn_pct)])

    if args.plan:
        plans: List[Dict[str, Any]] = []
        try:
            plans.append(_plan_doctags(doctags_args))
        except Exception as exc:  # pragma: no cover - plan path should handle gracefully
            plans.append(
                {
                    "stage": "doctags",
                    "mode": args.mode,
                    "input_dir": None,
                    "output_dir": None,
                    "total": 0,
                    "process": [],
                    "skip": [],
                    "notes": [f"DocTags plan unavailable ({exc})"],
                }
            )
        try:
            plans.append(_plan_chunk(chunk_args))
        except Exception as exc:  # pragma: no cover
            plans.append(
                {
                    "stage": "chunk",
                    "input_dir": None,
                    "output_dir": None,
                    "total": 0,
                    "process": [],
                    "skip": [],
                    "notes": [f"Chunk plan unavailable ({exc})"],
                }
            )
        try:
            plans.append(_plan_embed(embed_args))
        except Exception as exc:  # pragma: no cover
            plans.append(
                {
                    "stage": "embed",
                    "operation": "unknown",
                    "chunks_dir": None,
                    "vectors_dir": None,
                    "total": 0,
                    "process": [],
                    "skip": [],
                    "notes": [f"Embed plan unavailable ({exc})"],
                }
            )
        _display_plan(plans)
        return 0

    exit_code = _run_doctags(doctags_args)
    if exit_code != 0:
        log_event(
            logger,
            "error",
            "DocTags stage failed",
            stage="docparse_all",
            doc_id="__aggregate__",
            input_hash=None,
            error_code="DOCTAGS_STAGE_FAILED",
            exit_code=exit_code,
        )
        return exit_code

    exit_code = _run_chunk(chunk_args)
    if exit_code != 0:
        log_event(
            logger,
            "error",
            "Chunk stage failed",
            stage="docparse_all",
            doc_id="__aggregate__",
            input_hash=None,
            error_code="CHUNK_STAGE_FAILED",
            exit_code=exit_code,
        )
        return exit_code

    exit_code = _run_embed(embed_args)
    if exit_code != 0:
        log_event(
            logger,
            "error",
            "Embedding stage failed",
            stage="docparse_all",
            doc_id="__aggregate__",
            input_hash=None,
            error_code="EMBED_STAGE_FAILED",
            exit_code=exit_code,
        )
        return exit_code

    logger.info("docparse all completed", extra={"extra_fields": {"status": "success"}})
    return 0


class _Command:
    """Callable wrapper storing handler metadata for subcommands."""

    __slots__ = ("handler", "help")

    def __init__(self, handler: CommandHandler, help: str) -> None:
        """Capture the callable ``handler`` and its CLI help text."""
        self.handler = handler
        self.help = help


COMMANDS: Dict[str, _Command] = {
    "all": _Command(_run_all, "Run doctags → chunk → embed sequentially"),
    "chunk": _Command(_run_chunk, "Run the Docling hybrid chunker"),
    "embed": _Command(_run_embed, "Generate BM25/SPLADE/dense vectors"),
    "doctags": _Command(_run_doctags, "Convert HTML/PDF corpora into DocTags"),
    "token-profiles": _Command(
        _run_token_profiles,
        "Print token count ratios for DocTags samples across tokenizers",
    ),
    "plan": _Command(
        _run_plan,
        "Show the projected doctags → chunk → embed actions without running stages",
    ),
    "manifest": _Command(
        _run_manifest,
        "Inspect manifest artifacts (tail/summarize)",
    ),
}


def main(argv: Sequence[str] | None = None) -> int:
    """Dispatch to one of the DocParsing subcommands."""

    parser = argparse.ArgumentParser(
        description=CLI_DESCRIPTION,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("command", choices=COMMANDS.keys(), help="CLI to execute")
    parser.add_argument("args", nargs=argparse.REMAINDER, help="Arguments passed to the command")
    parsed = parser.parse_args(argv)
    command = COMMANDS[parsed.command]
    return command.handler(parsed.args)


def run_all(argv: Sequence[str] | None = None) -> int:
    """Public wrapper for the ``all`` subcommand."""

    return _run_all([] if argv is None else list(argv))


def chunk(argv: Sequence[str] | None = None) -> int:
    """Public wrapper for the ``chunk`` subcommand."""

    return _run_chunk([] if argv is None else list(argv))


def embed(argv: Sequence[str] | None = None) -> int:
    """Public wrapper for the ``embed`` subcommand."""

    return _run_embed([] if argv is None else list(argv))


def doctags(argv: Sequence[str] | None = None) -> int:
    """Public wrapper for the ``doctags`` subcommand."""

    return _run_doctags([] if argv is None else list(argv))


def token_profiles(argv: Sequence[str] | None = None) -> int:
    """Public wrapper for the ``token-profiles`` subcommand."""

    return _run_token_profiles([] if argv is None else list(argv))


def plan(argv: Sequence[str] | None = None) -> int:
    """Public wrapper for the ``plan`` subcommand."""

    return _run_plan([] if argv is None else list(argv))


def manifest(argv: Sequence[str] | None = None) -> int:
    """Public wrapper for the ``manifest`` subcommand."""

    return _run_manifest([] if argv is None else list(argv))
