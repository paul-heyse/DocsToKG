## 1. Implementation
- [ ] 1.1 Add `pyrate-limiter>=3.9,<4` to `pyproject.toml` (runtime dependency) and regenerate/commit any tracked lock or requirements artefacts so CI environments install the library.
- [ ] 1.2 Extend `DownloadConfiguration` (and nested defaults/env dataclasses) with `rate_limiter: Literal["pyrate", "legacy"] = "pyrate"`, wire it into env overrides, CLI doctor output, and the `about()`/telemetry payload so operators can confirm the active backend.
- [ ] 1.3 Create a `LimiterManager` in `io/rate_limit.py` that builds `pyrate_limiter.Limiter` instances keyed as `f"{service or '_'}:{host or 'default'}"`, lowercases both parts, and caches them until either the effective rate or the backend path changes.
- [ ] 1.4 Convert the string limits that come from `per_host_rate_limit` / `rate_limits[...]` into explicit `Rate` objects by parsing the unit (`/second`, `/minute`, `/hour`) and mapping it to the matching `Duration`. Validate the resulting list with `validate_rate_list` before creating the limiter.
- [ ] 1.5 Default to `InMemoryBucket`; when `shared_rate_limit_dir` is set, initialise a single `SQLiteBucket` under `<shared_dir>/ratelimit.sqlite` (create the directory if missing) and reuse it for all limiters. Emit a one-time debug log per limiter describing the backend in use.
- [ ] 1.6 Replace the legacy `TokenBucket`/`SharedTokenBucket` classes and JSON file locking with a thin adapter class that wraps a `Limiter`. The adapter SHALL expose `.consume(tokens: float = 1.0)` which calls `Limiter.try_acquire` with `weight=max(1, math.ceil(tokens))`, blocking until the limiter admits the request. Retain a `.reset()` hook that clears the limiter cache.
- [ ] 1.7 Update `get_bucket`, `apply_retry_after`, and `reset` to branch on `DownloadConfiguration.rate_limiter`, calling the legacy path only when explicitly requested and warning once when legacy mode is active. Preserve the `get_bucket_provider` extension hook by passing through third-party providers unchanged.
- [ ] 1.8 Rewrite `_apply_retry_after_from_response` (and the helper in `planning.py` / `checksums.py`) to return the parsed delay without mutating limiter state. Ensure the caller sleeps/backoffs for that value before the next `bucket.consume()`.
- [ ] 1.9 Update all call sites (`io/network.py`, `resolvers.py`, `planning.py`, `checksums.py`, and any helper functions) to stop importing `TokenBucket` directly. Ensure each site acquires the bucket once per HTTP attempt and waits the Retry-After delay when present.
- [ ] 1.10 Refresh `io/__init__.py` exports, remove the old classes, and update `DocsToKG.OntologyDownload.testing` fixtures so test suites can inject stub limiters (both pyrate-backed and legacy) for deterministic behaviour.
- [ ] 1.11 Document the new limiter mode: adjust `src/DocsToKG/OntologyDownload/README.md` and `LibraryDocumentation/pyrate-limiter.md` to show the configuration flag, backend selection (in-memory vs SQLite), and example usage for developers following the migration.

## 2. Verification
- [ ] 2.1 Add unit tests for the new manager covering key normalisation, rate override refresh, backend selection (in-memory vs SQLite), and cache reset behaviour. Use a temp directory for SQLite tests.
- [ ] 2.2 Add a regression test in `tests/ontology_download` that simulates a Retry-After response and asserts the retry loop sleeps before acquiring the limiter again without direct state mutation.
- [ ] 2.3 Add resolver-focused tests ensuring per-service overrides still apply (e.g., OLS vs default host), and that shared buckets throttle multiple resolver calls.
- [ ] 2.4 Ensure `DocsToKG.OntologyDownload.testing` exposes helpers to swap between `"pyrate"` and `"legacy"` modes; add tests that verify custom `get_bucket_provider` hooks continue to work.
- [ ] 2.5 Run `./.venv/bin/pytest tests/ontology_download -q` (both default and with `ONTOfetch_RATE_LIMITER=legacy` if exposed), plus `./.venv/bin/ruff check`, `./.venv/bin/mypy`, and documentation validators to satisfy CI parity.
- [ ] 2.6 Execute `./.venv/bin/python -m DocsToKG.OntologyDownload.cli doctor --json` and capture the updated HTTP diagnostics for release notes (showing the new `rate_limiter` field and backend per host).
