{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 0, "source_chunk_idxs": [0], "num_tokens": 500, "text": "Frontiers | Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nFrontiers in Digital Health\nAbout us\nAbout us\n- Who we are\n- Mission and values\n- History\n- Leadership\n- Awards\n- Impact and progress\n- Frontiers' impact\n- Our annual reports\n- Publishing model\n- How we publish\n- Open access\n- Peer review\n- Research integrity\n- Research Topics\n- FAIR\u00b2 Data Management\n- Fee policy\n- Services\n- Societies\n- National consortia\n- Institutional partnerships\n- Collaborators\n- More from Frontiers\n- Frontiers Forum\n- Frontiers Planet Prize\n- Press office\n- Sustainability\n- Career opportunities\n- Contact us\nAll journals\nAll articles\nSubmit your research\nSearch\nFrontiers in Digital Health\nSections\nSections\n- Connected Health\n- Digital Mental Health\n- Ethical Digital Health\n- Health Communications and Behavior Change\n- Health Informatics\n- Health Technology Implementation\n- Human Factors and Digital Health\n- Personalized Medicine\nArticles\nResearch Topics\nEditorial board\nAbout journal\nAbout journal\n- Scope\n- Field chief editors\n- Mission &amp; scope\n- Facts\n- Journal sections\n- Open access statement\n- Copyright statement\n- Quality\n- For authors\n- Why submit?\n- Article types\n- Author guidelines\n- Editor guidelines\n- Publishing fees\n- Submission checklist\n- Contact editorial office\nAbout us\nAbout us\n- Who we are\n- Mission and values\n- History\n- Leadership\n- Awards\n- Impact and progress\n- Frontiers' impact\n- Our annual reports\n- Publishing model\n- How we publish\n- Open access\n- Peer review\n- Research integrity\n- Research Topics\n- FAIR\u00b2 Data Management\n- Fee policy\n- Services\n- Societies\n- National consortia\n- Institutional partnerships\n- Collaborators\n- More from Frontiers\n- Frontiers Forum\n- Frontiers Planet Prize\n- Press office\n- Sustainability\n- Career opportunities\n- Contact us\nAll journals\nAll articles\nSubmit your research\nFrontiers in Digital Health\nSections\nSections\n- Connected Health\n- Digital Mental Health\n- Ethical Digital Health\n- Health Communications and Behavior Change\n- Health Informatics\n- Health Technology Implementation\n- Human Factors and Digital Health\n- Personalized Medicine\nArticles\nResearch Topics\nEditorial board\nAbout journal\nAbout journal\n- Scope\n- Field chief editors\n- Mission &amp; scope\n- Facts\n- Journal sections\n- Open access statement\n- Copyright statement\n- Quality\n- For authors\n- Why submit?\n- Article types\n- Author guidelines\n- Editor guidelines\n- Publishing fees\n- Submission checklist\n- Contact editorial office\nFrontiers in Digital Health\nSections\nSections\n- Connected Health\n- Digital Mental Health\n- Ethical Digital Health\n- Health Communications and Behavior Change\n- Health Informatics\n- Health Technology Implementation\n- Human Factors and Digital Health\n- Personalized Medicine\nArticles\nResearch Topics\nEditorial board\nAbout journal\nAbout journal\n- Scope\n- Field chief editors\n- Mission &amp; scope\n- Facts\n- Journal sections\n- Open access statement\n- Copyright statement\n- Quality", "doc_items_refs": ["#/texts/1", "#/texts/2", "#/texts/3", "#/texts/4", "#/texts/5", "#/texts/6", "#/texts/7", "#/texts/8", "#/texts/9", "#/texts/10", "#/texts/11", "#/texts/12", "#/texts/13", "#/texts/14", "#/texts/15", "#/texts/16", "#/texts/17", "#/texts/18", "#/texts/19", "#/texts/20", "#/texts/21", "#/texts/22", "#/texts/23", "#/texts/24", "#/texts/25", "#/texts/26", "#/texts/27", "#/texts/28", "#/texts/29", "#/texts/30", "#/texts/31", "#/texts/32", "#/texts/33", "#/texts/34", "#/texts/35", "#/texts/36", "#/texts/37", "#/texts/38", "#/texts/39", "#/texts/40", "#/texts/41", "#/texts/42", "#/texts/43", "#/texts/44", "#/texts/45", "#/texts/46", "#/texts/47", "#/texts/48", "#/texts/49", "#/texts/50", "#/texts/51", "#/texts/52", "#/texts/53", "#/texts/54", "#/texts/55", "#/texts/56", "#/texts/57", "#/texts/58", "#/texts/59", "#/texts/60", "#/texts/61", "#/texts/62", "#/texts/63", "#/texts/64", "#/texts/65", "#/texts/66", "#/texts/67", "#/texts/68", "#/texts/69", "#/texts/70", "#/texts/71", "#/texts/72", "#/texts/73", "#/texts/74", "#/texts/75", "#/texts/76", "#/texts/77", "#/texts/78", "#/texts/79", "#/texts/80", "#/texts/81", "#/texts/82", "#/texts/83", "#/texts/84", "#/texts/85", "#/texts/86", "#/texts/87", "#/texts/88", "#/texts/89", "#/texts/90", "#/texts/91", "#/texts/92", "#/texts/93", "#/texts/94", "#/texts/95", "#/texts/96", "#/texts/97", "#/texts/98", "#/texts/99", "#/texts/100", "#/texts/101", "#/texts/102", "#/texts/103", "#/texts/104", "#/texts/105", "#/texts/106", "#/texts/107", "#/texts/108", "#/texts/109", "#/texts/110", "#/texts/111", "#/texts/112", "#/texts/113", "#/texts/114", "#/texts/115", "#/texts/116", "#/texts/117", "#/texts/118", "#/texts/119", "#/texts/120", "#/texts/121", "#/texts/122", "#/texts/123", "#/texts/124", "#/texts/125", "#/texts/126", "#/texts/127", "#/texts/128", "#/texts/129", "#/texts/130", "#/texts/131", "#/texts/132", "#/texts/133", "#/texts/134", "#/texts/135", "#/texts/136", "#/texts/137", "#/texts/138", "#/texts/139", "#/texts/140", "#/texts/141", "#/texts/142", "#/texts/143", "#/texts/144", "#/texts/145", "#/texts/146", "#/texts/147", "#/texts/148", "#/texts/149", "#/texts/150", "#/texts/151", "#/texts/152", "#/texts/153", "#/texts/154", "#/texts/155", "#/texts/156"], "page_nos": [], "uuid": "e5d8bbbc-e8e4-4e8b-b010-cf5e97bc4a3c"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 1, "source_chunk_idxs": [1], "num_tokens": 153, "text": "Frontiers | Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n- For authors\n- Why submit?\n- Article types\n- Author guidelines\n- Editor guidelines\n- Publishing fees\n- Submission checklist\n- Contact editorial office\nSubmit your research\nSearch\nYour new experience awaits. Try the new design now and help us make it even better\nSwitch to the new experience\nORIGINAL RESEARCH article\nFront. Digit. Health , 03 July 2025\nSec. Digital Mental Health\nVolume 7 - 2025 |\nhttps://doi.org/10.3389/fdgth.2025.1571053\nThis article is part of the Research Topic Methodological and Technical Issues of Tele-neuropsychology: Remote Cognitive Assessment and Intervention Across the Life Span\nView all 6 articles", "doc_items_refs": ["#/texts/157", "#/texts/158", "#/texts/159", "#/texts/160", "#/texts/161", "#/texts/162", "#/texts/163", "#/texts/164", "#/texts/165", "#/texts/166", "#/texts/167", "#/texts/168", "#/texts/169", "#/texts/170", "#/texts/171", "#/texts/172", "#/texts/173", "#/texts/174", "#/texts/175"], "page_nos": [], "uuid": "db798857-0eb7-4184-9f9a-f36305a35f01"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 2, "source_chunk_idxs": [2], "num_tokens": 460, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nDuong Huynh\n1\nSiao Ye\n1\nReza Hosseini Ghomi\n1,2\nMary Patterson\n1\nBin Huang\n1*\n- 1\nDepartment of Clinical Development, BrainCheck Inc., Austin, TX, United States\n- 2\nDepartment of Neurology, Faculty, Institute for Neuroengineering, &amp; eScience Institute, University of Washington, Seattle, WA, United States\nIntroduction:\nEarly diagnosis of cognitive impairment and dementia relies on comprehensive, evidence-based cognitive assessments, which currently requires a clinic visit and access to skilled healthcare providers. This poses a challenge for people who live in areas with inadequate primary care services and those who have economic, insurance, or other transient hardships (transportation, time, etc.) that limit their access to healthcare services. Digital cognitive assessments (DCAs) with remote testing capabilities have emerged as an efficient and cost-effective solution. The aim of this study was to validate the reliability of BrainCheck, a platform for DCAs, when self-administered remotely.\nMethods:\nA total of 46 participants (60.9% female; age range 52-76) remotely completed a battery of six BrainCheck cognitive assessments twice on the same device (iPad\u2009=\u20098, iPhone\u2009=\u20095, laptop\u2009=\u200933): the participants self-administered in one session and were administered by a research coordinator (RC) in the other session. Thirty participants completed the self-administered session first, while 16 completed it second. The inter-session interval (ISI) varied across participants, from within the same day to 21 days apart. Testing outcomes, including the duration of time needed to complete the battery, the raw score from each assessment, and the raw overall score, were compared between the two sessions.\nResults:\nWe found moderate or good agreement between self- and RC-administered performance, with intraclass correlation ranging from 0.59 to 0.83. Results from mixed-effects modeling further confirm the non-significant difference between self- vs. RC-administered testing performance, which is independent of other factors including testing order, ISI, device, and participants' demographic characteristics.\nDiscussion:\nThese results demonstrate the feasibility of remote self-administration using BrainCheck.", "doc_items_refs": ["#/texts/177", "#/texts/178", "#/texts/179", "#/texts/180", "#/texts/181", "#/texts/182", "#/texts/183", "#/texts/184", "#/texts/185", "#/texts/186", "#/texts/187", "#/texts/188", "#/texts/189", "#/texts/190", "#/texts/191", "#/texts/192", "#/texts/193", "#/texts/194", "#/texts/195", "#/texts/196"], "page_nos": [], "uuid": "014ea58d-36e0-4153-8c98-d8dc0880dfdb"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 3, "source_chunk_idxs": [3], "num_tokens": 419, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n1 Introduction\nDementia has become a public health concern worldwide with significant social and economic impacts (\n1\n-\n4\n). Early diagnosis of dementia is crucial for timely interventions, care planning, and reducing healthcare costs (\n5\n). However, early diagnosis relies on comprehensive, evidence-based cognitive assessments, which currently requires a clinic visit and access to trained healthcare providers. On the patient side, this poses a challenge for people who live in rural or underserved areas with inadequate medical services and those who have health, economic, insurance, or other transient hardships (transportation, time, etc.) that limit their access to healthcare services (\n6\n). On the provider side, with the shortage of specialists like geriatricians and neurologists (\n7\n,\n8\n), primary care providers (PCPs) are often on the front lines of providing dementia care (\n9\n). However, many are constrained by short appointment times and limited staff (\n10\n), which makes it difficult to offer routine cognitive assessment to meet the growing needs of an aging population and the rising prevalence of the disease. These challenges highlight a critical need for a better solution to improve the accessibility of cognitive assessments in the primary care setting.\nRemote cognitive assessment has emerged as a promising solution to improve accessibility, particularly in light of the limitations on in-person care imposed by the COVID-19 pandemic. With this shift, various remote cognitive testing methods have been developed. Videoconference-based teleneuropsychology facilitates real-time interaction between patients and providers, making it possible to administer conventional in-person cognitive tests remotely under supervision (\n11\n). However, many patients may lack access to the required equipment (e.g., videoconference capable devices) or struggle with technical complications during setup. Telephone-based testing is a convenient alternative (\n12\n), but may not be suitable for patients with hearing loss. Importantly, as assistance from a healthcare professional during testing is still required, these methods are not likely to be scalable and practical.", "doc_items_refs": ["#/texts/198", "#/texts/199", "#/texts/200", "#/texts/201", "#/texts/202", "#/texts/203", "#/texts/204", "#/texts/205", "#/texts/206", "#/texts/207", "#/texts/208", "#/texts/209", "#/texts/210", "#/texts/211", "#/texts/212", "#/texts/213", "#/texts/214", "#/texts/215", "#/texts/216", "#/texts/217", "#/texts/218", "#/texts/219"], "page_nos": [], "uuid": "fe8cd0e3-c92a-4262-8565-2785d5773d37"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 4, "source_chunk_idxs": [4], "num_tokens": 493, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n1 Introduction\nRecent development in innovative digital cognitive assessments (DCAs) that allow for self-administration presents a more efficient and cost-effective solution for remote testing by eliminating the need for professional oversight during testing. Unlike traditional paper-and-pen instruments, DCAs offer automated objective scoring and instant interpretation of results. Additionally, DCAs are able to capture granular measurements (e.g., response time) and provide multiple alternate sets of stimuli to minimize practice effects (\n13\n,\n14\n). By allowing patients to complete assessments independently at home, self-administered DCAs can significantly improve the screening, diagnosis, and monitoring of dementia, alleviating the burden on healthcare providers and enhancing the efficiency of routine and repeated cognitive testing. Self-administered DCAs also offer psychometric advantages. The home environment may reduce test anxiety, leading to more accurate reflections of a patient's day-to-day cognitive abilities and enhancing ecological validity (\n15\n). These advantages position self-administered DCAs as a vital tool for addressing the growing demand for dementia care, ultimately contributing to improved patient outcomes and more efficient utilization of healthcare resources.\nBrainCheck Assess (BC-Assess) is an evidenced-based DCA developed by BrainCheck (\n16\n-\n19\n). It consists of a brief battery of standardized assessments that evaluate multiple cognitive domains relevant to cognitive decline, such as memory, attention, and executive function. Previous validation studies have demonstrated that BC-Assess can reliably and sensitively measure cognitive decline among those with early cognitive impairment (\n16\n,\n17\n). Designed for flexibility and accessibility, the full battery takes just 10-15\u2005min to complete and can be administered by clinical support staff with minimal training in a clinical office setting or self-administered at home. Unlike other self-administered DCAs such as Cantab (\n20\n), BC-Assess is web-based, device-agnostic, and mobile-responsive, allowing users to take the test on any internet-connected device at any time without downloading an app or requiring technical setup. Other platforms, including TestMyBrain (\n21\n) and BRANCH (\n22\n-\n24\n), offer similar features, but BC-Assess is built specifically for clinical use. It offers direct integration with electronic health records (EHRs), includes clinical decision support in the results report, and provides a digital care planning tool for post-diagnosis support.", "doc_items_refs": ["#/texts/220", "#/texts/221", "#/texts/222", "#/texts/223", "#/texts/224", "#/texts/225", "#/texts/226", "#/texts/227", "#/texts/228", "#/texts/229", "#/texts/230", "#/texts/231", "#/texts/232", "#/texts/233", "#/texts/234", "#/texts/235", "#/texts/236", "#/texts/237", "#/texts/238", "#/texts/239", "#/texts/240", "#/texts/241", "#/texts/242", "#/texts/243"], "page_nos": [], "uuid": "d0a661ed-de85-4aeb-80ce-2d041277d98d"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 5, "source_chunk_idxs": [5, 6], "num_tokens": 372, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n1 Introduction\nWhile BC-Assess supports remote, unsupervised testing, its effectiveness depends not only on the design of the assessments but also on the testing environment. When self-administered remotely, the lack of supervision and real-time support may make the testing vulnerable to variability in the testing environment, including issues with connectivity, device capability, and users' technical literacy (\n25\n). Within this context, this study aimed to evaluate the feasibility and reliability of BC-Assess on different types of devices by comparing individuals' performance in self-administered vs. examiner-assisted settings. Comparisons were performed in terms of the time taken to complete the assessments and their testing result in each assessment.\n\nReliability of remote self-administered digital cognitive assessments: preliminary validation study\n2 Materials and methods\n2.1 Participants\nParticipants were recruited through convenience sampling, utilizing the personal networks of BrainCheck employees. Employees were encouraged to share study details within their networks, allowing interested individuals to volunteer. Additionally, participants were invited to refer other eligible individuals who might be interested, expanding the pool through a snowball recruitment approach. Interested participants received an invitation via email to complete a screening questionnaire. Eligible participants were those aged 50 years or older who self-reported as cognitively healthy, had no motor impairments that might affect their ability to complete cognitive assessments, and had no experience with BC-Assess prior to the study. Enrollment and data collection occurred between April 9, 2020, to May 4, 2020 during the COVID-19 pandemic. A total of 46 participants (60.9% female; age range 52-76, mean 64.0, standard deviation 5.8) participated in this study.\nTable 1\nsummarizes demographic characteristics of the participants.\nTable 1\nTable 1.\nDemographics of the study sample.", "doc_items_refs": ["#/texts/244", "#/texts/245", "#/texts/246", "#/texts/249", "#/texts/250", "#/texts/251", "#/texts/252", "#/texts/253", "#/texts/254"], "page_nos": [], "uuid": "1c7bbc4a-cdf8-4795-bef2-c5536aa03ed2"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 6, "source_chunk_idxs": [7], "num_tokens": 256, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n2 Materials and methods\n2.2 Procedure\nEach participant completed BC-Assess remotely in two sessions: one where the participant self-administered the assessments and another where a research coordinator (RC) administered it. For the self-administered session, the participant received general instructions via email. During the RC-administered session, which took place amid the COVID-19 pandemic, the RC connected with the participant over the phone call or video chat. If requested, the RC provided assistance to help the participant get set up. The RC stayed on the phone or video throughout the test with the participant, answering any questions that arose. In both sessions, testing was taken on the same device for each participant, which could be either an iPad (\nn\n=\u20098), iPhone (\nn\n=\u20095), or laptop (\nn\n=\u200933), based on the participant's accessibility of devices and preference. The time interval between the two sessions varied across participants, from within the same day to 21 days apart. The order of the self-administered and RC-administered sessions was randomized across participants. Among the 46 participants, 30 completed the self-administered session first, while 16 completed it second (see\nTable 1\n).", "doc_items_refs": ["#/texts/256", "#/texts/257", "#/texts/258", "#/texts/259", "#/texts/260", "#/texts/261", "#/texts/262", "#/texts/263", "#/texts/264"], "page_nos": [], "uuid": "61d53ff8-3592-47ed-b8aa-cf1de5a1ce32"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 7, "source_chunk_idxs": [8], "num_tokens": 479, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n2 Materials and methods\n2.3 Measurements\nBC-Assess included six assessments measuring key cognitive abilities:\n(1) Immediate Recognition: Participants were shown a list of 10 words and later asked to identify which of 20 words (10 original and 10 distractors) were presented earlier, assessing short-term memory through recognition.\n(2) Trail Making A: Participants tapped numbered circles in sequential order as quickly as possible, assessing visual attention and processing speed.\n(3) Trail Making B: An extension of Trail Making A, this task requires participants to alternate between tapping numbers and letters in order (e.g., 1-A-2-B), measuring cognitive flexibility and set-switching.\n(4) Stroop: This assessment measures executive function and inhibitory control using a color-word interference paradigm. Participants were shown a target word in black and must find it in a 4\u2009\u2009\u00d7\u2009\u20093 grid where word colors varied: neutral (black), congruent (word and color match), or incongruent (word and color conflict).\n(5) Digit Symbol Substitution: Participants were shown a key pairing symbols with digits and must quickly select the digit that corresponded to a target symbol, evaluating processing speed.\n(6) Delayed Recognition: Similar to Immediate Recognition but with a time delay filled with assessments (2)-(5). Without seeing the original 10 words again, participants were asked which of 20 words (10 original and 10 distractors) were presented, testing the ability to recognize information after a delay.\nA more detailed description of these assessments can be found in our previous studies (\n17\n,\n18\n). At the beginning of each assessment, participants were presented with test instructions and engaged in an interactive practice session that included simulations of the actual test and feedback on any incorrect responses. This approach was designed to ensure participants understand the tasks well before the actual test. To minimize practice effects, BC-Assess includes built-in mechanisms for randomly generating alternative forms of each assessment. For example, in Trail Making A, the spatial positions of the numbered dots are randomized with each administration. Performance on each assessment was quantified by either accuracy- or reaction time-based measures (\nTable 2\n).\nTable 2\nTable 2.\nRaw score (RS) metric and transformed score (TS) calculation for each assessment.", "doc_items_refs": ["#/texts/266", "#/texts/267", "#/texts/268", "#/texts/269", "#/texts/270", "#/texts/271", "#/texts/272", "#/texts/273", "#/texts/274", "#/texts/275", "#/texts/276", "#/texts/277", "#/texts/278", "#/texts/279", "#/texts/280", "#/texts/281", "#/texts/282"], "page_nos": [], "uuid": "7e7826ed-6215-4614-beca-bb587693acbf"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 8, "source_chunk_idxs": [9], "num_tokens": 256, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n2 Materials and methods\n2.3 Measurements\nIn addition to assessment-level scores, a battery-level overall score was calculated to provide a composite metric summarizing each participant's cognitive performance across multiple domains. The BC-Assess raw overall score was calculated as the mean of performance scores from all assessments in the BC-Assess (except the Trail Making B), where each assessment score had been transformed from its natural range into a common range [0,100], using the formula in\nTable 2\n. Although Trail Making B remains as an important component that informs clinicians' understanding and interpretation of an individual's cognitive functioning, it is excluded from the calculation of the overall score. This is because Trail Making B is administered only if Trail Making A is completed. This design is intended to reduce frustration for individuals with cognitive difficulties by avoiding the presentation of a significantly more difficult assessment following failure on a simpler one. Trail Making B places high demands on cognitive flexibility and set-switching, making it more challenging than Trail Making A. Therefore, including Trail Making B in the overall score would lead to missing data that is conditional on prior task performance. This could introduce bias and potentially skew the interpretation of overall cognitive functioning.", "doc_items_refs": ["#/texts/283", "#/texts/284", "#/texts/285"], "page_nos": [], "uuid": "b50a3e51-1483-4a4a-bd68-31f1fd439539"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 9, "source_chunk_idxs": [10], "num_tokens": 458, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n2 Materials and methods\n2.4 Data analysis\nParticipants' performance was evaluated through the following outcomes: the total duration of time needed to complete the battery, the raw score from each individual assessment in BC-Assess defined in\nTable 2\n, and the BC-Assess raw overall score. For each outcome, descriptive statistics were calculated separately for the self- and RC-administered sessions. The reliability of self-administered BC-Assess was examined by comparing participants' performance across the two testing sessions. Bland-Altman plots were used to identify systematic differences and evaluate the variability of the differences in performance between the two sessions. Paired-samples\nt\n-tests were used for significant testing of the mean of the differences. The consistency and absolute agreement of performance between the two sessions were measured by Pearson correlation (\nr\n) and intraclass correlation (ICC). To calculate ICC, a two-way mixed-effects model with administration mode (self- vs. RC-administered) as a fixed factor and participant as a random factor was used to determine between- and within-subject components of variance. ICC was then calculated as the ratio of between-subject variance to the total variance.\nTo further investigate the factors that might influence participants' performance, we applied two complementary modeling approaches:\nApproach-1. Absolute score modeling:\na mixed-effects linear regression model was run for each outcome to analyze the effects of administration mode and participant/session characteristics on performance. In the model, administration mode (RC-administered\u2009=\u20090; self-administered\u2009=\u20091) was included as the main fixed factor of interest. Five covariates were included as additional fixed factors:\n(1) testing order (first test\u2009=\u20090; second test\u2009=\u20091): to capture any difference in the testing outcome due solely to learning effect;\n(2) time point of test measured in hours (first test\u2009=\u20090): to capture the effect of inter-session interval on the second testing outcome;\n(3) device type (with iPhone selected to be the baseline): to capture differences in the testing outcome between computer browser, iPad and the baseline;\n(4) participant's age;\n(5) participant's sex (female\u2009=\u20090; male\u2009=\u20091).", "doc_items_refs": ["#/texts/287", "#/texts/288", "#/texts/289", "#/texts/290", "#/texts/291", "#/texts/292", "#/texts/293", "#/texts/294", "#/texts/295", "#/texts/296", "#/texts/297", "#/texts/298", "#/texts/299", "#/texts/300", "#/texts/301"], "page_nos": [], "uuid": "ba77dd91-2532-4865-99e7-1ed18235e4ae"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 10, "source_chunk_idxs": [11], "num_tokens": 436, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n2 Materials and methods\n2.4 Data analysis\nInteraction terms between administration mode and each of the five covariates were also included in the model to capture the possible influence of each factor on the difference in performance between the self- and RC-administered sessions. A random intercept by participant was included in the model to account for the effects of individual differences.\nApproach-2. Relative score difference modeling:\na fixed-effects linear regression model was run where each outcome variable was the within-subject difference in score between the self- and RC-administered sessions (self-RC) for each participant. This relative score difference modeling allowed us to examine whether the magnitude of discrepancy between administration modes could be systematically explained by participant or session-related variables. The predictors included delta testing order (+1 if self-administered testing occurred first; \u22121 otherwise), delta inter-session interval in hours (self-administered time minus RC-administered time), age, sex, and device type. Although participants used the same device in both sessions, device type was retained as a predictor in this model to assess whether the magnitude of performance difference varied across platforms. Age and sex were also included, as individual characteristics may relate to different patterns of performance change, even though they remained constant across sessions.\nTogether, the two approaches provide complementary insights. Approach 1 identifies which factors, including administration mode, are associated with absolute test performance across participants, accounting for individual differences. Approach 2 isolates the within-subject change between self- and RC-administered sessions, showing which factors explain variation in mode-related performance differences.\nIn both approaches, due to data missingness and the majority of participants having more than 12 years of education, education level was excluded from analysis. All non-categorical independent variables, as well as the dependent variable, were standardized to have a mean of 0 and standard deviation of 1 prior to model fitting. Model fitting was based on the Restricted Maximum Likelihood (REML) method for the mixed-effects model, and Ordinary Least Square (OLS) method for the fixed-effects model.", "doc_items_refs": ["#/texts/302", "#/texts/303", "#/texts/304", "#/texts/305", "#/texts/306"], "page_nos": [], "uuid": "340a3f6f-c45e-4739-aa63-3199cf7006db"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 11, "source_chunk_idxs": [12], "num_tokens": 447, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n3 Results\nThe mean and standard deviation of each outcome are provided in\nTables 3\nfor each administration mode.\nTable 3\nTable 3.\nMean (standard deviation),\nt\n-statistic from paired\nt\n-tests (degrees of freedom\u2009=\u200945), Pearson correlation\nr\n, intraclass correlation ICC, and their 95% confidence interval (CI) for each testing outcome.\nThe Bland-Altman plots in\nFigure 1\nshow for each outcome the distribution of the differences in performance between the self- and RC-administered sessions as a function of their means. Results from paired-samples\nt\n-tests show non-significant differences between self- and RC-administered performance for all testing outcomes except for Stroop [\nt\n(45)\u2009=\u20092.43;\np\n=\u20090.019]. We found moderate or strong consistency and agreement between self- and RC-administered performance for most outcomes, with Pearson correlation r in the range 0.62-0.85 and intra-class correlation ICC in the range 0.59-0.83 (\nTable 3\nand\nFigure 2\n). Pearson correlation and ICC were not calculated for Immediate/Delayed Recognition because no linear relationship was observed for the outcome measures of these assessments. It is worth noting that, unlike the other assessments, performance in these assessments is based only on accuracy of responses and does not take into account reaction times. When taking reaction times into account (with performance measured as the number of correct responses per median reaction time), we observed moderate Pearson correlations of 0.79 (95% CI: 0.59-0.88) and 0.69 (0.49-0.82), and ICCs of 0.78 (95% CI: 0.64-0.87) and 0.69 (0.50-0.81), for Immediate and Delayed Recognition, respectively (\nFigure 3\n). For Stroop, although the difference between self- and RC-administered performance is significant, the difference mean of 0.1\u2005s (\nFigure 1\n) is relatively small compared with the variance of Stroop raw scores across participants, reflected by a high ICC value for this assessment.\nFigure 1", "doc_items_refs": ["#/texts/308", "#/texts/309", "#/texts/310", "#/texts/311", "#/texts/312", "#/texts/313", "#/texts/314", "#/texts/315", "#/texts/316", "#/texts/317", "#/texts/318", "#/texts/319", "#/texts/320", "#/texts/321", "#/texts/322", "#/texts/323", "#/texts/324", "#/texts/325", "#/texts/326", "#/texts/327", "#/texts/328", "#/texts/329", "#/texts/330", "#/texts/331", "#/texts/332", "#/texts/333", "#/texts/334", "#/texts/335", "#/texts/336", "#/texts/337"], "page_nos": [], "uuid": "fe9c3b45-d628-4f4e-a065-38150619c63a"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 12, "source_chunk_idxs": [13], "num_tokens": 419, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n3 Results\nFigure 1.\nBland-Altman plot for each testing outcome: differences between self- and RC-administered performance (\ny\n-axis) as a function of their means (\nx\n-axis). The orange dashed lines represent point estimates of the upper and lower limits of agreement (LoA: \u00b11.96 standard deviations of the differences). The blue dashed line represents point estimate of the mean of the differences. The shaded areas along the lines represent the corresponding 95% confidence intervals.\nFigure 2\nFigure 2.\nLinear regression (solid blue line) and the line of identity (dashed black line) for comparing self-administered performance (\ny\n-axis) against RC-administered performance (\nx\n-axis). Linear regression was not run for Immediate/Delayed Recognition due to the data showing no linear relationship.\nFigure 3\nFigure 3.\nLinear regression (solid blue line) and the line of identity (dashed black line) for comparing self-administered performance (\ny\n-axis) against RC-administered performance (\nx\n-axis) for Immediate/Delayed Recognition, where performance is measured as the number of correct responses per median reaction time.\nStandardized estimates of fixed effects obtained from the mixed-effects modeling (Approach-1) for each testing outcome data, along with their 95% confidence intervals, are provided in\nTable 4A\n. For all outcomes, we found that the main effect of administration mode and all interactions between administration mode and the other five factors were not significant. These results indicate that participants performed equally well when they self-administered and when they were administered the assessments by a professional, regardless of testing order, the time interval between the two tests, testing device, and participants' age and sex. The non-significant effect of administration mode is consistent with the results from the\nt\n-tests for all outcomes except the Stroop raw score where\nt\n-test result is significant.\nTable 4A\nTable 4A.\nStandardized estimates and their 95% confidence intervals for fixed effects obtained from mixed-effects modeling (Approach 1) of different testing outcomes.", "doc_items_refs": ["#/texts/338", "#/texts/339", "#/texts/340", "#/texts/341", "#/texts/342", "#/texts/343", "#/texts/344", "#/texts/345", "#/texts/346", "#/texts/347", "#/texts/348", "#/texts/349", "#/texts/350", "#/texts/351", "#/texts/352", "#/texts/353", "#/texts/354", "#/texts/355", "#/texts/356", "#/texts/357", "#/texts/358", "#/texts/359", "#/texts/360", "#/texts/361", "#/texts/362", "#/texts/363", "#/texts/364", "#/texts/365", "#/texts/366", "#/texts/367"], "page_nos": [], "uuid": "a9f5f8fb-d78a-448a-bef7-18cab4b4f3e5"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 13, "source_chunk_idxs": [14], "num_tokens": 410, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n3 Results\nSignificant main effects of the covariates were found for certain testing outcomes. Testing order impacts only performance in Digit Symbol Substitution (\np\n=\u20090.025), suggesting participants got better at encoding and matching the symbols or developed a better strategy to complete the task after the first test. Inter-session interval does not significantly impact performance on any assessment. The effect of device type is found for only Trail Making A. With iPhone as the reference, the significant difference in Trail Making A response time when using laptop browser (\np\n=\u20090.020), and the non-significant difference when using iPad, reflect the advantages of searching and directly tapping visual items on touch-based devices (iPad and iPhone) compared with using a touchpad on laptops. The effect of age is significant for Trail Making B (\np\n=\u20090.011), Stroop (\np\n=\u20090.003), Digit Symbol Substitution (\np\np\n=\u20090.009).\nResults from the relative score difference modeling (Approach 2) are presented in\nTable 4B\n. Testing order emerged as a significant predictor of performance differences for both Digit Symbol Substitution (\np\n=\u20090.015) and Stroop (\np\n=\u20090.001). In both cases, results from the linear regression model reflect that participants performed better during whichever session occurred second, regardless of whether it was self- or RC-administered, relative to the first, suggesting a learning or practice effect independent of administration mode. Notably, no significant effects of age, device type, or inter-session interval were found for any outcome in the difference score model. These results suggest that, beyond testing order effects for Digit Symbol Substitution and Stroop, other participant and session characteristics did not systematically influence the magnitude of performance differences between administration modes.\nTable 4B\nTable 4B.\nStandardized estimates and their 95% confidence intervals of fixed effects obtained from fixed-effects modeling (Approach 2) of different testing outcomes.", "doc_items_refs": ["#/texts/368", "#/texts/369", "#/texts/370", "#/texts/371", "#/texts/372", "#/texts/373", "#/texts/374", "#/texts/375", "#/texts/376", "#/texts/377", "#/texts/379", "#/texts/380", "#/texts/381", "#/texts/382", "#/texts/383", "#/texts/384", "#/texts/385", "#/texts/386", "#/texts/387", "#/texts/388", "#/texts/389", "#/texts/390"], "page_nos": [], "uuid": "6c40e974-f13b-4c8f-be7b-0020cd6c928a"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 14, "source_chunk_idxs": [15], "num_tokens": 305, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n4 Discussion\nIn this study, we found that self-administration of BC-Assess resulted in testing performance comparable to that obtained through professional administration. Moderate to high consistency and agreement were observed across testing outcomes between the two administration methods, with Pearson correlations and ICCs ranging from 0.62 to 0.85 and 0.59 to 0.83, respectively. Findings from the mixed-effects model further reinforce this conclusion, showing no significant main effect of administration mode for any test outcome, and no significant interactions between administration mode and participant or session characteristics. This suggests that participants, regardless of age, sex, testing device, or the order and timing of sessions, performed similarly across self- and RC-administered assessments. These findings contribute to a growing body of research demonstrating the feasibility and reliability of unsupervised DCAs, highlighting key factors such as data quality and completion rates (\n24\n,\n26\n), participant retention (\n27\n), participants' adherence and acceptability (\n28\n), test-retest reliability (\n24\n,\n27\n,\n29\n-\n32\n), and convergent validity with an in-person \"gold standard\" paper-based neuropsychological test (\n30\n-\n32\n). While previous studies have directly compared self- and examiner-administered assessments (\n24\n,\n33\n), the current study offers a more comprehensive analysis by considering the effects of various session and participant characteristics, as well as including a broader range of assessments and cognitive domains.", "doc_items_refs": ["#/texts/392", "#/texts/393", "#/texts/394", "#/texts/395", "#/texts/396", "#/texts/397", "#/texts/398", "#/texts/399", "#/texts/400", "#/texts/401", "#/texts/402", "#/texts/403", "#/texts/404", "#/texts/405", "#/texts/406", "#/texts/407", "#/texts/408", "#/texts/409", "#/texts/410", "#/texts/411", "#/texts/412", "#/texts/413", "#/texts/414", "#/texts/415", "#/texts/416"], "page_nos": [], "uuid": "3d56725f-7048-4057-894a-f12efad436bb"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 15, "source_chunk_idxs": [16], "num_tokens": 321, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n4 Discussion\nWe observed from the mixed-effects model a general negative association between age and performance across assessments that emphasized response time (see\nTable 2\n). Specifically, older age was associated with slower performance, reflected in positive standardized coefficients for Trail Making A (marginally), Trail Making B, and Stroop, where performance is evaluated solely based on response time. A similar trend was observed for Digit Symbol Substitution, where performance is measured as the number of correct responses per second; here, older age was associated with a negative standardized coefficient, indicating a decline in response efficiency with age. These associations contributed to a significant effect of age in the overall score, which integrates performance across assessments. The findings align with established evidence of age-related declines in processing speed, executive function, and attention (\n34\n-\n36\n). In contrast, the model showed no significant effect of age for Immediate and Delayed Recognition, which assess memory. While aging is known to affect memory (\n37\n,\n38\n), the recognition tasks might not have been sufficiently demanding to capture such changes among healthy participants in this study. Interestingly, despite the significant effect of age on measures of response times, the model revealed no age-related differences in the total duration to complete the full battery. One possible explanation for this is that total duration covers not only the time spent on task execution but also differences in how participants navigate the testing process. For example, while some participants may take additional time during the practice sessions to fully understand the tasks, others may proceed through them more quickly.", "doc_items_refs": ["#/texts/417", "#/texts/418", "#/texts/419", "#/texts/420", "#/texts/421", "#/texts/422", "#/texts/423", "#/texts/424", "#/texts/425", "#/texts/426", "#/texts/427"], "page_nos": [], "uuid": "5d6420dc-8855-4d04-9ce3-20f6a21da2be"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 16, "source_chunk_idxs": [17], "num_tokens": 350, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n4 Discussion\nWe did not observe a clear linear relationship between scores from the self- and RC-administered sessions for Immediate and Delayed Recognition, and thus Pearson correlation and ICC were not calculated for these assessments due to violations of linearity and homoscedasticity assumptions. This lack of linearity likely reflects a ceiling effect and limited variability on the lower end of the score spectrum, due to the use of a healthy participant sample, rather than being an artifact of the self-administered format. Supporting this interpretation, results from our previous study (\n19\n) showed that, even when these assessments were administered in person by a professional, the distributions of test-retest differences in accuracy-based scores from Immediate and Delayed Recognition were similar to those observed in the current study. Ceiling effects have been noted in memory tests, including the verbal paired associates and word list tasks, from the Wechsler Memory Scales (WMS) (\n39\n), the Rey Auditory Verbal Learning Test (RAVLT) (\n40\n), and the California Verbal Learning Test (CVLT) (\n41\n), where performance variability is often limited among high-functioning individuals (\n42\n). These effects can obscure the ability to detect subtle individual differences and make it difficult to assess the true reliability of memory tasks in healthy populations, as the reduced score range complicates consistency evaluation. In this study, when response time was incorporated into the performance metric (i.e., number of correct responses per median reaction time), we observed strong correlations and ICCs for both assessments (\nFigure 3\n). Including reaction time introduces a continuous and more sensitive measure of individual differences, which increases variability across participants (\n43\n).", "doc_items_refs": ["#/texts/428", "#/texts/429", "#/texts/430", "#/texts/431", "#/texts/432", "#/texts/433", "#/texts/434", "#/texts/435", "#/texts/436", "#/texts/437", "#/texts/438", "#/texts/439", "#/texts/440", "#/texts/441", "#/texts/442"], "page_nos": [], "uuid": "6b5d58ac-2ed3-4cb7-8e05-db82fc82c9fc"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 17, "source_chunk_idxs": [18], "num_tokens": 321, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n4 Discussion\nPractice effects were observed for the Digit Symbol Substitution assessment in both the absolute and the relative score difference approaches, and for the Stroop assessment only in the relative approach. These findings highlight the role of task familiarity and strategy adaptation, particularly in assessments involving processing speed and executive function. The discrepancy in the Stroop results suggests that the relative score approach may be more sensitive to subtle within-subject changes, as it isolates the effect of testing order by removing between-subject variability. In contrast, the mixed-effects model incorporates both within- and between-subject sources of variation, which may reduce sensitivity to smaller effects that are consistent at the individual level but not large enough to be detected across participants. Practice effects may explain why a significant Stroop difference between self- and RC-administered sessions was detected in the paired-sample\nt\n-test, particularly given the imbalance in administration order where 30 of the 46 participants completed the self-administered session first. As the\nt\n-tests included different components of variance, observed differences between self- and RC-administered sessions in this analysis may be influenced by a combination of factors, including practice effects. Conversely, although practice effects were significant in both models for the Digit Symbol Substitution, no significant difference emerged from the paired\nt\n-test for this assessment. Closer inspection of individual data showed that Stroop practice effects were pronounced for many participants, potentially driving the group-level results. In contrast, practice effects for Digit Symbol Substitution appeared more modest and consistent across individuals.", "doc_items_refs": ["#/texts/443", "#/texts/444", "#/texts/445", "#/texts/446", "#/texts/447", "#/texts/448", "#/texts/449"], "page_nos": [], "uuid": "16c72656-8d67-4e5d-b05d-32d3aab420b5"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 18, "source_chunk_idxs": [19], "num_tokens": 464, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n4 Discussion\nAlthough BrainCheck utilizes alternative test forms in all assessments to reduce content-specific practice effects, procedural learning, such as adapting to task structure, navigation, or timing, cannot be entirely eliminated. This is especially relevant for tasks like Digit Symbol Substitution (\n44\n), where participants may develop more efficient encoding strategies with repeated exposure, and Stroop (\n45\n,\n46\n), where familiarity may enhance cognitive control and reduce interference. Practically, these findings highlight the importance of accounting for practice effects in repeated cognitive assessments. Without appropriate controls, familiarity with the task can lead to inflated improvements or obscure real cognitive declines. The fact that practice effects emerged across both self- and RC-administered sessions further supports the comparability of administration modes, suggesting that exposure, rather than supervision, drives these gains. Future studies with larger samples and repeated assessments across multiple time points are needed to better estimate the magnitude and timing of practice effects, and inform strategies to mitigate practice effects, such as optimizing test schedules and applying statistical corrections.\nFor the Trail Making A, performance differed across device types. Participants using touchscreen devices (iPad, iPhone) completed the task more quickly than those using laptops. This difference may be due to the task's requirement for rapid, sequential selection of spatially dispersed targets, where the touchscreen interface provides a more direct input method, in contrast to the slower, more mechanical navigation with a trackpad or mouse. Interestingly, no device effects were observed for other assessments such as Trail Making B, Digit Symbol Substitution, or Stroop. For Trail Making B, its greater cognitive demands might have reduced the relative influence of input mechanics on performance. The other assessments involve more centralized and repetitive inputs, where the type of input device may have less impact on performance, thus explaining the absence of a device effect. While the observed device-related variability is not a concern in the current within-subject design, where each participant used the same device in both sessions, it highlights the importance of applying device-specific corrections in studies involving direct comparisons between devices, especially for tasks that involve spatial or motor coordination. To support this, BrainCheck employs device-specific normative data to ensure valid performance evaluation and accurate comparisons across devices.", "doc_items_refs": ["#/texts/450", "#/texts/451", "#/texts/452", "#/texts/453", "#/texts/454", "#/texts/455", "#/texts/456", "#/texts/457"], "page_nos": [], "uuid": "5fa5ed02-c18c-4d95-a9d2-6965b115b335"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 19, "source_chunk_idxs": [20], "num_tokens": 462, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n4 Discussion\nPerformance in cognitive testing when self-administered remotely should not be immediately assumed to match that when administered in a provider office setting. Compared with in-person testing, remote and self-administered testing may introduce greater variability due to uncontrolled factors in the testing environment, such as unexpected interruptions, and the lack of in-time support. To ensure accuracy and validity, it is important to evaluate how well the assessments withstand such variability and how intuitive they are for users without assistance from a test administrator. Encouragingly, BC-Assess demonstrated to be highly reliable when self-administered remotely. The moderate to high consistency and agreement between self- and RC-administered testing sessions observed in this study are in similar ranges with previous evaluations of test-retest reliability, both in supervised settings for BrainCheck (\n18\n) and in unsupervised settings for other DCA tools such as Cantab and Neurotrack Cognitive Battery (\n29\n,\n47\n,\n48\n). Several design features of BC-Assess may contribute to its reliability in remote and self-administered testing, although these remain untested hypotheses and should be investigated further in future work. First, the adaptation of traditional paper-based tests into a digital format might help reduce errors that could arise in an unsupervised setting. For example, in the BrainCheck Trail Making A/B test, participants tap or click dots rather than drawing lines to connect them, as in the paper version. This digital format may help limit the variability introduced by more open-ended actions like freehand drawing, potentially making the task clearer and easier to perform accurately. Second, BC-Assess typically takes 10-15\u2005min to complete. This shorter format may make it easier for individuals to find a quiet, uninterrupted time to finish the test. Lastly, the clear instructions and interactive practice session at the start of each test allow participants to practice as needed to fully understand the task before the actual test, helping reduce misunderstandings or mistakes during the actual test. Future studies that quantify usability, task comprehension, and within-subject variability could help determine whether and how these design features contribute to reliability in self-administered cognitive testing.", "doc_items_refs": ["#/texts/458", "#/texts/459", "#/texts/460", "#/texts/461", "#/texts/462", "#/texts/463", "#/texts/464", "#/texts/465", "#/texts/466"], "page_nos": [], "uuid": "eb7d250b-ff4b-423c-b751-46015aecb94e"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 20, "source_chunk_idxs": [21], "num_tokens": 294, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n4 Discussion\nIn this study, the participants were allowed to use their preferred devices to take the tests. The majority 71.7% of participants used laptops while only 10.9% and 17.4% used iPads and iPhones. At the time of data collection, BrainCheck DCAs on mobile devices were limited to iOS systems and required an app download. The low usage of iPads and iPhones could be due to the limited ownership of these iOS devices among the participants. Participants with limited digital skills or those who preferred not to download another app on their mobile device might end up choosing to use a laptop where BrainCheck could be run directly on a web browser. For optimal accessibility, it is essential that DCAs are compatible with a wide range of devices and simple in setup, allowing individuals of diverse backgrounds and levels of digital literacy to complete them with ease. Since the study, BC-Assess has been expanded and optimized to support any devices with a browser-whether a smartphone, tablet, or laptop-enabling users to start an assessment simply by clicking a link sent via email or text message, without the need to download an app. Based on the promising results from this pilot study and the recent improvements, our future studies will evaluate the usability and feasibility of BrainCheck DCAs for remote self-administered testing across diverse populations and devices in real-world settings.", "doc_items_refs": ["#/texts/467"], "page_nos": [], "uuid": "7e8ac8a1-82a7-4b2d-b9f8-94d76a249500"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 21, "source_chunk_idxs": [22], "num_tokens": 317, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\n4 Discussion\nWhile the results are promising, this study has several limitations that should be considered when interpreting the findings. First, the sample size in this pilot study is small, and inter-session intervals varied significantly between participants. Additionally, the study only examined two time points; a longitudinal study would be needed to provide further insights into the feasibility of using BC-Assess for long-term monitoring of cognitive health. Second, the study did not include participants with cognitive impairments, who may struggle to complete the assessments independently. This limits the generalizability of our findings to more vulnerable populations who might be the primary beneficiaries of cognitive assessments. Third, the study sample mainly consisted of participants with higher education levels, who are also likely to have higher digital literacy. Therefore, the results may not be applicable to individuals with lower education levels or limited digital literacy. Furthermore, the sample was drawn from a research setting rather than a real-world clinical environment, where contextual factors such as variability in support and test-taking conditions may influence feasibility and performance. Future studies with larger and more diverse samples are certainly needed to further assess the feasibility of self-administration in broader demographic settings.\nDespite the limitations, this pilot study provides initial evidence demonstrating the feasibility and reliability of BC-Assess as a tool for remote cognitive evaluations. Effective use of remote and self-administered DCAs has the potential to improve the accessibility of cognitive care, particularly for rural and underserved populations, thereby addressing critical gaps in the current healthcare landscape.", "doc_items_refs": ["#/texts/468", "#/texts/469"], "page_nos": [], "uuid": "a3f1ea9a-2ac0-48d4-bef8-62f188a87f21"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 22, "source_chunk_idxs": [23, 24, 25, 26, 27, 28, 29], "num_tokens": 512, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nData availability statement\nThe datasets presented in this article are not readily available because the dataset used in this study is proprietary to BrainCheck Inc. and cannot be shared due to confidentiality agreements. As such, the data is not available for public distribution or by request to the corresponding author. However, interested parties may contact BrainCheck Inc. directly for inquiries related to access or collaboration opportunities involving the dataset. Requests to access the datasets should be directed to Bin Huang, YmluQGJyYWluY2hlY2suY29t .\n\nReliability of remote self-administered digital cognitive assessments: preliminary validation study\nEthics statement\nThis study, involving human participants, was reviewed and approved by Solutions IRB (1MAY15-93), and written informed consent was obtained from all participants.\n\nReliability of remote self-administered digital cognitive assessments: preliminary validation study\nAuthor contributions\nDH: Formal analysis, Methodology, Writing - original draft, Writing - review &amp; editing. SY: Data curation, Writing - review &amp; editing. RH: Writing - review &amp; editing, Conceptualization. MP: Writing - review &amp; editing. BH: Writing - original draft, Writing - review &amp; editing, Conceptualization.\n\nReliability of remote self-administered digital cognitive assessments: preliminary validation study\nFunding\nThe author(s) declare that financial support was received for the research and/or publication of this article. The authors declare that this study received funding from BrainCheck, Inc. The funder was not involved in the study design, collection, analysis, interpretation of data, the writing of this article or the decision to submit it for publication.\n\nReliability of remote self-administered digital cognitive assessments: preliminary validation study\nConflict of interest\nDH, SY, RH, MP and BH report receiving salaries and stock options from BrainCheck, Inc.\n\nReliability of remote self-administered digital cognitive assessments: preliminary validation study\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the creation of this manuscript.\n\nReliability of remote self-administered digital cognitive assessments: preliminary validation study\nPublisher's note\nAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.", "doc_items_refs": ["#/texts/471", "#/texts/473", "#/texts/475", "#/texts/477", "#/texts/479", "#/texts/481", "#/texts/483"], "page_nos": [], "uuid": "c62a0559-b961-4f5f-baca-1e6d1afdcb79"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 23, "source_chunk_idxs": [30], "num_tokens": 484, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\n1. Kelley AS, McGarry K, Gorges R, Skinner JS. The burden of health care costs for patients with dementia in the last 5 years of life.\nAnn Intern Med\n. (2015) 163(10):729-36. doi: 10.7326/M15-0381\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n2. Alzheimer's Association. 2016 Alzheimer's disease facts and figures.\nAlzheimers Dement\n. (2016) 12(4):459-509. doi: 10.1016/j.jalz.2016.03.001\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n3. Oba H, Kadoya Y, Okamoto H, Matsuoka T, Abe Y, Shibata K, et al. The economic burden of dementia: evidence from a survey of households of people with dementia and their caregivers.\nInt J Environ Res Public Health\n. (2021) 18(5):2717. doi: 10.3390/ijerph18052717\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n4. Hurd MD, Martorell P, Delavande A, Mullen KJ, Langa KM. Monetary costs of dementia in the United States.\nN Engl J Med\n. (2013) 368(14):1326-34. doi: 10.1056/NEJMsa1204629\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n5. Prince M, Bryce DR, Ferri DC. World Alzheimer Report 2011: The benefits of early diagnosis and intervention.\nGoogle Scholar\n6. Health Resources &amp; Services Administration. Shortage areas. Available online at:\nhttps://data.hrsa.gov/topics/health-workforce/shortage-areas?tab=muapHeader\n(Accessed March 22, 2024).\nGoogle Scholar\n7. KFF. Primary Care Health Professional Shortage Areas (HPSAs). KFF. Available online at:\nhttps://www.kff.org/other/state-indicator/primary-care-health-professional-shortage-areas-hpsas/\n(Accessed March 22, 2024).\nGoogle Scholar", "doc_items_refs": ["#/texts/485", "#/texts/486", "#/texts/487", "#/texts/488", "#/texts/489", "#/texts/490", "#/texts/491", "#/texts/492", "#/texts/493", "#/texts/494", "#/texts/495", "#/texts/496", "#/texts/497", "#/texts/498", "#/texts/499", "#/texts/500", "#/texts/501", "#/texts/502", "#/texts/503", "#/texts/504", "#/texts/505", "#/texts/506", "#/texts/507", "#/texts/508", "#/texts/509", "#/texts/510", "#/texts/511", "#/texts/512", "#/texts/513", "#/texts/514", "#/texts/515", "#/texts/516", "#/texts/517", "#/texts/518", "#/texts/519", "#/texts/520", "#/texts/521", "#/texts/522", "#/texts/523", "#/texts/524", "#/texts/525", "#/texts/526"], "page_nos": [], "uuid": "b97d6be0-dbea-4090-a70b-2927404a6669"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 24, "source_chunk_idxs": [31], "num_tokens": 456, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\n8. Alzheimer's Association. 2020 Alzheimer's disease facts and figures.\nAlzheimers Dement\n. (2020) 16(3):391-460. Section 5: Caregiving. p. 423-9. doi: 10.1002/alz.12068\nCrossref Full Text\n|\nGoogle Scholar\n9. Alzheimer's Association. 2020 Alzheimer's disease facts and figures.\nAlzheimers Dement\n. (2020) 16(3):391-460. Section 7: Special Report \"On the Front Lines: Primary Care Physicians and Alzheimer's Care in America\". p. 455-60. doi: 10.1002/alz.12068.\nCrossref Full Text\n|\nGoogle Scholar\n10. Neprash HT, Mulcahy JF, Cross DA, Gaugler JE, Golberstein E, Ganguli I. Association of primary care visit length with potentially inappropriate prescribing.\nJAMA Health Forum\n. (2023) 4(3):e230052. doi: 10.1001/jamahealthforum.2023.0052\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n11. Cullum CM, Hynan LS, Grosch M, Parikh M, Weiner MF. Teleneuropsychology: evidence for video teleconference-based neuropsychological assessment.\nJ Int Neuropsychol Soc\n. (2014) 20(10):1028-33. doi: 10.1017/S1355617714000873\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n12. Castanho TC, Amorim L, Zihl J, Palha JA, Sousa N, Santos NC. Telephone-based screening tools for mild cognitive impairment and dementia in aging studies: a review of validated instruments.\nFront Aging Neurosci\n. (2014) 6:16. doi: 10.3389/fnagi.2014.00016\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar", "doc_items_refs": ["#/texts/527", "#/texts/528", "#/texts/529", "#/texts/530", "#/texts/531", "#/texts/532", "#/texts/533", "#/texts/534", "#/texts/535", "#/texts/536", "#/texts/537", "#/texts/538", "#/texts/539", "#/texts/540", "#/texts/541", "#/texts/542", "#/texts/543", "#/texts/544", "#/texts/545", "#/texts/546", "#/texts/547", "#/texts/548", "#/texts/549", "#/texts/550", "#/texts/551", "#/texts/552", "#/texts/553", "#/texts/554", "#/texts/555", "#/texts/556", "#/texts/557", "#/texts/558", "#/texts/559", "#/texts/560", "#/texts/561", "#/texts/562"], "page_nos": [], "uuid": "c2bac5ef-61e9-4dd5-9919-6c862eb52fee"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 25, "source_chunk_idxs": [32], "num_tokens": 481, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\n13. Beglinger L, Gaydos B, Tangphaodaniels O, Duff K, Kareken D, Crawford J, et al. Practice effects and the use of alternate forms in serial neuropsychological testing.\nArch Clin Neuropsychol\n. (2005) 20(4):517-29. doi: 10.1016/j.acn.2004.12.003\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n14. Goldberg TE, Harvey PD, Wesnes KA, Snyder PJ, Schneider LS. Practice effects due to serial cognitive assessment: implications for preclinical Alzheimer's disease randomized controlled trials.\nAlzheimers Dement Diagn Assess Dis Monit\n. (2015) 1(1):103-11. doi: 10.1016/j.dadm.2014.11.003\nCrossref Full Text\n|\nGoogle Scholar\n15. Chaytor N, Schmitter-Edgecombe M. The ecological validity of neuropsychological tests: a review of the literature on everyday cognitive skills.\nNeuropsychol Rev\n. (2003) 13(4):181-97. doi: 10.1023/B:NERV.0000009483.91468.fb\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n16. Ye S, Sun K, Huynh D, Phi HQ, Ko B, Huang B, et al. A computerized cognitive test battery for detection of dementia and mild cognitive impairment: instrument validation study.\nJMIR Aging\n. (2022) 5(2):e36825. doi: 10.2196/36825\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n17. Groppell S, Soto-Ruiz KM, Flores B, Dawkins W, Smith I, Eagleman DM, et al. A rapid, Mobile neurocognitive screening test to aid in identifying cognitive impairment and dementia (BrainCheck): cohort study.\nJMIR Aging\n. (2019) 2(1):e12615. doi: 10.2196/12615\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar", "doc_items_refs": ["#/texts/563", "#/texts/564", "#/texts/565", "#/texts/566", "#/texts/567", "#/texts/568", "#/texts/569", "#/texts/570", "#/texts/571", "#/texts/572", "#/texts/573", "#/texts/574", "#/texts/575", "#/texts/576", "#/texts/577", "#/texts/578", "#/texts/579", "#/texts/580", "#/texts/581", "#/texts/582", "#/texts/583", "#/texts/584", "#/texts/585", "#/texts/586", "#/texts/587", "#/texts/588", "#/texts/589", "#/texts/590", "#/texts/591", "#/texts/592", "#/texts/593", "#/texts/594", "#/texts/595", "#/texts/596", "#/texts/597", "#/texts/598", "#/texts/599", "#/texts/600"], "page_nos": [], "uuid": "a7014211-5885-40f8-875c-54f26b28f6f5"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 26, "source_chunk_idxs": [33], "num_tokens": 489, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\n18. Yang S, Flores B, Magal R, Harris K, Gross J, Ewbank A, et al. Diagnostic accuracy of tablet-based software for the detection of concussion.\nPLoS One\n. (2017) 12(7):e0179352. doi: 10.1371/journal.pone.0179352\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n19. Huynh D, Sun K, Ghomi RH, Huang B. Comparing psychometric characteristics of a computerized cognitive test (BrainCheck assess) against the Montreal cognitive assessment.\nFront Psychol\n. (2024) 15:1428560. doi: 10.3389/fpsyg.2024.1428560\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n20. Backx R, Skirrow C, Dente P, Barnett JH, Cormack FK. Comparing web-based and lab-based cognitive assessment using the cambridge neuropsychological test automated battery: a within-subjects counterbalanced study (Preprint) (2019). Available online at:\nhttp://preprints.jmir.org/preprint/16792\n(Accessed April 25, 2025).\nGoogle Scholar\n21. Singh S, Strong RW, Jung L, Li FH, Grinspoon L, Scheuer LS, et al. The TestMyBrain digital neuropsychology toolkit: development and psychometric characteristics.\nJ Clin Exp Neuropsychol\n. (2021) 43(8):786-95. doi: 10.1080/13803395.2021.2002269\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n22. Weizenbaum EL, Soberanes D, Hsieh S, Molinare CP, Buckley RF, Betensky RA, et al. Capturing learning curves with the multiday Boston remote assessment of neurocognitive health (BRANCH): feasibility, reliability, and validity.\nNeuropsychology\n. (2024) 38(2):198-210. doi: 10.1037/neu0000933\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar", "doc_items_refs": ["#/texts/601", "#/texts/602", "#/texts/603", "#/texts/604", "#/texts/605", "#/texts/606", "#/texts/607", "#/texts/608", "#/texts/609", "#/texts/610", "#/texts/611", "#/texts/612", "#/texts/613", "#/texts/614", "#/texts/615", "#/texts/616", "#/texts/617", "#/texts/618", "#/texts/619", "#/texts/620", "#/texts/621", "#/texts/622", "#/texts/623", "#/texts/624", "#/texts/625", "#/texts/626", "#/texts/627", "#/texts/628", "#/texts/629", "#/texts/630", "#/texts/631", "#/texts/632", "#/texts/633", "#/texts/634", "#/texts/635", "#/texts/636"], "page_nos": [], "uuid": "d2b6af37-83fe-4a58-90fe-e1391cfad36b"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 27, "source_chunk_idxs": [34], "num_tokens": 501, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\n23. Weizenbaum EL, Hsieh S, Molinare C, Soberanes D, Christiano C, Viera AM, et al. Validation of the multi-day Boston remote assessment of neurocognitive health (BRANCH) among cognitively impaired &amp; unimpaired older adults.\nJ Prev Alzheimers Dis\n. (2025) 12(3):100057. doi: 10.1016/j.tjpad.2025.100057\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n24. Papp KV, Samaroo A, Chou HC, Buckley R, Schneider OR, Hsieh S, et al. Unsupervised mobile cognitive testing for use in preclinical Alzheimer's disease.\nAlzheimers Dement Amst Neth\n. (2021) 13(1):e12243. doi: 10.1002/dad2.12243\nCrossref Full Text\n|\nGoogle Scholar\n25. Polk SE, \u00d6hman F, Hassenstab J, K\u00f6nig A, Papp KV, Sch\u00f6ll M, et al. A scoping review of remote and unsupervised digital cognitive assessments in preclinical Alzheimer's disease (2024). Available online at:\nhttp://medrxiv.org/lookup/doi/10.1101/2024.09.25.24314349\n(Accessed February 3, 2025).\nGoogle Scholar\n26. Balit N, Sun S, Zhang Y, Sharp M. Online unsupervised performance-based cognitive testing: a feasible and reliable approach to scalable cognitive phenotyping of Parkinson's patients.\nParkinsonism Relat Disord\n. (2024) 129:107183. doi: 10.1016/j.parkreldis.2024.107183\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n27. Berron D, Ziegler G, Vieweg P, Billette O, G\u00fcsten J, Grande X, et al. Feasibility of digital memory assessments in an unsupervised and remote study setting.\nFront Digit Health\n. (2022) 4:892997. doi: 10.3389/fdgth.2022.892997", "doc_items_refs": ["#/texts/637", "#/texts/638", "#/texts/639", "#/texts/640", "#/texts/641", "#/texts/642", "#/texts/643", "#/texts/644", "#/texts/645", "#/texts/646", "#/texts/647", "#/texts/648", "#/texts/649", "#/texts/650", "#/texts/651", "#/texts/652", "#/texts/653", "#/texts/654", "#/texts/655", "#/texts/656", "#/texts/657", "#/texts/658", "#/texts/659", "#/texts/660", "#/texts/661", "#/texts/662", "#/texts/663", "#/texts/664", "#/texts/665"], "page_nos": [], "uuid": "00b91a2b-f237-4eae-99c6-5eb6be68e264"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 28, "source_chunk_idxs": [35], "num_tokens": 427, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n28. Thompson LI, Harrington KD, Roque N, Strenger J, Correia S, Jones RN, et al. A highly feasible, reliable, and fully remote protocol for mobile app-based cognitive assessment in cognitively healthy older adults.\nAlzheimers Dement Amst Neth\n. (2022) 14(1):e12283. doi: 10.1002/dad2.12283\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n29. Erkkinen MG, Butler M, Brown R, Hobbs M, Gabelle A, Becker A, et al. Reliability of unsupervised digital cognitive assessment in a large adult population across the aging lifespan from INTUITION: a brain health study (S8.009).\nNeurology\n. (2023) 100(17 Suppl 2):3379. doi: 10.1212/WNL.0000000000203227\nCrossref Full Text\n|\nGoogle Scholar\n30. Kochan NA, Heffernan M, Valenzuela M, Sachdev PS, Lam BCP, Fiatarone Singh M, et al. Reliability, validity, and user-experience of remote unsupervised computerized neuropsychological assessments in community-living 55- to 75-year-olds.\nJ Alzheimers Dis\n. (2022) 90(4):1629-45. doi: 10.3233/JAD-220665\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n31. Morrissey S, Gillings R, Hornberger M. Feasibility and reliability of online vs in-person cognitive testing in healthy older people.\nPLoS One\n. (2024) 19(8):e0309006. doi: 10.1371/journal.pone.0309006\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar", "doc_items_refs": ["#/texts/666", "#/texts/667", "#/texts/668", "#/texts/669", "#/texts/670", "#/texts/671", "#/texts/672", "#/texts/673", "#/texts/674", "#/texts/675", "#/texts/676", "#/texts/677", "#/texts/678", "#/texts/679", "#/texts/680", "#/texts/681", "#/texts/682", "#/texts/683", "#/texts/684", "#/texts/685", "#/texts/686", "#/texts/687", "#/texts/688", "#/texts/689", "#/texts/690", "#/texts/691", "#/texts/692", "#/texts/693", "#/texts/694", "#/texts/695", "#/texts/696", "#/texts/697", "#/texts/698", "#/texts/699", "#/texts/700"], "page_nos": [], "uuid": "249c37fe-d07d-46a3-bd8c-83f508749cf8"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 29, "source_chunk_idxs": [36], "num_tokens": 449, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\n32. \u00d6hman F, Berron D, Papp KV, Kern S, Skoog J, Hadarsson Bodin T, et al. Unsupervised mobile app-based cognitive testing in a population-based study of older adults born 1944.\nFront Digit Health\n. (2022) 4:933265. doi: 10.3389/fdgth.2022.933265\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n33. Atkins AS, Kraus MS, Welch M, Yuan Z, Stevens H, Welsh-Bohmer KA, et al. Remote self-administration of digital cognitive tests using the brief assessment of cognition: feasibility, reliability, and sensitivity to subjective cognitive decline.\nFront Psychiatry\n. (2022) 13:910896. doi: 10.3389/fpsyt.2022.910896\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n34. Verhaeghen P, Cerella J. Aging, executive control, and attention: a review of meta-analyses.\nNeurosci Biobehav Rev\n. (2002) 26(7):849-57. doi: 10.1016/S0149-7634(02)00071-4\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n35. Gilsoul J, Simon J, Hogge M, Collette F. Do attentional capacities and processing speed mediate the effect of age on executive functioning?\nAging Neuropsychol Cogn\n. (2019) 26(2):282-317. doi: 10.1080/13825585.2018.1432746\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n36. Harada CN, Natelson Love MC, Triebel KL. Normal cognitive aging.\nClin Geriatr Med\n. (2013) 29(4):737-52. doi: 10.1016/j.cger.2013.07.002\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar", "doc_items_refs": ["#/texts/701", "#/texts/702", "#/texts/703", "#/texts/704", "#/texts/705", "#/texts/706", "#/texts/707", "#/texts/708", "#/texts/709", "#/texts/710", "#/texts/711", "#/texts/712", "#/texts/713", "#/texts/714", "#/texts/715", "#/texts/716", "#/texts/717", "#/texts/718", "#/texts/719", "#/texts/720", "#/texts/721", "#/texts/722", "#/texts/723", "#/texts/724", "#/texts/725", "#/texts/726", "#/texts/727", "#/texts/728", "#/texts/729", "#/texts/730", "#/texts/731", "#/texts/732", "#/texts/733", "#/texts/734", "#/texts/735", "#/texts/736", "#/texts/737", "#/texts/738", "#/texts/739", "#/texts/740"], "page_nos": [], "uuid": "bddf06db-d804-4ad3-948b-298a0a73a174"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 30, "source_chunk_idxs": [37], "num_tokens": 493, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\n37. Nyberg L, L\u00f6vd\u00e9n M, Riklund K, Lindenberger U, B\u00e4ckman L. Memory aging and brain maintenance.\nTrends Cogn Sci\n. (2012) 16(5):292-305. doi: 10.1016/j.tics.2012.04.005\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n38. Salthouse TA. Memory aging from 18 to 80.\nAlzheimer Dis Assoc Disord\n. (2003) 17(3):162-7. doi: 10.1097/00002093-200307000-00008\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n39. Holdnack JA, Drozdick LW. Using WAIS-IV with WMS-IV. In: Weiss LG, Saklofske DH, Holdnack JA, editors.\nWAIS-IV Clinical Use and Interpretation\n. San Diego, CA: Elsevier Academic Press (2010). p. 237-83. doi: 10.1016/B978-0-12-375035-8.10009-6\nCrossref Full Text\n|\nGoogle Scholar\n40. Est\u00e9vez-Gonz\u00e1lez A, Kulisevsky J, Boltes A, Oterm\u00edn P, Garc\u00eda-S\u00e1nchez C. Rey verbal learning test is a useful tool for differential diagnosis in the preclinical phase of Alzheimer's disease: comparison with mild cognitive impairment and normal aging.\nInt J Geriatr Psychiatry\n. (2003) 18(11):1021-8. doi: 10.1002/gps.1010\nCrossref Full Text\n|\nGoogle Scholar\n41. Delis DC, Kramer JH, Kaplan E, Ober BA.\nCalifornia Verbal Learning Test-Second Edition\n. San Diego, CA: Elsevier Academic Press (2016). doi: 10.1037/t15072-000\nCrossref Full Text\n|\nGoogle Scholar\n42. Uttl B. Measurement of individual differences: lessons from memory assessment in research and clinical practice.\nPsychol Sci\n. (2005) 16(6):460-7. doi: 10.1111/j.0956-7976.2005.01557.x\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar", "doc_items_refs": ["#/texts/741", "#/texts/742", "#/texts/743", "#/texts/744", "#/texts/745", "#/texts/746", "#/texts/747", "#/texts/748", "#/texts/749", "#/texts/750", "#/texts/751", "#/texts/752", "#/texts/753", "#/texts/754", "#/texts/755", "#/texts/756", "#/texts/757", "#/texts/758", "#/texts/759", "#/texts/760", "#/texts/761", "#/texts/762", "#/texts/763", "#/texts/764", "#/texts/765", "#/texts/766", "#/texts/767", "#/texts/768", "#/texts/769", "#/texts/770", "#/texts/771", "#/texts/772", "#/texts/773", "#/texts/774", "#/texts/775", "#/texts/776", "#/texts/777", "#/texts/778", "#/texts/779", "#/texts/780", "#/texts/781", "#/texts/782"], "page_nos": [], "uuid": "d36a3d97-8e65-46cf-8182-8756b890cb49"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 31, "source_chunk_idxs": [38], "num_tokens": 457, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\n43. Hartle L, Martorelli M, Balboni G, Souza R, Charchat-Fichman H. Diagnostic accuracy of CompCog: reaction time as a screening measure for mild cognitive impairment.\nArq Neuropsiquiatr\n. (2022) 80(6):570-9. doi: 10.1590/0004-282x-anp-2021-0099\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n44. Hinton-Bayre A, Geffen G. Comparability, reliability, and practice effects on alternate forms of the digit symbol substitution and symbol digit modalities tests.\nPsychol Assess\n. (2005) 17(2):237-41. doi: 10.1037/1040-3590.17.2.237\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n45. Dulaney CL, Rogers WA. Mechanisms underlying reduction in stroop interference with practice for young and old adults.\nJ Exp Psychol Learn Mem Cogn\n. (1994) 20(2):470-84. doi: 10.1037/0278-7393.20.2.470\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n46. Davidson DJ, Zacks RT, Williams CC. Stroop interference, practice, and aging.\nNeuropsychol Dev Cogn B\n. (2003) 10(2):85-98. doi: 10.1076/anec.10.2.85.14463\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\n47. Berron D, Glanz W, Clark L, Basche K, Grande X, G\u00fcsten J, et al. A remote digital memory composite to detect cognitive impairment in memory clinic samples in unsupervised settings using mobile devices.\nNpj Digit Med\n. (2024) 7(1):79. doi: 10.1038/s41746-024-00999-9\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar", "doc_items_refs": ["#/texts/783", "#/texts/784", "#/texts/785", "#/texts/786", "#/texts/787", "#/texts/788", "#/texts/789", "#/texts/790", "#/texts/791", "#/texts/792", "#/texts/793", "#/texts/794", "#/texts/795", "#/texts/796", "#/texts/797", "#/texts/798", "#/texts/799", "#/texts/800", "#/texts/801", "#/texts/802", "#/texts/803", "#/texts/804", "#/texts/805", "#/texts/806", "#/texts/807", "#/texts/808", "#/texts/809", "#/texts/810", "#/texts/811", "#/texts/812", "#/texts/813", "#/texts/814", "#/texts/815", "#/texts/816", "#/texts/817", "#/texts/818", "#/texts/819", "#/texts/820", "#/texts/821", "#/texts/822"], "page_nos": [], "uuid": "bb0dce93-890f-46cd-baa3-785dd6ba9ee9"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 32, "source_chunk_idxs": [39], "num_tokens": 495, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\n48. Myers JR, Glenn JM, Madero EN, Anderson J, Mak-McCully R, Gray M, et al. Asynchronous remote assessment for cognitive impairment: reliability verification of the neurotrack cognitive battery.\nJMIR Form Res\n. (2022) 6(2):e34237. doi: 10.2196/34237\nPubMed Abstract\n|\nCrossref Full Text\n|\nGoogle Scholar\nKeywords: dementia diagnosis, digital cognitive assessment, self-administered testing, BrainCheck, reliability\nCitation: Huynh D, Ye S, Hosseini Ghomi R, Patterson M and Huang B (2025) Reliability of remote self-administered digital cognitive assessments: preliminary validation study.\nFront. Digit. Health\n7:1571053. doi: 10.3389/fdgth.2025.1571053\nReceived: 4 February 2025; Accepted: 19 June 2025; Published: 3 July 2025.\nEdited by:\nScott Sperling\n, Cleveland Clinic, United States\nReviewed by:\nSadouanouan Malo\n, Nazi Boni University, Burkina Faso\nSarah E. Polk\n, Helmholtz Association of German Research Centers (HZ), Germany\nCopyright: \u00a9 2025 Huynh, Ye, Hosseini Ghomi, Patterson and Huang. This is an open-access article distributed under the terms of the\nCreative Commons Attribution License (CC BY)\n. The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.\n*Correspondence: Bin Huang, YmluQGJyYWluY2hlY2suY29t\nDisclaimer: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher.\nDownload article\n- Download PDF\n- ReadCube\n- epub\n- XML\nShare on\nExport citation\n- EndNote\n- Reference Manager\n- Simple Text file\n- BibTex", "doc_items_refs": ["#/texts/823", "#/texts/824", "#/texts/825", "#/texts/826", "#/texts/827", "#/texts/828", "#/texts/829", "#/texts/830", "#/texts/831", "#/texts/832", "#/texts/833", "#/texts/834", "#/texts/835", "#/texts/836", "#/texts/837", "#/texts/838", "#/texts/839", "#/texts/840", "#/texts/841", "#/texts/842", "#/texts/843", "#/texts/844", "#/texts/845", "#/texts/846", "#/texts/847", "#/texts/848", "#/texts/849", "#/texts/850", "#/texts/851", "#/texts/852", "#/texts/853", "#/texts/854", "#/texts/855", "#/texts/856", "#/texts/857", "#/texts/858", "#/texts/859"], "page_nos": [], "uuid": "a2f82b86-f1ba-4f15-83c6-9af364eed1a3"}
{"doc_id": "2025__Reliability_of_remote_self-administered_digital_cognitive_assessments_preliminar__W4412012853", "source_path": "<redacted:source_path>", "chunk_id": 33, "source_chunk_idxs": [40, 41], "num_tokens": 222, "text": "Reliability of remote self-administered digital cognitive assessments: preliminary validation study\nReferences\n1,206 Total views 264 Downloads Citation numbers are available from Dimensions\nView article impact\nView altmetric score\nShare on\nEdited by\nS S Scott  Sperling\nReviewed by\nS E Sarah  E. Polk\nS M Sadouanouan  MALO\nTable of contents\n- Abstract\n- 1 Introduction\n- 2 Materials and methods\n- 3 Results\n- 4 Discussion\n- Data availability statement\n- Ethics statement\n- Author contributions\n- Funding\n- Conflict of interest\n- Generative AI statement\n- Publisher's note\n- References\nExport citation\n- EndNote\n- Reference Manager\n- Simple Text file\n- BibTex\nCheck for updates\nFrontiers' impact\n\nReliability of remote self-administered digital cognitive assessments: preliminary validation study\nArticles published with Frontiers have received 12 million total citations\nYour research is the real superpower - learn how we maximise its impact through our leading community journals\nExplore our impact metrics\nSupplementary Material\nDownload article\nDownload\n- Download PDF\n- ReadCube\n- epub\n- XML\n- Guidelines\n- Explore\n- Outreach\n- Connect\nFollow us\n\u00a9 2025 Frontiers Media S.A. All rights reserved\nPrivacy policy\n|\nTerms and conditions", "doc_items_refs": ["#/texts/860", "#/texts/861", "#/texts/862", "#/texts/863", "#/texts/864", "#/texts/865", "#/texts/866", "#/texts/867", "#/texts/868", "#/texts/869", "#/texts/870", "#/texts/871", "#/texts/872", "#/texts/873", "#/texts/874", "#/texts/875", "#/texts/876", "#/texts/877", "#/texts/878", "#/texts/879", "#/texts/880", "#/texts/881", "#/texts/882", "#/texts/883", "#/texts/884", "#/texts/885", "#/texts/886", "#/texts/887", "#/texts/888", "#/texts/889", "#/texts/891", "#/texts/892", "#/texts/893", "#/texts/894", "#/texts/895", "#/texts/896", "#/texts/897", "#/texts/898", "#/texts/899", "#/texts/900", "#/texts/901", "#/texts/902", "#/texts/903", "#/texts/904", "#/texts/905", "#/texts/906", "#/texts/907", "#/texts/908"], "page_nos": [], "uuid": "22e40e33-f012-4861-b2bf-a7939600b725"}
