"""Namespace router tests covering snapshot caching and lazy FAISS restores.

Exercises per-namespace store provisioning, eviction/snapshot bookkeeping, and
error propagation so the router remains compatible with the FAISS snapshot
format generated by `ManagedFaissAdapter` and described in the README.
Additional scenarios ensure routing tables rebuild correctly from serialized
states, namespaces map to dense stores, and health checks propagate errors when
adapters fail. Verifies concurrent refreshes and metric bookkeeping.
"""

from __future__ import annotations

import json
import time
from types import SimpleNamespace
from typing import Callable, Mapping, Optional, Sequence

import numpy as np
import pytest

from DocsToKG.HybridSearch.config import DenseIndexConfig
from DocsToKG.HybridSearch.router import DEFAULT_NAMESPACE, FaissRouter
from DocsToKG.HybridSearch.store import ManagedFaissAdapter


class DummyDenseStore:
    """Lightweight in-memory stand-in satisfying ``DenseVectorStore``."""

    def __init__(self, namespace: str) -> None:
        self.namespace = namespace
        self._dim = 3
        self._device = -1
        self._config = DenseIndexConfig()
        self._resolver: Optional[Callable[[int], Optional[str]]] = None
        self._vectors: dict[str, float] = {}

    @property
    def dim(self) -> int:
        return self._dim

    @property
    def ntotal(self) -> int:
        return len(self._vectors)

    @property
    def device(self) -> int:
        return self._device

    @property
    def config(self) -> DenseIndexConfig:
        return self._config

    @property
    def adapter_stats(self) -> SimpleNamespace:
        return SimpleNamespace(
            device=self._device,
            ntotal=self.ntotal,
            index_description="dummy",
            nprobe=0,
            multi_gpu_mode="single",
            replicated=False,
            fp16_enabled=False,
            resources=None,
        )

    def add(self, vectors: Sequence[np.ndarray], vector_ids: Sequence[str]) -> None:
        for vector_id in vector_ids:
            self._vectors[vector_id] = float(len(self._vectors))

    def remove(self, vector_ids: Sequence[str]) -> None:
        for vector_id in vector_ids:
            self._vectors.pop(vector_id, None)

    def search(self, query: np.ndarray, top_k: int):  # pragma: no cover - unused stub
        return []

    def search_many(self, queries: np.ndarray, top_k: int):  # pragma: no cover - unused stub
        return []

    def search_batch(self, queries: np.ndarray, top_k: int):  # pragma: no cover - unused stub
        return []

    def range_search(
        self, query: np.ndarray, min_score: float, *, limit: Optional[int] = None
    ):  # pragma: no cover - unused stub
        return []

    def serialize(self) -> bytes:
        payload = json.dumps({"ids": sorted(self._vectors.keys())})
        return payload.encode("utf-8")

    def restore(self, payload: bytes, *, meta: Optional[Mapping[str, object]] = None) -> None:
        data = json.loads(payload.decode("utf-8"))
        ids = data.get("ids", [])
        self._vectors = {vector_id: float(index) for index, vector_id in enumerate(ids)}

    def stats(self) -> Mapping[str, float | str]:
        return {"ntotal": float(self.ntotal)}

    def set_id_resolver(self, resolver: Callable[[int], Optional[str]]) -> None:
        self._resolver = resolver


class RecordingFaissStore:
    """Minimal FAISS-like store capturing restore metadata for assertions."""

    def __init__(self, namespace: str) -> None:
        self.namespace = namespace
        self._dim = 3
        self._device = -1
        self._nprobe = 17
        self._use_cuvs = True
        self._vectors: list[str] = []
        self._resolver: Optional[Callable[[int], Optional[str]]] = None
        self.last_restore_meta: Optional[Mapping[str, object]] = None
        self._snapshot_meta: dict[str, object] = {
            "namespace": namespace,
            "device": self._device,
            "nprobe": self._nprobe,
            "use_cuvs": self._use_cuvs,
        }

    @property
    def dim(self) -> int:
        return self._dim

    @property
    def ntotal(self) -> int:
        return len(self._vectors)

    @property
    def device(self) -> int:
        return self._device

    @property
    def adapter_stats(self) -> SimpleNamespace:
        return SimpleNamespace(
            device=self._device,
            ntotal=self.ntotal,
            index_description="recording",
            nprobe=self._nprobe,
            multi_gpu_mode="single",
            replicated=False,
            fp16_enabled=False,
            resources=None,
        )

    def add(self, vectors: Sequence[np.ndarray], vector_ids: Sequence[str]) -> None:
        for vector_id in vector_ids:
            if vector_id not in self._vectors:
                self._vectors.append(vector_id)

    def remove(self, vector_ids: Sequence[str]) -> None:
        for vector_id in vector_ids:
            if vector_id in self._vectors:
                self._vectors.remove(vector_id)

    def serialize(self) -> bytes:
        payload = json.dumps({"ids": list(self._vectors), "namespace": self.namespace})
        return payload.encode("utf-8")

    def restore(self, payload: bytes, *, meta: Optional[Mapping[str, object]] = None) -> None:
        data = json.loads(payload.decode("utf-8"))
        ids = data.get("ids", [])
        self._vectors = [str(vector_id) for vector_id in ids]
        self.last_restore_meta = dict(meta) if meta is not None else None

    def snapshot_meta(self) -> Mapping[str, object]:
        return dict(self._snapshot_meta)

    def stats(self) -> Mapping[str, float | str]:
        return {"ntotal": float(self.ntotal)}

    def set_id_resolver(self, resolver: Callable[[int], Optional[str]]) -> None:
        self._resolver = resolver

    def rebuild_if_needed(self) -> bool:
        return False


class RichStatsStore(DummyDenseStore):
    """Dummy store exposing richer stats for aggregate tests."""

    def __init__(
        self,
        namespace: str,
        *,
        serialized_bytes: float,
        rebuild_required: bool,
        index_size: float,
    ) -> None:
        super().__init__(namespace)
        self._serialized_bytes = float(serialized_bytes)
        self._rebuild_required = rebuild_required
        self._index_size = float(index_size)

    def stats(self) -> Mapping[str, float | str | bool]:  # type: ignore[override]
        return {
            "ntotal": float(self.ntotal),
            "serialized_bytes": self._serialized_bytes,
            "index_size": self._index_size,
            "rebuild_required": self._rebuild_required,
        }


def test_restore_all_rehydrates_multiple_namespaces() -> None:
    """Ensure per-namespace restores hydrate every serialized store."""

    router = FaissRouter(
        per_namespace=True,
        default_store=DummyDenseStore("__default__"),
        factory=lambda namespace: DummyDenseStore(namespace),
    )
    for namespace in ("alpha", "beta"):
        store = router.get(namespace)
        store.add([np.zeros(3, dtype=np.float32)], [f"{namespace}-vector"])

    payloads = router.serialize_all()
    for namespace, packed in payloads.items():
        assert set(packed.keys()) == {"payload", "meta"}
        assert isinstance(packed["payload"], bytes)
        assert packed.get("meta") is None

    restored_router = FaissRouter(
        per_namespace=True,
        default_store=DummyDenseStore("__default__"),
        factory=lambda namespace: DummyDenseStore(namespace),
    )
    restored_router._snapshots["alpha"] = (b"legacy", None)
    before_restore = time.time()

    restored_router.restore_all(payloads)

    for namespace in ("alpha", "beta"):
        assert namespace in restored_router._stores
        restored_store = restored_router._stores[namespace]
        assert restored_store.ntotal == 1
        assert restored_router._last_used[namespace] >= before_restore
        assert namespace not in restored_router._snapshots


def test_managed_adapter_restores_with_snapshot_metadata() -> None:
    """Managed adapters should serialize/restore with metadata passthrough."""

    router = FaissRouter(
        per_namespace=True,
        default_store=ManagedFaissAdapter(RecordingFaissStore("__default__")),
        factory=lambda namespace: ManagedFaissAdapter(RecordingFaissStore(namespace)),
    )
    managed_store = router.get("alpha")
    managed_store.add([np.zeros(3, dtype=np.float32)], ["alpha-vector"])

    payloads = router.serialize_all()
    alpha_payload = payloads["alpha"]
    snapshot_meta = {"namespace": "alpha", "marker": "router-test"}
    router._snapshots["alpha"] = (alpha_payload["faiss"], snapshot_meta)
    del router._stores["alpha"]

    restored_router = FaissRouter(
        per_namespace=True,
        default_store=ManagedFaissAdapter(RecordingFaissStore("__default__")),
        factory=lambda namespace: ManagedFaissAdapter(RecordingFaissStore(namespace)),
    )
    restored_router.restore_all(payloads)

    restored_store = restored_router.get("alpha")
    inner_store = restored_store._inner  # type: ignore[attr-defined]
    assert isinstance(inner_store, RecordingFaissStore)
    assert inner_store.last_restore_meta == alpha_payload["meta"]
    assert inner_store._vectors == ["alpha-vector"]

    snapshot_meta = {"namespace": "alpha", "marker": "router-test"}
    router._snapshots["alpha"] = (
        alpha_payload["faiss"],
        snapshot_meta,
    )
    del router._stores["alpha"]

    rehydrated_store = router.get("alpha")
    inner_rehydrated = rehydrated_store._inner  # type: ignore[attr-defined]
    assert isinstance(inner_rehydrated, RecordingFaissStore)
    assert inner_rehydrated.last_restore_meta == snapshot_meta
    assert inner_rehydrated._vectors == ["alpha-vector"]


def test_stats_aggregate_handles_mixed_metric_types() -> None:
    """Aggregate stats should sum gauges, max timestamps, and track booleans."""

    router = FaissRouter(
        per_namespace=True,
        default_store=RichStatsStore(
            "__default__",
            serialized_bytes=0.0,
            rebuild_required=False,
            index_size=0.0,
        ),
        factory=lambda namespace: RichStatsStore(
            namespace,
            serialized_bytes=111.0 if namespace == "alpha" else 222.0,
            rebuild_required=namespace == "beta",
            index_size=10.0 if namespace == "alpha" else 20.0,
        ),
    )

    alpha_store = router.get("alpha")
    beta_store = router.get("beta")
    alpha_store.add([np.zeros(3, dtype=np.float32)], ["alpha-vector"])
    beta_store.add([np.zeros(3, dtype=np.float32)], ["beta-vector-1", "beta-vector-2"])

    router._last_used[DEFAULT_NAMESPACE] = 1.0
    router._last_used["alpha"] = 123.456
    router._last_used["beta"] = 789.101

    stats = router.stats()
    aggregate = stats["aggregate"]

    assert aggregate["ntotal"] == pytest.approx(3.0)
    assert aggregate["serialized_bytes"] == pytest.approx(333.0)
    assert aggregate["index_size"] == pytest.approx(30.0)
    assert aggregate["last_used_ts"] == pytest.approx(789.101)

    boolean_fields = aggregate.get("boolean_fields")
    assert isinstance(boolean_fields, Mapping)
    assert "evicted" in boolean_fields
    assert boolean_fields["evicted"] == {"true": 0, "false": 3}
    assert boolean_fields["rebuild_required"] == {"true": 1, "false": 2}
    assert "evicted" not in aggregate
    assert "rebuild_required" not in aggregate

def test_serialize_and_restore_roundtrip_carries_metadata() -> None:
    """Router serialization should retain metadata for restore_all."""

    router = FaissRouter(
        per_namespace=True,
        default_store=ManagedFaissAdapter(RecordingFaissStore("__default__")),
        factory=lambda namespace: ManagedFaissAdapter(RecordingFaissStore(namespace)),
    )
    store = router.get("alpha")
    store.add([np.zeros(3, dtype=np.float32)], ["alpha-vector"])

    payloads = router.serialize_all()
    alpha_payload = payloads["alpha"]
    assert isinstance(alpha_payload["meta"], Mapping)

    router._stores.pop("alpha")
    router.restore_all(payloads)

    restored_store = router._stores["alpha"]
    inner_store = restored_store._inner  # type: ignore[attr-defined]
    assert isinstance(inner_store, RecordingFaissStore)
    assert inner_store.last_restore_meta == alpha_payload["meta"]
    assert inner_store._vectors == ["alpha-vector"]


def test_evict_idle_preserves_snapshot_metadata() -> None:
    """Evicted managed stores should restore with cached snapshot metadata."""

    router = FaissRouter(
        per_namespace=True,
        default_store=ManagedFaissAdapter(RecordingFaissStore("__default__")),
        factory=lambda namespace: ManagedFaissAdapter(RecordingFaissStore(namespace)),
    )
    managed_store = router.get("alpha")
    expected_meta = dict(managed_store.snapshot_meta())
    managed_store.add([np.zeros(3, dtype=np.float32)], ["alpha-vector"])

    router._last_used["alpha"] = 0.0
    evicted = router.evict_idle(max_idle_seconds=1, skip_default=True)
    assert evicted == 1
    _, meta = router._snapshots["alpha"]
    assert isinstance(meta, Mapping)
    assert dict(meta) == expected_meta

    restored_store = router.get("alpha")
    inner_store = restored_store._inner  # type: ignore[attr-defined]
    assert isinstance(inner_store, RecordingFaissStore)
    assert inner_store.last_restore_meta == expected_meta
    assert inner_store._vectors == ["alpha-vector"]
