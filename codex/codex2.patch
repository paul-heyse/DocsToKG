 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF'
diff --git a/docs/04-api/retrieval.md b/docs/04-api/retrieval.md
index 5d096f40f008bc4dbb4e8eb1f5c4415f59755ce8..221a9c3420b334f28ef0489f632a009d850b2266 100644
--- a/docs/04-api/retrieval.md
+++ b/docs/04-api/retrieval.md
@@ -1,50 +1,51 @@
 # Module: retrieval

 Hybrid search execution across sparse and dense channels.

 ## Functions

 ### `search(self, request)`

 *No documentation available.*

 ### `_execute_bm25(self, request, filters, config, query_features, timings)`

 *No documentation available.*

 ### `_execute_splade(self, request, filters, config, query_features, timings)`

 *No documentation available.*

 ### `_execute_dense(self, request, filters, config, query_features, timings)`

 *No documentation available.*

 ### `_filter_dense_hits(self, hits, filters)`

-*No documentation available.*
+Return the dense hits that satisfy the active filters along with a pre-fetched
+chunk map for downstream processing.

 ### `_matches_filters(self, chunk, filters)`

 *No documentation available.*

 ### `_dedupe_candidates(self, candidates, fused_scores)`

 *No documentation available.*

 ### `_validate_request(self, request)`

 *No documentation available.*

 ## Classes

 ### `RequestValidationError`

 Raised when the caller submits an invalid search request.

 ### `ChannelResults`

 *No documentation available.*

 ### `HybridSearchService`

diff --git a/docs/hybrid_search_runbook.md b/docs/hybrid_search_runbook.md
index 58105fc0a24e883e3370cbef8cf0fb40247fc79f..62692a26fc37dec8f5a458cd52391bd173fa3351 100644
--- a/docs/hybrid_search_runbook.md
+++ b/docs/hybrid_search_runbook.md
@@ -1,38 +1,38 @@
 # Hybrid search operations runbook

 ## Overview
 Hybrid search combines BM25, SPLADE, and FAISS dense retrieval. This runbook summarizes daily operations, calibration routines, and contingency procedures for the ingestion and retrieval subsystems.

 ## Calibration sweeps
-1. Launch the validation harness: `python -m DocsToKG.HybridSearch.validation --dataset tests/data/hybrid_dataset.jsonl`.
+1. Launch the validation harness: `python -m DocsToKG.HybridSearch.tools.run_real_vector_ci --pytest-args "-q --real-vectors"` or invoke `python -m DocsToKG.HybridSearch.validation --dataset tests/data/hybrid_dataset.jsonl` for ad-hoc checks.
 2. Review `calibration.json` in the generated report directory. Confirm that self-hit accuracy is ≥0.95 for oversample ≥2.
 3. If accuracy drops below threshold:
    - Increase `dense.oversample` in `hybrid_config.json`.
    - Reload the configuration (`HybridSearchConfigManager.reload`).
    - Re-run the validation harness and compare the new report.

 ## Namespace onboarding
 1. Create a namespace entry in the configuration with chunk window overrides if needed.
 2. Run the ingestion pipeline with a smoke dataset for the new namespace.
 3. Verify OpenSearch stats using `build_stats_snapshot` and ensure `document_count` > 0 for the namespace.
 4. Execute `verify_pagination` for a representative query to confirm cursor stability.

 ## Failover and rollback
 1. Serialize the FAISS index with `serialize_state` and persist the payload alongside OpenSearch snapshots.
-2. During failover, restore OpenSearch from snapshot and call `restore_state` on the FAISS manager.
+2. During failover, restore OpenSearch from snapshot and call `restore_state` on the FAISS manager. Confirm startup logs include a `faiss-index-config` event with the expected `index_type`, `nlist`, `nprobe`, and device metadata before serving traffic.
 3. Warm the cache by issuing `HybridSearchRequest` probes for top queries.
 4. Use `verify_pagination` to ensure cursor continuity after failover.
 5. If errors persist, revert to the previous config file and call `HybridSearchConfigManager.reload`.

 ## Backup and restore drills
 1. Schedule weekly jobs to run `serialize_state` and OpenSearch snapshots.
 2. Store artifacts in durable object storage with retention >=30 days.
 3. Quarterly, perform a restore into a staging environment and execute the validation harness to confirm parity.
 4. Record results in the operational log and escalate if any validation report fails.

 ## Delete churn monitoring
 - Track `delete_chunks` and `ingest_chunks` counters from `Observability.metrics_snapshot`.
 - When `should_rebuild_index` returns `True`, schedule a full rebuild:
   1. Drain ingestion traffic.
   2. Reconstruct FAISS from the authoritative registry list.
   3. Re-enable ingestion and confirm `ntotal` aligns with registry count.
diff --git a/openspec/changes/optimize-hybrid-faiss-pipeline/tasks.md b/openspec/changes/optimize-hybrid-faiss-pipeline/tasks.md
index a4f9e690e1c368f70a296885d9a7ebff80056a02..dd89be73c14221354cc0e5a50ec73d5dea17ae65 100644
--- a/openspec/changes/optimize-hybrid-faiss-pipeline/tasks.md
+++ b/openspec/changes/optimize-hybrid-faiss-pipeline/tasks.md
@@ -1,19 +1,19 @@
 ## 1. FAISS Index Construction
-- [ ] 1.1 Replace manual CPU→GPU build logic in `dense.py` with factory/GPU one-liners and retain `IndexIDMap2` wrapping.
-- [ ] 1.2 Apply IVF parameters (`nprobe`, future PQ knobs) through `GpuParameterSpace`/`ParameterSpace` immediately after index creation and log the effective values.
+- [x] 1.1 Replace manual CPU→GPU build logic in `dense.py` with factory/GPU one-liners and retain `IndexIDMap2` wrapping.
+- [x] 1.2 Apply IVF parameters (`nprobe`, future PQ knobs) through `GpuParameterSpace`/`ParameterSpace` immediately after index creation and log the effective values.

 ## 2. ID Lifecycle & Delete Path
-- [ ] 2.1 Swap `_remove_ids` to use `faiss.IDSelectorBatch`, preserve CPU fallback, and clean up bookkeeping in one place.
-- [ ] 2.2 Introduce a FAISS-ID→vector-ID bridge inside `ChunkRegistry`, refactor `FaissIndexManager` to drop `_id_lookup`, and adjust serialization/restore accordingly.
+- [x] 2.1 Swap `_remove_ids` to use `faiss.IDSelectorBatch`, preserve CPU fallback, and clean up bookkeeping in one place.
+- [x] 2.2 Introduce a FAISS-ID→vector-ID bridge inside `ChunkRegistry`, refactor `FaissIndexManager` to drop `_id_lookup`, and adjust serialization/restore accordingly.

 ## 3. GPU-Assisted Similarity
-- [ ] 3.1 Update MMR diversification to batch compute cosine similarities via `faiss.pairwise_distance_gpu`, with NumPy fallback for CPU-only runs.
-- [ ] 3.2 Rework `ResultShaper` duplicate detection to batch compare embeddings using the same GPU helper.
+- [x] 3.1 Update MMR diversification to batch compute cosine similarities via `faiss.pairwise_distance_gpu`, with NumPy fallback for CPU-only runs.
+- [x] 3.2 Rework `ResultShaper` duplicate detection to batch compare embeddings using the same GPU helper.

 ## 4. Sparse Search Simplification
-- [ ] 4.1 Introduce `_search_sparse` in `storage.py` and refactor BM25/SPLADE search methods to delegate scoring into lambdas.
-- [ ] 4.2 Ensure pagination/tests still assert identical behaviour after refactor.
+- [x] 4.1 Introduce `_search_sparse` in `storage.py` and refactor BM25/SPLADE search methods to delegate scoring into lambdas.
+- [x] 4.2 Ensure pagination/tests still assert identical behaviour after refactor.

 ## 5. Tests & Docs
-- [ ] 5.1 Extend/adjust unit tests covering FAISS add/remove, serialization, GPU batching fallbacks, and MMR/duplicate behaviour to reflect the new paths and optional dataset-driven dense queries.
-- [ ] 5.2 Update developer documentation/README references to point to the relocated hybrid CLI tools under `src/DocsToKG/HybridSearch/tools/` and note the centralised FAISS tuning.
+- [x] 5.1 Extend/adjust unit tests covering FAISS add/remove, serialization, GPU batching fallbacks, and MMR/duplicate behaviour to reflect the new paths and optional dataset-driven dense queries.
+- [x] 5.2 Update developer documentation/README references to point to the relocated hybrid CLI tools under `src/DocsToKG/HybridSearch/tools/` and note the centralised FAISS tuning.
diff --git a/src/DocsToKG/HybridSearch/dense.py b/src/DocsToKG/HybridSearch/dense.py
index 58a55e6cb4be12b2d4db47cc75c1dfa8348a5680..339cdd25b1907099e6deee0419ce88611ae75d1c 100644
--- a/src/DocsToKG/HybridSearch/dense.py
+++ b/src/DocsToKG/HybridSearch/dense.py
@@ -1,50 +1,50 @@
 """
 FAISS index management with GPU-aware fallbacks.

 This module provides comprehensive FAISS index management for DocsToKG,
 including GPU acceleration support, automatic fallbacks, and memory-efficient
 vector storage and retrieval operations.

 The module supports multiple FAISS index types and automatically handles
 GPU availability with graceful CPU fallbacks for environments without
 GPU support.
 """

 from __future__ import annotations

 import base64
 import io
 import json
 import logging
-import uuid
 from dataclasses import dataclass
-from typing import Dict, List, Sequence
+from typing import Callable, Dict, List, Optional, Sequence

 import numpy as np

 from .config import DenseIndexConfig
+from .ids import vector_uuid_to_faiss_int

 logger = logging.getLogger(__name__)

 try:  # pragma: no cover - import tested indirectly
     import faiss  # type: ignore

     _FAISS_AVAILABLE = all(
         hasattr(faiss, attr) for attr in ("IndexFlatIP", "IndexIDMap2", "IDSelectorArray")
     )
 except Exception:  # pragma: no cover - environment without GPU/FAISS deps
     faiss = None  # type: ignore
     _FAISS_AVAILABLE = False


 @dataclass(slots=True)
 class FaissSearchResult:
     """Dense search hit returned by FAISS.

     This class represents a single result from FAISS vector similarity search,
     containing the vector identifier and similarity score.

     Attributes:
         vector_id: Unique identifier for the vector in the FAISS index
         score: Similarity score (lower values indicate higher similarity for distance metrics)

@@ -57,107 +57,114 @@ class FaissSearchResult:

     vector_id: str
     score: float


 class FaissIndexManager:
     """Manage lifecycle of a FAISS index with optional GPU acceleration.

     This class provides comprehensive FAISS index management including:
     - Automatic GPU detection and resource management
     - Multiple index type support (Flat, IVF, PQ)
     - Memory-efficient vector storage and retrieval
     - Graceful fallbacks for GPU-unavailable environments
     - Index persistence and restoration capabilities

     The manager automatically selects the best available index type and
     acceleration method based on the provided configuration and available
     hardware resources.

     Attributes:
         _dim: Vector dimensionality
         _config: Dense index configuration parameters
         _use_native: Whether FAISS native acceleration is available
         _gpu_resources: GPU resources for acceleration (if available)
         _index: FAISS index instance
-        _id_lookup: Mapping from internal IDs to vector IDs
         _vectors: In-memory vector storage for fallback operations
         _remove_fallbacks: Counter for fallback operations

     Examples:
         >>> config = DenseIndexConfig(index_type="ivf_pq")
         >>> manager = FaissIndexManager(dimension=768, config=config)
         >>> manager.add_vectors([vector1, vector2], ["id1", "id2"])
         >>> results = manager.search(query_vector, k=10)
     """

     def __init__(self, dim: int, config: DenseIndexConfig) -> None:
         """Initialize FAISS index manager with configuration.

         Sets up the FAISS index manager with the specified configuration,
         automatically detecting GPU availability and creating appropriate
         index structures.

         Args:
             dim: Vector dimensionality for the index
             config: Dense index configuration parameters

         Raises:
             ValueError: If configuration parameters are invalid
         """
         self._dim = dim
         self._config = config
         self._use_native = _FAISS_AVAILABLE
         self._gpu_resources = self._init_gpu_resources() if self._use_native else None
         self._index = self._create_index() if self._use_native else None
-        self._id_lookup: Dict[int, str] = {}
         self._vectors: Dict[str, np.ndarray] = {}
         self._remove_fallbacks = 0
+        self._id_resolver: Optional[Callable[[int], Optional[str]]] = None
+
+        if self._use_native and self._index is not None:
+            self._apply_search_parameters(self._index)

     @property
     def ntotal(self) -> int:
         """Get total number of vectors in the index.

         Returns the total count of vectors currently stored in the index,
         using native FAISS counting when available or fallback storage.

         Returns:
             Total number of vectors in the index
         """
         if self._use_native and self._index is not None:
             return int(self._index.ntotal)
         return len(self._vectors)

     @property
     def config(self) -> DenseIndexConfig:
         """Get the current index configuration.

         Returns:
             Current DenseIndexConfig instance
         """
         return self._config

+    def set_id_resolver(self, resolver: Callable[[int], Optional[str]]) -> None:
+        """Attach a resolver used to translate FAISS int IDs to vector IDs."""
+
+        self._id_resolver = resolver
+
     def train(self, vectors: Sequence[np.ndarray]) -> None:
         """Train the FAISS index with sample vectors.

         This method trains IVF-based indexes with representative data
         to optimize search performance. Flat indexes don't require training.

         Args:
             vectors: Sample vectors for index training

         Raises:
             ValueError: If training vectors are required but not provided
         """
         if not self._use_native or self._index is None:
             return
         if not hasattr(self._index, "is_trained"):
             return
         if getattr(self._index, "is_trained"):
             return
         if not vectors:
             raise ValueError("Training vectors required for IVF indexes")
         matrix = np.stack([self._ensure_dim(vec) for vec in vectors]).astype(np.float32)
         faiss.normalize_L2(matrix)
         self._index.train(matrix)

     def needs_training(self) -> bool:
@@ -168,271 +175,309 @@ class FaissIndexManager:
         """
         if not self._use_native or self._index is None:
             return False
         return not bool(getattr(self._index, "is_trained"))

     def add(self, vectors: Sequence[np.ndarray], vector_ids: Sequence[str]) -> None:
         """Add vectors to the FAISS index with associated IDs.

         This method adds new vectors to the index, optionally using GPU
         acceleration when available and falling back to CPU storage when needed.

         Args:
             vectors: Sequence of vector arrays to add
             vector_ids: Corresponding vector identifiers

         Raises:
             ValueError: If vectors and vector_ids lengths don't match
         """
         if len(vectors) != len(vector_ids):
             raise ValueError("vectors and vector_ids must align")
         if not vectors:
             return
         matrix = np.stack([self._ensure_dim(vec) for vec in vectors]).astype(np.float32)
         faiss.normalize_L2(matrix) if self._use_native else self._normalize(matrix)
         if self._use_native and self._index is not None:
-            ids = np.array([self._uuid_to_int64(vid) for vid in vector_ids], dtype=np.int64)
+            ids = np.array([vector_uuid_to_faiss_int(vid) for vid in vector_ids], dtype=np.int64)
             self._remove_ids(ids)
             self._index.add_with_ids(matrix, ids)
-            for internal_id, vector_id in zip(ids, vector_ids):
-                self._id_lookup[int(internal_id)] = vector_id
         else:
             for vector, vector_id in zip(matrix, vector_ids):
                 self._vectors[vector_id] = vector.copy()

     def remove(self, vector_ids: Sequence[str]) -> None:
         """Remove vectors from the index by their IDs.

         This method removes specified vectors from the index, updating
         both the FAISS index and the internal ID lookup mappings.

         Args:
             vector_ids: Sequence of vector IDs to remove

         Note:
             FAISS doesn't support direct vector removal, so this method
             marks vectors as removed in the lookup table
         """
         if not vector_ids:
             return
         if self._use_native and self._index is not None:
-            ids = np.array([self._uuid_to_int64(vid) for vid in vector_ids], dtype=np.int64)
+            ids = np.array([vector_uuid_to_faiss_int(vid) for vid in vector_ids], dtype=np.int64)
             self._remove_ids(ids)
-            for internal_id in ids:
-                self._id_lookup.pop(int(internal_id), None)
         else:
             for vector_id in vector_ids:
                 self._vectors.pop(vector_id, None)

     def search(self, query: np.ndarray, top_k: int) -> List[FaissSearchResult]:
         """Search for similar vectors in the index.

         This method performs similarity search using either FAISS native
         acceleration or CPU-based fallback, returning the most similar
         vectors with their similarity scores.

         Args:
             query: Query vector for similarity search
             top_k: Maximum number of results to return

         Returns:
             List of FaissSearchResult objects with vector IDs and scores

         Examples:
             >>> results = manager.search(query_vector, k=10)
             >>> for result in results:
             ...     print(f"Vector {result.vector_id}: score {result.score}")
         """
         query_matrix = self._ensure_dim(query).reshape(1, -1).astype(np.float32)
         faiss.normalize_L2(query_matrix) if self._use_native else self._normalize(query_matrix)
         if self._use_native and self._index is not None:
             scores, ids = self._index.search(query_matrix, top_k)
             results: List[FaissSearchResult] = []
             for score, internal_id in zip(scores[0], ids[0]):
                 if internal_id == -1:
                     continue
-                vector_id = self._id_lookup.get(int(internal_id))
+                vector_id = self._resolve_vector_id(int(internal_id))
                 if vector_id is None:
                     continue
                 results.append(FaissSearchResult(vector_id=vector_id, score=float(score)))
             return results
         results: List[FaissSearchResult] = []
         if not self._vectors:
             return results
         query_vec = query_matrix[0]
         all_items = [
             (vector_id, float(np.dot(query_vec, stored)))
             for vector_id, stored in self._vectors.items()
         ]
         all_items.sort(key=lambda item: item[1], reverse=True)
         for vector_id, score in all_items[:top_k]:
             results.append(FaissSearchResult(vector_id=vector_id, score=score))
         return results

     def serialize(self) -> bytes:
         """Serialize the index state for persistence.

         This method converts the index to a byte representation that can be
         stored and later restored, supporting both native FAISS and fallback
         storage modes.

         Returns:
             Serialized index data as bytes

         Raises:
             RuntimeError: If serialization fails
         """
         if self._use_native and self._index is not None:
             cpu_index = self._to_cpu(self._index)
             index_bytes = faiss.serialize_index(cpu_index)
             payload = {
                 "mode": "native",
                 "index": base64.b64encode(index_bytes).decode("ascii"),
-                "id_lookup": {
-                    str(internal_id): vector_id
-                    for internal_id, vector_id in self._id_lookup.items()
-                },
             }
             return json.dumps(payload).encode("utf-8")
         buffer = io.BytesIO()
         payload = {vector_id: vector.tolist() for vector_id, vector in self._vectors.items()}
         buffer.write(json.dumps(payload).encode("utf-8"))
         return buffer.getvalue()

     def restore(self, payload: bytes) -> None:
         """Restore index state from serialized data.

         This method reconstructs the index from previously serialized data,
         supporting both native FAISS and fallback storage restoration.

         Args:
             payload: Serialized index data from serialize() method

         Raises:
             ValueError: If payload format is invalid or incompatible
             RuntimeError: If restoration fails
         """
         if self._use_native:
             try:
                 data = json.loads(payload.decode("utf-8"))
             except (json.JSONDecodeError, UnicodeDecodeError):
                 cpu_index = faiss.deserialize_index(np.frombuffer(payload, dtype=np.uint8))
                 self._index = self._maybe_to_gpu(cpu_index)
-                self._id_lookup = {}
+                self._apply_search_parameters(self._index)
                 return
             if data.get("mode") == "native":
                 encoded = data.get("index")
                 if not isinstance(encoded, str):
                     raise ValueError("Invalid FAISS payload")
                 index_bytes = base64.b64decode(encoded.encode("ascii"))
                 cpu_index = faiss.deserialize_index(np.frombuffer(index_bytes, dtype=np.uint8))
                 self._index = self._maybe_to_gpu(cpu_index)
-                raw_lookup = data.get("id_lookup", {})
-                self._id_lookup = {int(key): str(value) for key, value in raw_lookup.items()}
+                self._apply_search_parameters(self._index)
             else:
                 cpu_index = faiss.deserialize_index(np.frombuffer(payload, dtype=np.uint8))
                 self._index = self._maybe_to_gpu(cpu_index)
-                self._id_lookup = {}
+                self._apply_search_parameters(self._index)
         else:
             data = json.loads(payload.decode("utf-8"))
             self._vectors = {
                 vector_id: np.array(values, dtype=np.float32) for vector_id, values in data.items()
             }

     def stats(self) -> Dict[str, float | str]:
         return {
             "ntotal": float(self.ntotal),
             "index_type": self._config.index_type,
             "gpu_remove_fallbacks": float(self._remove_fallbacks),
+            "nprobe": float(self._config.nprobe) if self._use_native else 0.0,
         }

     def _create_index(self) -> "faiss.Index":
         if not self._use_native:
             return None
-        if self._config.index_type == "flat":
-            base_index = faiss.IndexFlatIP(self._dim)
-        elif self._config.index_type == "ivf_flat":
-            quantizer = faiss.IndexFlatIP(self._dim)
-            base_index = faiss.IndexIVFFlat(
-                quantizer,
-                self._dim,
-                self._config.nlist,
-                faiss.METRIC_INNER_PRODUCT,
-            )
-        else:
-            quantizer = faiss.IndexFlatIP(self._dim)
-            base_index = faiss.IndexIVFPQ(
-                quantizer,
-                self._dim,
-                self._config.nlist,
-                self._config.pq_m,
-                self._config.pq_bits,
-            )
-        index = faiss.IndexIDMap2(base_index)
-        return self._maybe_to_gpu(index)
+
+        metric = faiss.METRIC_INNER_PRODUCT
+        index_type = self._config.index_type
+
+        if (
+            self._gpu_resources is not None
+            and index_type == "flat"
+            and hasattr(faiss, "GpuIndexFlatIP")
+        ):
+            base = faiss.GpuIndexFlatIP(self._gpu_resources, self._dim)
+            return faiss.IndexIDMap2(base)
+
+        spec_map = {
+            "flat": "Flat",
+            "ivf_flat": f"IVF{self._config.nlist},Flat",
+            "ivf_pq": f"IVF{self._config.nlist},PQ{self._config.pq_m}x{self._config.pq_bits}",
+        }
+        try:
+            spec = spec_map[index_type]
+        except KeyError as exc:  # pragma: no cover - config validation elsewhere
+            raise ValueError(f"Unsupported FAISS index type: {index_type}") from exc
+
+        cpu_index = faiss.index_factory(self._dim, spec, metric)
+        mapped = faiss.IndexIDMap2(cpu_index)
+        return self._maybe_to_gpu(mapped)

     def _maybe_to_gpu(self, index: "faiss.Index") -> "faiss.Index":
         if not self._use_native or self._gpu_resources is None:
             return index
         try:
             gpu_index = faiss.index_cpu_to_gpu(self._gpu_resources, 0, index)
             logger.info("FAISS index promoted to GPU")
             return gpu_index
         except Exception as exc:  # pragma: no cover - GPU promotion path
             logger.warning("Failed to promote FAISS index to GPU: %s", exc)
             return index

     def _to_cpu(self, index: "faiss.Index") -> "faiss.Index":
         if not self._use_native:
             return index
         if hasattr(faiss, "index_gpu_to_cpu"):
             try:
                 return faiss.index_gpu_to_cpu(index)
             except Exception:  # pragma: no cover - best effort fallback
                 return index
         return index

+    def _apply_search_parameters(self, index: "faiss.Index | None") -> None:
+        if not self._use_native or index is None:
+            return
+
+        if self._config.index_type.startswith("ivf"):
+            nprobe = int(self._config.nprobe)
+            applied = False
+            if self._gpu_resources is not None and hasattr(faiss, "GpuParameterSpace"):
+                try:
+                    faiss.GpuParameterSpace().set_index_parameter(index, "nprobe", nprobe)
+                    applied = True
+                except Exception:  # pragma: no cover - GPU parameter guard
+                    applied = False
+            if not applied and hasattr(faiss, "ParameterSpace"):
+                try:
+                    space = faiss.ParameterSpace()
+                    space.set_index_parameter(index, "nprobe", nprobe)
+                except Exception:  # pragma: no cover - CPU parameter guard
+                    logger.debug("Unable to set nprobe via ParameterSpace", exc_info=True)
+
+        self._log_index_configuration(index)
+
+    def _log_index_configuration(self, index: "faiss.Index") -> None:
+        if not self._use_native or index is None:
+            return
+        device: Optional[int] = None
+        if hasattr(index, "getDevice"):
+            try:
+                device = int(index.getDevice())
+            except Exception:  # pragma: no cover - defensive logging guard
+                device = None
+        event = {
+            "index_type": self._config.index_type,
+            "nlist": int(self._config.nlist),
+            "nprobe": int(self._config.nprobe),
+            "pq_m": int(self._config.pq_m),
+            "pq_bits": int(self._config.pq_bits),
+            "device": device,
+        }
+        logger.info("faiss-index-config", extra={"event": event})
+
+    def _resolve_vector_id(self, internal_id: int) -> Optional[str]:
+        if self._id_resolver is None:
+            return None
+        try:
+            return self._id_resolver(internal_id)
+        except Exception:  # pragma: no cover - resolver failures should not crash search
+            logger.debug("id-resolver failure", exc_info=True)
+            return None
+
     def _init_gpu_resources(self) -> "faiss.StandardGpuResources | None":
         if not self._use_native or not hasattr(faiss, "StandardGpuResources"):
             return None
         try:
             resources = faiss.StandardGpuResources()
             return resources
         except Exception as exc:  # pragma: no cover - GPU setup only when available
             logger.warning("GPU resources unavailable, using CPU FAISS: %s", exc)
             return None

     def _ensure_dim(self, vector: np.ndarray) -> np.ndarray:
         if vector.shape != (self._dim,):
             raise ValueError(f"Expected embedding dimension {self._dim}, got {vector.shape}")
         return vector

-    def _uuid_to_int64(self, value: str) -> int:
-        return uuid.UUID(value).int & ((1 << 63) - 1)
-
     def _remove_ids(self, ids: np.ndarray) -> None:
         if not self._use_native or self._index is None or ids.size == 0:
             return
+
         id_array = ids.astype(np.int64)
-        try:
-            selector = faiss.IDSelectorArray(id_array.size, faiss.swig_ptr(id_array))
-        except AttributeError:
-            selector = faiss.IDSelectorBatch(id_array)
+        selector = faiss.IDSelectorBatch(id_array)
         try:
             self._index.remove_ids(selector)
         except RuntimeError as exc:
-            message = str(exc).lower()
-            if "remove_ids not implemented" not in message:
+            if "remove_ids not implemented" not in str(exc).lower():
                 raise
             logger.warning("FAISS remove_ids not implemented on GPU index, falling back to CPU")
             self._remove_fallbacks += 1
             cpu_index = self._to_cpu(self._index)
             cpu_index.remove_ids(selector)
             self._index = self._maybe_to_gpu(cpu_index)
-        finally:
-            for internal_id in id_array:
-                self._id_lookup.pop(int(internal_id), None)
+            self._apply_search_parameters(self._index)

     def _normalize(self, matrix: np.ndarray) -> None:
         norms = np.linalg.norm(matrix, axis=1, keepdims=True)
         norms[norms == 0.0] = 1.0
         matrix /= norms
diff --git a/src/DocsToKG/HybridSearch/fusion.py b/src/DocsToKG/HybridSearch/fusion.py
index 3f2be5bb2134b32e9e0b7401a8f066e682bb5ee2..b41f66462925ba145b72d7013e7f3dc88ae23e43 100644
--- a/src/DocsToKG/HybridSearch/fusion.py
+++ b/src/DocsToKG/HybridSearch/fusion.py
@@ -1,66 +1,69 @@
 """Reciprocal Rank Fusion and diversification utilities."""
 from __future__ import annotations

 from collections import defaultdict
 from typing import Dict, Iterable, List, Mapping, Sequence, Tuple

 import numpy as np

 from .types import FusionCandidate
+from .similarity import normalize_rows, pairwise_inner_products


 class ReciprocalRankFusion:
     """Combine ranked lists using Reciprocal Rank Fusion."""

     def __init__(self, k0: float = 60.0) -> None:
         if k0 <= 0:
             raise ValueError("k0 must be positive")
         self._k0 = k0

     def fuse(self, candidates: Sequence[FusionCandidate]) -> Dict[str, float]:
         scores: Dict[str, float] = defaultdict(float)
         for candidate in candidates:
             contribution = 1.0 / (self._k0 + candidate.rank)
             scores[candidate.chunk.vector_id] += contribution
         return dict(scores)


 def apply_mmr_diversification(
     fused_candidates: Sequence[FusionCandidate],
     fused_scores: Mapping[str, float],
     lambda_param: float,
     top_k: int,
 ) -> List[FusionCandidate]:
     if not 0.0 <= lambda_param <= 1.0:
         raise ValueError("lambda_param must be within [0, 1]")
-    remaining = list(fused_candidates)
-    selected: List[FusionCandidate] = []
-    while remaining and len(selected) < top_k:
-        best_candidate = None
+    if not fused_candidates:
+        return []
+
+    embeddings = np.stack(
+        [candidate.chunk.features.embedding.astype(np.float32, copy=False) for candidate in fused_candidates]
+    )
+    normalized = normalize_rows(embeddings)
+    similarity_matrix = pairwise_inner_products(normalized)
+
+    candidate_indices = list(range(len(fused_candidates)))
+    selected_indices: List[int] = []
+
+    while candidate_indices and len(selected_indices) < top_k:
+        best_idx: int | None = None
         best_score = float("-inf")
-        for candidate in remaining:
-            relevance = fused_scores[candidate.chunk.vector_id]
-            diversity_penalty = 0.0
-            if selected:
-                similarities = [
-                    _cosine_similarity(candidate.chunk.features.embedding, other.chunk.features.embedding)
-                    for other in selected
-                ]
-                diversity_penalty = max(similarities)
+        for idx in candidate_indices:
+            vector_id = fused_candidates[idx].chunk.vector_id
+            relevance = fused_scores.get(vector_id, 0.0)
+            if selected_indices:
+                diversity_penalty = float(np.max(similarity_matrix[idx, selected_indices]))
+            else:
+                diversity_penalty = 0.0
             score = lambda_param * relevance - (1 - lambda_param) * diversity_penalty
             if score > best_score:
-                best_candidate = candidate
+                best_idx = idx
                 best_score = score
-        if best_candidate is None:
+        if best_idx is None:
             break
-        selected.append(best_candidate)
-        remaining = [candidate for candidate in remaining if candidate.chunk.vector_id != best_candidate.chunk.vector_id]
-    return selected
-
+        selected_indices.append(best_idx)
+        candidate_indices = [idx for idx in candidate_indices if idx != best_idx]

-def _cosine_similarity(lhs: np.ndarray, rhs: np.ndarray) -> float:
-    denom = float(np.linalg.norm(lhs) * np.linalg.norm(rhs))
-    if denom == 0.0:
-        return 0.0
-    return float(np.dot(lhs, rhs) / denom)
+    return [fused_candidates[idx] for idx in selected_indices]

diff --git a/src/DocsToKG/HybridSearch/ids.py b/src/DocsToKG/HybridSearch/ids.py
new file mode 100644
index 0000000000000000000000000000000000000000..09e8986767892dd67e7ab66339c53147f216c1d6
--- /dev/null
+++ b/src/DocsToKG/HybridSearch/ids.py
@@ -0,0 +1,13 @@
+"""Helpers for converting between vector UUIDs and FAISS int identifiers."""
+from __future__ import annotations
+
+import uuid
+
+__all__ = ["vector_uuid_to_faiss_int"]
+
+_MASK_63_BITS = (1 << 63) - 1
+
+
+def vector_uuid_to_faiss_int(vector_id: str) -> int:
+    """Return the FAISS-compatible int identifier for a vector UUID."""
+    return uuid.UUID(vector_id).int & _MASK_63_BITS
diff --git a/src/DocsToKG/HybridSearch/ingest.py b/src/DocsToKG/HybridSearch/ingest.py
index 059ba951f827d9bebe7fa1510925487719314cd3..68b16a70d6c4ea3a153ec910364f9c4b8fee9c93 100644
--- a/src/DocsToKG/HybridSearch/ingest.py
+++ b/src/DocsToKG/HybridSearch/ingest.py
@@ -25,50 +25,51 @@ class RetryableIngestError(IngestError):

 @dataclass(slots=True)
 class IngestMetrics:
     """Simple metrics bundle used by tests."""

     chunks_upserted: int = 0
     chunks_deleted: int = 0


 class ChunkIngestionPipeline:
     """Coordinate loading of chunk/vector artifacts and dual writes."""

     def __init__(
         self,
         *,
         faiss_index: FaissIndexManager,
         opensearch: OpenSearchSimulator,
         registry: ChunkRegistry,
         observability: Optional[Observability] = None,
     ) -> None:
         self._faiss = faiss_index
         self._opensearch = opensearch
         self._registry = registry
         self._metrics = IngestMetrics()
         self._observability = observability or Observability()
+        self._faiss.set_id_resolver(self._registry.resolve_faiss_id)

     @property
     def metrics(self) -> IngestMetrics:
         return self._metrics

     @property
     def faiss_index(self) -> FaissIndexManager:
         return self._faiss

     def upsert_documents(self, documents: Sequence[DocumentInput]) -> List[ChunkPayload]:
         new_chunks: List[ChunkPayload] = []
         try:
             for document in documents:
                 with self._observability.trace("ingest_document", namespace=document.namespace):
                     loaded = self._load_precomputed_chunks(document)
                     if loaded:
                         self._delete_existing_for_doc(document.doc_id, document.namespace)
                     new_chunks.extend(loaded)
         except Exception as exc:  # pragma: no cover - defensive guard
             self._observability.logger.exception("chunk-ingest-error", extra={"event": {"error": str(exc)}})
             raise RetryableIngestError("Failed to transform document") from exc

         if not new_chunks:
             return []

diff --git a/src/DocsToKG/HybridSearch/results.py b/src/DocsToKG/HybridSearch/results.py
index ce89880e8a720e90718b45cfb802efed4176a525..ed824ba833de9e9ba18e3240e77435a55b80ae5b 100644
--- a/src/DocsToKG/HybridSearch/results.py
+++ b/src/DocsToKG/HybridSearch/results.py
@@ -1,97 +1,96 @@
 """Result shaping utilities for hybrid search responses."""
 from __future__ import annotations

 from collections import defaultdict
 from typing import Dict, Iterable, List, Mapping, Sequence

 import numpy as np

 from .config import FusionConfig
+from .similarity import max_inner_product, normalize_rows
 from .storage import OpenSearchSimulator
 from .tokenization import tokenize
 from .types import (
     ChunkPayload,
     HybridSearchDiagnostics,
     HybridSearchRequest,
     HybridSearchResult,
 )


 class ResultShaper:
     """Collapse duplicates, enforce quotas, and generate highlights."""

     def __init__(self, opensearch: OpenSearchSimulator, fusion_config: FusionConfig) -> None:
         self._opensearch = opensearch
         self._fusion_config = fusion_config

     def shape(
         self,
         ordered_chunks: Sequence[ChunkPayload],
         fused_scores: Mapping[str, float],
         request: HybridSearchRequest,
         channel_scores: Mapping[str, Dict[str, float]],
     ) -> List[HybridSearchResult]:
         doc_buckets: Dict[str, int] = defaultdict(int)
         emitted_vectors: List[str] = []
+        emitted_embeddings: List[np.ndarray] = []
         results: List[HybridSearchResult] = []
         query_tokens = tokenize(request.query)
         for rank, chunk in enumerate(ordered_chunks, start=1):
             if not self._within_doc_limit(chunk.doc_id, doc_buckets):
                 continue
-            if self._is_near_duplicate(chunk, emitted_vectors):
+            embedding = self._normalize_embedding(chunk.features.embedding)
+            if self._is_near_duplicate(embedding, emitted_embeddings):
                 continue
             highlights = self._build_highlights(chunk, query_tokens)
             diagnostics = HybridSearchDiagnostics(
                 bm25_score=channel_scores.get("bm25", {}).get(chunk.vector_id),
                 splade_score=channel_scores.get("splade", {}).get(chunk.vector_id),
                 dense_score=channel_scores.get("dense", {}).get(chunk.vector_id),
             )
             provenance = [chunk.char_offset] if chunk.char_offset else []
             results.append(
                 HybridSearchResult(
                     doc_id=chunk.doc_id,
                     chunk_id=chunk.chunk_id,
                     namespace=chunk.namespace,
                     score=fused_scores[chunk.vector_id],
                     fused_rank=rank,
                     text=chunk.text,
                     highlights=highlights,
                     provenance_offsets=provenance,
                     diagnostics=diagnostics,
                     metadata=dict(chunk.metadata),
                 )
             )
             emitted_vectors.append(chunk.vector_id)
+            emitted_embeddings.append(embedding)
         return results

     def _within_doc_limit(self, doc_id: str, doc_buckets: Dict[str, int]) -> bool:
         doc_buckets[doc_id] += 1
         return doc_buckets[doc_id] <= self._fusion_config.max_chunks_per_doc

-    def _is_near_duplicate(self, chunk: ChunkPayload, emitted_vector_ids: Sequence[str]) -> bool:
-        for vector_id in emitted_vector_ids:
-            existing = self._opensearch.fetch([vector_id])
-            if not existing:
-                continue
-            existing_chunk = existing[0]
-            similarity = self._cosine_similarity(
-                chunk.features.embedding,
-                existing_chunk.features.embedding,
-            )
-            if similarity >= self._fusion_config.cosine_dedupe_threshold:
-                return True
-        return False
+    def _is_near_duplicate(
+        self,
+        embedding: np.ndarray,
+        emitted_embeddings: Sequence[np.ndarray],
+    ) -> bool:
+        if not emitted_embeddings:
+            return False
+        corpus = np.stack(emitted_embeddings)
+        similarity = max_inner_product(embedding, corpus)
+        return similarity >= self._fusion_config.cosine_dedupe_threshold

     def _build_highlights(self, chunk: ChunkPayload, query_tokens: Sequence[str]) -> List[str]:
         highlights = self._opensearch.highlight(chunk, query_tokens)
         if highlights:
             return highlights
         snippet = chunk.text[:200]
         return [snippet] if snippet else []

-    def _cosine_similarity(self, lhs: np.ndarray, rhs: np.ndarray) -> float:
-        denom = float(np.linalg.norm(lhs) * np.linalg.norm(rhs))
-        if denom == 0.0:
-            return 0.0
-        return float(np.dot(lhs, rhs) / denom)
+    def _normalize_embedding(self, embedding: np.ndarray) -> np.ndarray:
+        normalized = normalize_rows(embedding.reshape(1, -1).astype(np.float32, copy=False))
+        return normalized[0]

diff --git a/src/DocsToKG/HybridSearch/retrieval.py b/src/DocsToKG/HybridSearch/retrieval.py
index ddf22b6ba503cb1a2dc716312fd489c25c111b4b..59e02a120f20e525679cdd4b05490af4758b220f 100644
--- a/src/DocsToKG/HybridSearch/retrieval.py
+++ b/src/DocsToKG/HybridSearch/retrieval.py
@@ -1,49 +1,55 @@
 """
 Hybrid search execution across sparse and dense channels.

 This module provides the core hybrid search service for DocsToKG, orchestrating
 multiple retrieval methods (BM25, SPLADE, dense vectors) and fusing their results
 for optimal document retrieval performance.

 The service supports configurable search strategies, real-time observability,
 and comprehensive result ranking through advanced fusion techniques.
 """
 from __future__ import annotations

 import time
 from dataclasses import dataclass
 from typing import Dict, List, Mapping, Optional, Sequence

 from .config import HybridSearchConfig, HybridSearchConfigManager
 from .dense import FaissIndexManager, FaissSearchResult
 from .features import FeatureGenerator
 from .fusion import ReciprocalRankFusion, apply_mmr_diversification
 from .observability import Observability
 from .results import ResultShaper
 from .storage import ChunkRegistry, OpenSearchSimulator
-from .types import ChunkFeatures, FusionCandidate, HybridSearchRequest, HybridSearchResponse
+from .types import (
+    ChunkFeatures,
+    ChunkPayload,
+    FusionCandidate,
+    HybridSearchRequest,
+    HybridSearchResponse,
+)


 class RequestValidationError(ValueError):
     """Raised when the caller submits an invalid search request.

     This exception is raised when a hybrid search request contains invalid
     parameters, malformed data, or violates system constraints.

     Attributes:
         message: Description of the validation error
         field: Optional field name that caused the error

     Examples:
         >>> try:
         ...     service.search(invalid_request)
         ... except RequestValidationError as e:
         ...     print(f"Invalid request: {e.message}")
     """


 @dataclass(slots=True)
 class ChannelResults:
     """Results from a single retrieval channel (BM25, SPLADE, or dense).

     This class encapsulates the candidates and scoring information returned
@@ -89,50 +95,51 @@ class HybridSearchService:
         ...     config_manager=config_manager,
         ...     feature_generator=feature_generator,
         ...     faiss_index=faiss_index,
         ...     opensearch=opensearch,
         ...     registry=registry
         ... )
         >>> results = service.search(request)
     """

     def __init__(
         self,
         *,
         config_manager: HybridSearchConfigManager,
         feature_generator: FeatureGenerator,
         faiss_index: FaissIndexManager,
         opensearch: OpenSearchSimulator,
         registry: ChunkRegistry,
         observability: Optional[Observability] = None,
     ) -> None:
         self._config_manager = config_manager
         self._feature_generator = feature_generator
         self._faiss = faiss_index
         self._opensearch = opensearch
         self._registry = registry
         self._observability = observability or Observability()
+        self._faiss.set_id_resolver(self._registry.resolve_faiss_id)

     def search(self, request: HybridSearchRequest) -> HybridSearchResponse:
         config = self._config_manager.get()
         self._validate_request(request)
         filters = dict(request.filters)
         if request.namespace:
             filters["namespace"] = request.namespace

         with self._observability.trace("hybrid_search", namespace=request.namespace or "*"):
             timings: Dict[str, float] = {}
             total_start = time.perf_counter()
             query_start = time.perf_counter()
             query_features = self._feature_generator.compute_features(request.query)
             timings["feature_ms"] = (time.perf_counter() - query_start) * 1000

             bm25 = self._execute_bm25(request, filters, config, query_features, timings)
             splade = self._execute_splade(request, filters, config, query_features, timings)
             dense = self._execute_dense(request, filters, config, query_features, timings)

             fusion = ReciprocalRankFusion(config.fusion.k0)
             combined_candidates = bm25.candidates + splade.candidates + dense.candidates
             fused_scores = fusion.fuse(combined_candidates)
             unique_candidates = self._dedupe_candidates(combined_candidates, fused_scores)

             if request.diversification and config.fusion.enable_mmr:
@@ -218,78 +225,83 @@ class HybridSearchService:
             filters,
             top_k=config.retrieval.splade_top_k,
         )
         timings["splade_ms"] = (time.perf_counter() - start) * 1000
         self._observability.metrics.increment("search_channel_requests", channel="splade")
         self._observability.metrics.observe("search_channel_candidates", len(hits), channel="splade")
         candidates = [
             FusionCandidate(source="splade", score=score, chunk=chunk, rank=idx + 1)
             for idx, (chunk, score) in enumerate(hits)
         ]
         scores = {chunk.vector_id: score for chunk, score in hits}
         return ChannelResults(candidates=candidates, scores=scores)

     def _execute_dense(
         self,
         request: HybridSearchRequest,
         filters: Mapping[str, object],
         config: HybridSearchConfig,
         query_features: ChunkFeatures,
         timings: Dict[str, float],
     ) -> ChannelResults:
         start = time.perf_counter()
         oversampled = request.page_size * config.dense.oversample
         hits = self._faiss.search(query_features.embedding, min(config.retrieval.dense_top_k, oversampled))
         timings["dense_ms"] = (time.perf_counter() - start) * 1000
-        filtered = self._filter_dense_hits(hits, filters)
+        filtered, chunk_map = self._filter_dense_hits(hits, filters)
         self._observability.metrics.increment("search_channel_requests", channel="dense")
         self._observability.metrics.observe("search_channel_candidates", len(filtered), channel="dense")
         candidates: List[FusionCandidate] = []
         scores: Dict[str, float] = {}
         for idx, hit in enumerate(filtered):
-            chunk = self._registry.get(hit.vector_id)
+            chunk = chunk_map.get(hit.vector_id)
             if chunk is None:
                 continue
             candidates.append(
                 FusionCandidate(source="dense", score=hit.score, chunk=chunk, rank=idx + 1)
             )
             scores[hit.vector_id] = hit.score
         return ChannelResults(candidates=candidates, scores=scores)

     def _filter_dense_hits(
         self,
         hits: Sequence[FaissSearchResult],
         filters: Mapping[str, object],
-    ) -> List[FaissSearchResult]:
+    ) -> tuple[List[FaissSearchResult], Dict[str, ChunkPayload]]:
+        if not hits:
+            return [], {}
+        chunk_map: Dict[str, ChunkPayload] = {
+            chunk.vector_id: chunk for chunk in self._registry.bulk_get([hit.vector_id for hit in hits])
+        }
         filtered: List[FaissSearchResult] = []
         for hit in hits:
-            chunk = self._registry.get(hit.vector_id)
+            chunk = chunk_map.get(hit.vector_id)
             if chunk is None:
                 continue
             if self._matches_filters(chunk, filters):
                 filtered.append(hit)
-        return filtered
+        return filtered, chunk_map

     def _matches_filters(self, chunk, filters: Mapping[str, object]) -> bool:
         for key, expected in filters.items():
             if key == "namespace":
                 if chunk.namespace != expected:
                     return False
                 continue
             value = chunk.metadata.get(key)
             if isinstance(expected, list):
                 if isinstance(value, list):
                     if not any(item in value for item in expected):
                         return False
                 else:
                     if value not in expected:
                         return False
             else:
                 if value != expected:
                     return False
         return True

     def _dedupe_candidates(
         self,
         candidates: Sequence[FusionCandidate],
         fused_scores: Mapping[str, float],
     ) -> List[FusionCandidate]:
diff --git a/src/DocsToKG/HybridSearch/similarity.py b/src/DocsToKG/HybridSearch/similarity.py
new file mode 100644
index 0000000000000000000000000000000000000000..38009f7cad955b7fa65922495a16769621bd40ff
--- /dev/null
+++ b/src/DocsToKG/HybridSearch/similarity.py
@@ -0,0 +1,82 @@
+"""Shared helpers for cosine similarity using FAISS GPU fallbacks."""
+from __future__ import annotations
+
+from typing import Optional
+
+import numpy as np
+
+try:  # pragma: no cover - exercised indirectly in GPU environments
+    import faiss  # type: ignore
+
+    _FAISS_AVAILABLE = hasattr(faiss, "pairwise_distance_gpu")
+except Exception:  # pragma: no cover - fallback for CPU-only test rigs
+    faiss = None  # type: ignore
+    _FAISS_AVAILABLE = False
+
+_GPU_RESOURCES: Optional["faiss.StandardGpuResources"] = None
+
+
+def normalize_rows(matrix: np.ndarray) -> np.ndarray:
+    """Return L2-normalised rows for cosine similarity operations."""
+
+    norms = np.linalg.norm(matrix, axis=1, keepdims=True)
+    norms[norms == 0.0] = 1.0
+    return matrix / norms
+
+
+def pairwise_inner_products(matrix: np.ndarray) -> np.ndarray:
+    """Compute pairwise inner products, using GPU helpers when available."""
+
+    if matrix.size == 0:
+        return np.zeros((0, 0), dtype=np.float32)
+    matrix = matrix.astype(np.float32, copy=False)
+    resources = _get_gpu_resources()
+    if resources is not None and _FAISS_AVAILABLE:
+        try:
+            sims = faiss.pairwise_distance_gpu(  # type: ignore[attr-defined]
+                resources,
+                matrix,
+                matrix,
+                metric=faiss.METRIC_INNER_PRODUCT,
+                device=0,
+            )
+            return np.asarray(sims, dtype=np.float32)
+        except Exception:  # pragma: no cover - GPU helper may fail if device busy
+            pass
+    return matrix @ matrix.T
+
+
+def max_inner_product(target: np.ndarray, corpus: np.ndarray) -> float:
+    """Return the maximum inner product between a target vector and corpus rows."""
+
+    if corpus.size == 0:
+        return 0.0
+    target = target.astype(np.float32, copy=False)
+    corpus = corpus.astype(np.float32, copy=False)
+    resources = _get_gpu_resources()
+    if resources is not None and _FAISS_AVAILABLE:
+        try:
+            sims = faiss.pairwise_distance_gpu(  # type: ignore[attr-defined]
+                resources,
+                target.reshape(1, -1),
+                corpus,
+                metric=faiss.METRIC_INNER_PRODUCT,
+                device=0,
+            )
+            return float(np.max(np.asarray(sims)))
+        except Exception:  # pragma: no cover - GPU helper may fail if device busy
+            pass
+    return float(np.max(target @ corpus.T))
+
+
+def _get_gpu_resources() -> Optional["faiss.StandardGpuResources"]:
+    global _GPU_RESOURCES
+    if not _FAISS_AVAILABLE or not hasattr(faiss, "StandardGpuResources"):
+        return None
+    if _GPU_RESOURCES is None:
+        try:  # pragma: no cover - GPU path depends on host environment
+            _GPU_RESOURCES = faiss.StandardGpuResources()
+        except Exception:
+            _GPU_RESOURCES = None
+    return _GPU_RESOURCES
+
diff --git a/src/DocsToKG/HybridSearch/storage.py b/src/DocsToKG/HybridSearch/storage.py
index 28893e652efbf6c4d642757d3a19003ff52b5c99..b96935c4097574ba9cc056dd8ab3ee9c7c0b76c4 100644
--- a/src/DocsToKG/HybridSearch/storage.py
+++ b/src/DocsToKG/HybridSearch/storage.py
@@ -1,141 +1,143 @@
 """In-memory storage simulators for OpenSearch and chunk registry."""
 from __future__ import annotations

 from dataclasses import dataclass
-from typing import TYPE_CHECKING, Dict, List, Mapping, Optional, Sequence
+from typing import TYPE_CHECKING, Callable, Dict, List, Mapping, Optional, Sequence

 import numpy as np

+from .ids import vector_uuid_to_faiss_int
 from .tokenization import tokenize
 from .types import ChunkPayload

 if TYPE_CHECKING:  # pragma: no cover - import guard for type checking only
     from .schema import OpenSearchIndexTemplate


 @dataclass(slots=True)
 class StoredChunk:
     """Internal representation of a chunk stored in the OpenSearch simulator."""

     payload: ChunkPayload


 class ChunkRegistry:
     """Durable mapping of `vector_id` → `ChunkPayload` for joins and reconciliation."""

     def __init__(self) -> None:
         self._chunks: Dict[str, ChunkPayload] = {}
+        self._bridge: Dict[int, str] = {}

     def upsert(self, chunks: Sequence[ChunkPayload]) -> None:
         for chunk in chunks:
             self._chunks[chunk.vector_id] = chunk
+            self._bridge[vector_uuid_to_faiss_int(chunk.vector_id)] = chunk.vector_id

     def delete(self, vector_ids: Sequence[str]) -> None:
         for vector_id in vector_ids:
             self._chunks.pop(vector_id, None)
+            self._bridge.pop(vector_uuid_to_faiss_int(vector_id), None)

     def get(self, vector_id: str) -> Optional[ChunkPayload]:
         return self._chunks.get(vector_id)

     def bulk_get(self, vector_ids: Sequence[str]) -> List[ChunkPayload]:
         return [self._chunks[vid] for vid in vector_ids if vid in self._chunks]

+    def resolve_faiss_id(self, internal_id: int) -> Optional[str]:
+        return self._bridge.get(internal_id)
+
     def all(self) -> List[ChunkPayload]:
         return list(self._chunks.values())

     def count(self) -> int:
         return len(self._chunks)

     def vector_ids(self) -> List[str]:
         return list(self._chunks.keys())


 class OpenSearchSimulator:
     """Subset of OpenSearch capabilities required for hybrid retrieval tests."""

     def __init__(self) -> None:
         self._chunks: Dict[str, StoredChunk] = {}
         self._avg_length: float = 0.0
         self._templates: Dict[str, "OpenSearchIndexTemplate"] = {}

     def bulk_upsert(self, chunks: Sequence[ChunkPayload]) -> None:
         for chunk in chunks:
             self._chunks[chunk.vector_id] = StoredChunk(chunk)
         self._recompute_avg_length()

     def bulk_delete(self, vector_ids: Sequence[str]) -> None:
         for vector_id in vector_ids:
             self._chunks.pop(vector_id, None)
         self._recompute_avg_length()

     def fetch(self, vector_ids: Sequence[str]) -> List[ChunkPayload]:
         return [self._chunks[vid].payload for vid in vector_ids if vid in self._chunks]

     def vector_ids(self) -> List[str]:
         return list(self._chunks.keys())

     def register_template(self, template: "OpenSearchIndexTemplate") -> None:
         self._templates[template.namespace] = template

     def template_for(self, namespace: str) -> Optional["OpenSearchIndexTemplate"]:
         return self._templates.get(namespace)

     def search_bm25(
         self,
         query_weights: Mapping[str, float],
         filters: Mapping[str, object],
         top_k: int,
         cursor: Optional[int] = None,
     ) -> tuple[List[tuple[ChunkPayload, float]], Optional[int]]:
-        candidates = self._filtered_chunks(filters)
-        scores: List[tuple[ChunkPayload, float]] = []
-        for chunk in candidates:
-            score = self._bm25_score(chunk, query_weights)
-            if score > 0.0:
-                scores.append((chunk.payload, score))
-        scores.sort(key=lambda item: item[1], reverse=True)
-        return self._paginate(scores, top_k, cursor)
+        return self._search_sparse(
+            lambda stored: self._bm25_score(stored, query_weights),
+            filters,
+            top_k,
+            cursor,
+        )

     def search_splade(
         self,
         query_weights: Mapping[str, float],
         filters: Mapping[str, object],
         top_k: int,
         cursor: Optional[int] = None,
     ) -> tuple[List[tuple[ChunkPayload, float]], Optional[int]]:
-        candidates = self._filtered_chunks(filters)
-        scores: List[tuple[ChunkPayload, float]] = []
-        for chunk in candidates:
+        def score_chunk(stored: StoredChunk) -> float:
             score = 0.0
             for token, weight in query_weights.items():
-                if token in chunk.payload.features.splade_weights:
-                    score += weight * chunk.payload.features.splade_weights[token]
-            if score > 0.0:
-                scores.append((chunk.payload, score))
-        scores.sort(key=lambda item: item[1], reverse=True)
-        return self._paginate(scores, top_k, cursor)
+                if token in stored.payload.features.splade_weights:
+                    score += weight * stored.payload.features.splade_weights[token]
+            return float(score)
+
+        return self._search_sparse(score_chunk, filters, top_k, cursor)

     def highlight(self, chunk: ChunkPayload, query_tokens: Sequence[str]) -> List[str]:
         highlights: List[str] = []
         lower_text = chunk.text.lower()
         for token in query_tokens:
             token_lower = token.lower()
             if token_lower in lower_text:
                 highlights.append(token)
         return highlights

     def _filtered_chunks(self, filters: Mapping[str, object]) -> List[StoredChunk]:
         results: List[StoredChunk] = []
         for stored in self._chunks.values():
             if self._matches_filters(stored.payload, filters):
                 results.append(stored)
         return results

     def _matches_filters(self, chunk: ChunkPayload, filters: Mapping[str, object]) -> bool:
         for key, expected in filters.items():
             if key == "namespace":
                 if chunk.namespace != expected:
                     return False
                 continue
             value = chunk.metadata.get(key)
             if isinstance(expected, list):
@@ -149,38 +151,54 @@ class OpenSearchSimulator:
                 if value != expected:
                     return False
         return True

     def _bm25_score(self, stored: StoredChunk, query_weights: Mapping[str, float]) -> float:
         score = 0.0
         for token, weight in query_weights.items():
             chunk_weight = stored.payload.features.bm25_terms.get(token)
             if chunk_weight is None:
                 continue
             score += weight * chunk_weight
         return float(score)

     def _paginate(
         self,
         scores: List[tuple[ChunkPayload, float]],
         top_k: int,
         cursor: Optional[int],
     ) -> tuple[List[tuple[ChunkPayload, float]], Optional[int]]:
         offset = cursor or 0
         end = offset + top_k
         page = scores[offset:end]
         next_cursor = end if end < len(scores) else None
         return page, next_cursor

+    def _search_sparse(
+        self,
+        scoring_fn: Callable[[StoredChunk], float],
+        filters: Mapping[str, object],
+        top_k: int,
+        cursor: Optional[int],
+    ) -> tuple[List[tuple[ChunkPayload, float]], Optional[int]]:
+        candidates = self._filtered_chunks(filters)
+        scored: List[tuple[ChunkPayload, float]] = []
+        for stored in candidates:
+            score = scoring_fn(stored)
+            if score > 0.0:
+                scored.append((stored.payload, float(score)))
+        scored.sort(key=lambda item: item[1], reverse=True)
+        return self._paginate(scored, top_k, cursor)
+
     def _recompute_avg_length(self) -> None:
         if not self._chunks:
             self._avg_length = 0.0
             return
         total_length = sum(chunk.payload.token_count for chunk in self._chunks.values())
         self._avg_length = total_length / len(self._chunks)

     def stats(self) -> Mapping[str, float]:
         return {
             "document_count": float(len(self._chunks)),
             "avg_token_length": float(self._avg_length),
         }

diff --git a/src/DocsToKG/HybridSearch/types.py b/src/DocsToKG/HybridSearch/types.py
index d4d6cfbf6c6ce5b35b1909172206818dfcf460ef..07048b2f8c10a24477dce941c6cc288ae6bfe1ea 100644
--- a/src/DocsToKG/HybridSearch/types.py
+++ b/src/DocsToKG/HybridSearch/types.py
@@ -104,50 +104,64 @@ class ChunkFeatures:
             dict(self.splade_weights),
             self.embedding.copy(),
         )


 @dataclass(slots=True)
 class ChunkPayload:
     """Fully materialized chunk stored in OpenSearch and FAISS.

     This class represents a document chunk that has been processed and indexed
     for both lexical and semantic search. It contains all the information
     needed for hybrid retrieval operations.

     Attributes:
         doc_id: Source document identifier
         chunk_id: Unique chunk identifier within the document
         vector_id: Corresponding vector identifier in FAISS
         namespace: Logical grouping for search scoping
         text: Original chunk text content
     metadata: MutableMapping[str, Any]
     features: ChunkFeatures
     token_count: int
     source_chunk_idxs: Sequence[int]
     doc_items_refs: Sequence[str]
     char_offset: Optional[Tuple[int, int]] = None
+    """
+
+    doc_id: str
+    chunk_id: str
+    vector_id: str
+    namespace: str
+    text: str
+    metadata: MutableMapping[str, Any]
+    features: ChunkFeatures
+    token_count: int
+    source_chunk_idxs: Sequence[int]
+    doc_items_refs: Sequence[str]
+    char_offset: Optional[Tuple[int, int]] = None
+


 @dataclass(slots=True)
 class HybridSearchDiagnostics:
     """Per-channel diagnostics for a hybrid search result.

     This class provides detailed scoring information for each retrieval
     method (BM25, SPLADE, dense vectors) used in hybrid search, enabling
     analysis of individual method performance and result quality.

     Attributes:
         bm25_score: BM25 lexical similarity score (None if not used)
         splade_score: SPLADE sparse embedding score (None if not used)
         dense_score: Dense vector similarity score (None if not used)

     Examples:
         >>> diagnostics = HybridSearchDiagnostics(
         ...     bm25_score=0.85,
         ...     splade_score=0.92,
         ...     dense_score=0.78
         ... )
     """
     bm25_score: Optional[float] = None
     splade_score: Optional[float] = None
     dense_score: Optional[float] = None
diff --git a/tests/test_hybrid_search.py b/tests/test_hybrid_search.py
index 2aee8ad90c8780bcc252e9bd1407eb4233f23b67..f74e275a23ec61cee831a4afc5d0ce6f7277944a 100644
--- a/tests/test_hybrid_search.py
+++ b/tests/test_hybrid_search.py
@@ -1,57 +1,60 @@
 from __future__ import annotations

 import json
 from http import HTTPStatus
 from pathlib import Path
 from typing import Callable, List, Mapping, Sequence

+import numpy as np
 import pytest

 from DocsToKG.HybridSearch import (
     ChunkIngestionPipeline,
     HybridSearchAPI,
     DocumentInput,
     FeatureGenerator,
     HybridSearchConfigManager,
     HybridSearchRequest,
     HybridSearchService,
     HybridSearchValidator,
     Observability,
     OpenSearchSchemaManager,
     build_stats_snapshot,
     restore_state,
     serialize_state,
     should_rebuild_index,
     verify_pagination,
 )
+from DocsToKG.HybridSearch.config import DenseIndexConfig
 from DocsToKG.HybridSearch.dense import FaissIndexManager
 from DocsToKG.HybridSearch.storage import ChunkRegistry, OpenSearchSimulator
 from DocsToKG.HybridSearch.validation import load_dataset
 from DocsToKG.HybridSearch.tokenization import tokenize
 from DocsToKG.HybridSearch.ingest import IngestError
-from uuid import NAMESPACE_URL, uuid5
+from DocsToKG.HybridSearch.types import ChunkFeatures, ChunkPayload
+from uuid import NAMESPACE_URL, uuid5, uuid4


 def _build_config(tmp_path: Path) -> HybridSearchConfigManager:
     config_payload = {
         "dense": {"index_type": "flat", "oversample": 3},
         "fusion": {"k0": 50.0, "mmr_lambda": 0.7, "cosine_dedupe_threshold": 0.95, "max_chunks_per_doc": 2},
         "retrieval": {"bm25_top_k": 20, "splade_top_k": 20, "dense_top_k": 20},
     }
     path = tmp_path / "hybrid_config.json"
     path.write_text(json.dumps(config_payload), encoding="utf-8")
     return HybridSearchConfigManager(path)


 @pytest.fixture
 def dataset() -> Sequence[Mapping[str, object]]:
     return load_dataset(Path("tests/data/hybrid_dataset.jsonl"))


 @pytest.fixture
 def stack(
     tmp_path: Path,
 ) -> Callable[[], tuple[
     ChunkIngestionPipeline,
     HybridSearchService,
     ChunkRegistry,
@@ -312,25 +315,55 @@ def test_operations_snapshot_and_restore_roundtrip(
     assert not should_rebuild_index(registry, deleted_since_snapshot=0, threshold=0.5)
     assert should_rebuild_index(registry, deleted_since_snapshot=max(1, registry.count() // 2), threshold=0.25)


 def test_ingest_missing_vector_raises(
     stack: Callable[[], tuple[ChunkIngestionPipeline, HybridSearchService, ChunkRegistry, HybridSearchValidator, FeatureGenerator, OpenSearchSimulator]],
     tmp_path: Path,
 ) -> None:
     ingestion, _, _, _, feature_generator, _ = stack()
     artifacts_dir = tmp_path / "docs"
     doc = _write_document_artifacts(
         artifacts_dir,
         doc_id="doc-missing",
         namespace="research",
         text="Chunk without matching vector entry",
         metadata={},
         feature_generator=feature_generator,
     )
     vector_entries = [json.loads(line) for line in doc.vector_path.read_text(encoding="utf-8").splitlines() if line.strip()]
     vector_entries[0]["UUID"] = "00000000-0000-0000-0000-000000000000"
     doc.vector_path.write_text("\n".join(json.dumps(entry) for entry in vector_entries) + "\n", encoding="utf-8")

     with pytest.raises(IngestError):
         ingestion.upsert_documents([doc])

+
+def test_faiss_index_uses_registry_bridge(tmp_path: Path) -> None:
+    pytest.importorskip("faiss")
+    config = DenseIndexConfig(index_type="flat")
+    manager = FaissIndexManager(dim=4, config=config)
+    registry = ChunkRegistry()
+    manager.set_id_resolver(registry.resolve_faiss_id)
+
+    embedding = np.array([1.0, 0.0, 0.0, 0.0], dtype=np.float32)
+    features = ChunkFeatures(bm25_terms={}, splade_weights={}, embedding=embedding)
+    chunk = ChunkPayload(
+        doc_id="doc-bridge",
+        chunk_id="0",
+        vector_id=str(uuid4()),
+        namespace="bridge",
+        text="example chunk",
+        metadata={},
+        features=features,
+        token_count=int(embedding.size),
+        source_chunk_idxs=(0,),
+        doc_items_refs=(),
+        char_offset=(0, len("example chunk")),
+    )
+
+    manager.add([features.embedding], [chunk.vector_id])
+    registry.upsert([chunk])
+
+    hits = manager.search(embedding, 1)
+    assert hits and hits[0].vector_id == chunk.vector_id
+
diff --git a/tests/test_hybrid_search_real_vectors.py b/tests/test_hybrid_search_real_vectors.py
index 7bea472fce42fdd4b49ce0d541a267df07ed94f7..2fa4352e9d3eaa84f014a4feca5253249bdb317e 100644
--- a/tests/test_hybrid_search_real_vectors.py
+++ b/tests/test_hybrid_search_real_vectors.py
@@ -213,57 +213,58 @@ def test_real_fixture_reingest_and_reports(


 def test_real_fixture_api_roundtrip(
     stack: Callable[[], tuple[ChunkIngestionPipeline, HybridSearchService, ChunkRegistry, HybridSearchValidator, FaissIndexManager, OpenSearchSimulator]],
     real_dataset: Sequence[Mapping[str, object]],
 ) -> None:
     ingestion, service, registry, _, _, _ = stack()
     documents = _to_documents(real_dataset)
     ingestion.upsert_documents(documents)
     api = HybridSearchAPI(service)
     first_entry = real_dataset[0]
     query = first_entry["queries"][0]
     status, body = api.post_hybrid_search(
         {
             "query": query["query"],
             "namespace": query["namespace"],
             "page_size": 3,
         }
     )
     assert status == HTTPStatus.OK
     assert body["results"], "Expected API to return results"
     assert body["results"][0]["doc_id"] == query["expected_doc_id"]


 def test_remove_ids_cpu_fallback(monkeypatch: pytest.MonkeyPatch) -> None:
+    pytest.importorskip("faiss")
     manager = FaissIndexManager(dim=8, config=DenseIndexConfig())
     manager._use_native = True
-    manager._id_lookup = {1: "chunk-1", 2: "chunk-2"}

     class FailingIndex:
         def __init__(self) -> None:
             self.ntotal = 2

         def remove_ids(self, selector: object) -> None:  # pragma: no cover - forced failure
             raise RuntimeError("remove_ids not implemented for this type of index")

     class CPUIndex:
         def __init__(self) -> None:
             self.ntotal = 2
             self.removed = 0

         def remove_ids(self, selector: object) -> None:
             self.removed += 1
             self.ntotal = 0

     failing_index = FailingIndex()
     cpu_index = CPUIndex()

     monkeypatch.setattr(manager, "_index", failing_index, raising=False)
     monkeypatch.setattr(manager, "_to_cpu", lambda index: cpu_index)
     monkeypatch.setattr(manager, "_maybe_to_gpu", lambda index: index)
+    monkeypatch.setattr(manager, "_apply_search_parameters", lambda index: None)

     manager._remove_ids(np.array([1, 2], dtype=np.int64))
     assert manager._remove_fallbacks == 1
     assert cpu_index.removed == 1
     assert manager._index is cpu_index
     assert manager.ntotal == cpu_index.ntotal

EOF
)
