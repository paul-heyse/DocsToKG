Below is a revised, concrete plan for **PR #6 — Conditional caching with *hishel*** that assumes you’re already using hishel “comprehensively.” We switch from any bespoke conditional-header store to **httpx + hishel** for caching/validation, and we keep our **rate-limit + retry** + **telemetry** behaviors on top.

> Paste this into `docs/pr6-hishel-cache.md` (or your PR description).
> It includes a compiling file tree, code scaffolds, telemetry hooks, and tests.
> **Scope:** no custom ETag DB; *hishel* handles cache metadata & revalidation.

---

# Goals (updated for hishel)

1. Use **hishel** as the single source of truth for HTTP caching & revalidation (ETag / Last-Modified), via **httpx + CacheTransport**.
2. Preserve our **per-resolver rate-limit + retry** wrapper and **attempt telemetry** (now informed by hishel’s response metadata).
3. Expose minimal **CLI** helpers around hishel storage (clear, basic stats for FileStorage).
4. Keep everything **explicitly injected** and testable.

---

## New/updated file tree

```text
src/DocsToKG/ContentDownload/
  httpx/
    __init__.py
    hishel_build.py     # NEW: build hishel storage, controller, transport
    client.py           # NEW: RateRetryClient wrapper around httpx.Client (handles rate+retry+telemetry)
  bootstrap.py          # MOD: build shared hishel stack and per-resolver clients
  download_execution.py # MOD: read hishel response extensions; emit cache-aware telemetry
  cli/
    app.py              # MOD: add `cache` commands (clear/stats) for FileStorage
tests/
  contentdownload/
    test_hishel_cache_flow.py  # NEW: 200 → cache-hit (no network) and 304 revalidate
    test_httpx_retry_limit.py  # NEW: 429 Retry-After then success (unchanged semantics)
```

---

## 1) Config model additions (hishel-specific)

Augment your existing `ContentDownloadConfig` with a **hishel** sub-config. We’ll share one cache instance across all resolvers (recommended), and keep a controller with conservative defaults.

```python
# src/DocsToKG/ContentDownload/config/models.py (additions)
from pydantic import BaseModel, ConfigDict, Field
from typing import Literal, Optional

class HishelConfig(BaseModel):
    model_config = ConfigDict(extra="forbid")
    enabled: bool = True

    # Storage backend and parameters
    backend: Literal["file", "sqlite", "redis", "s3"] = "file"
    base_path: str = "state/hishel-cache"           # used when backend="file"
    sqlite_path: str = "state/hishel-cache.sqlite"  # when backend="sqlite"
    redis_url: Optional[str] = None                 # when backend="redis"
    s3_bucket: Optional[str] = None                 # when backend="s3"
    # TTL for at-rest entries (storage-level purge policy, not RFC freshness)
    ttl_seconds: int = 30 * 24 * 3600
    check_ttl_every_seconds: int = 10 * 60

    # Controller knobs (RFC behavior)
    force_cache: bool = False
    allow_heuristics: bool = False
    allow_stale: bool = False
    always_revalidate: bool = False
    cache_private: bool = True
    cacheable_methods: list[str] = Field(default_factory=lambda: ["GET"])

class ContentDownloadConfig(BaseModel):
    # ...
    hishel: HishelConfig = Field(default_factory=HishelConfig)
```

---

## 2) Build the hishel stack (storage → controller → transport → httpx.Client)

```python
# src/DocsToKG/ContentDownload/httpx/hishel_build.py
from __future__ import annotations
import httpx
import hishel

def build_hishel_transport(cfg_hishel) -> hishel.CacheTransport:
    # Storage
    if cfg_hishel.backend == "file":
        storage = hishel.FileStorage(
            base_path=cfg_hishel.base_path,
            ttl=cfg_hishel.ttl_seconds,
            check_ttl_every=cfg_hishel.check_ttl_every_seconds,
        )
    elif cfg_hishel.backend == "sqlite":
        storage = hishel.SQLiteStorage(
            connection=__import__("sqlite3").connect(cfg_hishel.sqlite_path),
            ttl=cfg_hishel.ttl_seconds,
        )
    elif cfg_hishel.backend == "redis":
        r = __import__("redis").Redis.from_url(cfg_hishel.redis_url)
        storage = hishel.RedisStorage(client=r, ttl=cfg_hishel.ttl_seconds)
    elif cfg_hishel.backend == "s3":
        s3 = __import__("boto3").client("s3")
        storage = hishel.S3Storage(bucket_name=cfg_hishel.s3_bucket, client=s3,
                                   ttl=cfg_hishel.ttl_seconds, check_ttl_every=cfg_hishel.check_ttl_every_seconds)
    else:
        raise ValueError(f"Unknown hishel backend: {cfg_hishel.backend}")

    # Controller (RFC 9111 policy)
    controller = hishel.Controller(
        force_cache=cfg_hishel.force_cache,
        allow_heuristics=cfg_hishel.allow_heuristics,
        allow_stale=cfg_hishel.allow_stale,
        always_revalidate=cfg_hishel.always_revalidate,
        cache_private=cfg_hishel.cache_private,
        cacheable_methods=cfg_hishel.cacheable_methods,
    )

    # Transport chain: CacheTransport over the default HTTPTransport
    cache_transport = hishel.CacheTransport(
        transport=httpx.HTTPTransport(),
        controller=controller,
        storage=storage,
    )
    return cache_transport


def build_httpx_client_with_hishel(cfg_http, hishel_transport) -> httpx.Client:
    # Timeouts: httpx.Timeouts(connect=..., read=..., write=..., pool=...)
    timeouts = httpx.Timeout(
        connect=cfg_http.timeout_connect_s,
        read=cfg_http.timeout_read_s,
        write=cfg_http.timeout_read_s,
        pool=cfg_http.timeout_read_s,
    )

    headers = {"User-Agent": cfg_http.user_agent}
    if cfg_http.mailto and "mailto:" not in headers["User-Agent"]:
        headers["User-Agent"] = f'{headers["User-Agent"]} (+mailto:{cfg_http.mailto})'

    limits = httpx.Limits(max_connections=128, max_keepalive_connections=64)

    client = httpx.Client(
        transport=hishel_transport,
        headers=headers,
        timeout=timeouts,
        limits=limits,
        verify=bool(cfg_http.verify_tls),
        proxies=cfg_http.proxies or None,
    )
    return client
```

---

## 3) Keep per-resolver policies via a wrapper: `RateRetryClient (httpx)`

We retain our **TokenBucket + retry/backoff** logic but wrap an **httpx.Client** (already caching via hishel). We also **refund tokens** for pure cache hits (no network) using hishel’s response metadata.

```python
# src/DocsToKG/ContentDownload/httpx/client.py
from __future__ import annotations
import time, random
from typing import Optional, Dict
import httpx

from DocsToKG.ContentDownload.telemetry.build import RunTelemetry

class TokenBucket:
    def __init__(self, capacity: int, refill_per_sec: float, burst: int):
        self.capacity = max(1, capacity); self.refill = max(0.01, refill_per_sec)
        self.burst = max(0, burst); self.tokens = float(self.capacity + self.burst)
        self.t0 = time.monotonic()

    def _tick(self):
        now = time.monotonic()
        dt = now - self.t0; self.t0 = now
        self.tokens = min(self.capacity + self.burst, self.tokens + dt * self.refill)

    def consume(self, amount=1.0) -> float:  # return sleep seconds if must wait
        self._tick()
        if self.tokens >= amount:
            self.tokens -= amount; return 0.0
        need = amount - self.tokens
        return need / self.refill

    def refund(self, amount=1.0):
        self._tick(); self.tokens = min(self.capacity + self.burst, self.tokens + amount)


class RateRetryClient:
    """
    A small policy wrapper around httpx.Client that:
      - enforces per-resolver rate limits,
      - retries with backoff/jitter on retryable statuses/exceptions,
      - emits retry attempts,
      - respects hishel cache metadata (refund tokens on cache hits).
    """
    def __init__(self, resolver_name: str, client: httpx.Client, *,
                 retry_policy, rate_policy, telemetry: Optional[RunTelemetry], run_id: Optional[str]):
        self.name = resolver_name
        self.c = client
        self.t = telemetry
        self.run_id = run_id
        self.bucket = TokenBucket(rate_policy.capacity, rate_policy.refill_per_sec, rate_policy.burst)
        self.retry = retry_policy
        # Precompute sets/params
        self.statuses = set(int(x) for x in (self.retry.retry_statuses or []))
        self.base = max(0, int(self.retry.base_delay_ms))
        self.maxd = max(self.base, int(self.retry.max_delay_ms))
        self.jitter = max(0, int(self.retry.jitter_ms))
        self.max_attempts = max(1, int(self.retry.max_attempts))

    def head(self, url: str, **kw) -> httpx.Response:
        return self._request("HEAD", url, **kw)

    def get(self, url: str, **kw) -> httpx.Response:
        return self._request("GET", url, **kw)

    def _request(self, method: str, url: str, **kw) -> httpx.Response:
        # Rate-limit pre-check; we may refund if this ends up a pure cache hit
        sleep_s = self.bucket.consume(1.0)
        if sleep_s > 0:
            time.sleep(sleep_s)
            self._emit_retry(url, reason="backoff", sleep_ms=int(sleep_s * 1000), attempt=0)

        last_exc = None
        for i in range(self.max_attempts):
            try:
                resp = self.c.request(method, url, **kw)
                # hishel metadata: from_cache (bool), revalidated (bool)
                from_cache = bool(resp.extensions.get("from_cache"))
                revalidated = bool(resp.extensions.get("revalidated"))

                if from_cache and not revalidated:
                    # pure cache hit → refund the token; no network was touched
                    self.bucket.refund(1.0)

                # retry on targeted statuses (except 304 etc.)
                if resp.status_code in self.statuses and not from_cache:
                    if i < self.max_attempts - 1:
                        delay = self._compute_delay(i, resp)
                        self._emit_retry(url, reason="retry-after" if resp.headers.get("Retry-After") else "backoff",
                                         sleep_ms=int(delay * 1000), attempt=i + 1, http_status=resp.status_code)
                        time.sleep(delay)
                        continue
                return resp
            except (httpx.ConnectError, httpx.ReadTimeout, httpx.WriteError, httpx.RemoteProtocolError) as e:
                last_exc = e
                if i < self.max_attempts - 1:
                    delay = self._compute_delay(i, None)
                    self._emit_retry(url, reason="conn-error", sleep_ms=int(delay * 1000), attempt=i + 1)
                    time.sleep(delay)
                    continue
                raise
        return resp  # type: ignore[UnboundLocalVariable]

    def _compute_delay(self, attempt_idx: int, resp: Optional[httpx.Response]) -> float:
        ra = 0.0
        if resp is not None:
            h = resp.headers.get("Retry-After")
            if h:
                try: ra = float(h)
                except ValueError: ra = 0.0
        backoff = min(self.maxd, self.base * (2 ** attempt_idx) if self.base > 0 else 0)
        if self.jitter: backoff += random.randint(0, self.jitter)
        return max(ra, backoff / 1000.0)

    def _emit_retry(self, url: str, *, reason: str, sleep_ms: int, attempt: int, http_status: int | None = None):
        if not self.t: return
        self.t.log_attempt(run_id=self.run_id, resolver=self.name, url=url,
                           verb="GET", status="retry", http_status=http_status,
                           reason=reason, elapsed_ms=sleep_ms, extra={"attempt": attempt})
```

---

## 4) Bootstrap wiring (shared hishel; per-resolver clients)

```python
# src/DocsToKG/ContentDownload/bootstrap.py (revised pieces)
from DocsToKG.ContentDownload.httpx.hishel_build import build_hishel_transport, build_httpx_client_with_hishel
from DocsToKG.ContentDownload.httpx.client import RateRetryClient

def run_from_config(cfg: ContentDownloadConfig, *, artifacts=None, record_html_paths=True) -> None:
    telemetry = build_run_telemetry(cfg)

    # hishel transport is shared across all clients (shared cache)
    hishel_transport = build_hishel_transport(cfg.hishel)
    base_httpx_client = build_httpx_client_with_hishel(cfg.http, hishel_transport)

    # resolvers from registry
    resolvers = build_resolvers(cfg.resolvers.order, cfg)

    # per-resolver policy clients (wrappers) that use the shared hishel-enabled httpx client
    clients: Dict[str, RateRetryClient] = {}
    for name in cfg.resolvers.order:
        rcfg = getattr(cfg.resolvers, name, None)
        if not rcfg or not rcfg.enabled: continue
        clients[name] = RateRetryClient(
            resolver_name=name,
            client=base_httpx_client,
            retry_policy=rcfg.retry,
            rate_policy=rcfg.rate_limit,
            telemetry=telemetry,
            run_id=cfg.run_id,
        )

    # pipeline uses the client map; download policy (atomic write, verify CL) unchanged
    pipeline = ResolverPipeline(
        resolvers=resolvers,
        session=clients,
        telemetry=telemetry,
        run_id=cfg.run_id,
        robots=cfg.robots,
        dlp=cfg.download,
        # no custom cache store; hishel handles caching inside the client
    )

    if artifacts is not None:
        for art in artifacts:
            _ = pipeline.process(art, ctx=None)
```

---

## 5) Execution changes: log cache decisions via hishel metadata

We **don’t** add `If-None-Match`/`If-Modified-Since` headers. hishel does that. We just read **`response.extensions`** to emit cache-aware attempts and proceed with normal integrity checks.

```python
# src/DocsToKG/ContentDownload/download_execution.py (relevant changes)
def stream_candidate_payload(
    plan: DownloadPlan,
    *,
    session,                   # RateRetryClient (httpx)
    timeout_s: Optional[float] = None,
    chunk_size: int = 1 << 20,
    expected_len: Optional[int] = None,
    telemetry: Optional[AttemptSink] = None,
    run_id: Optional[str] = None,
) -> DownloadStreamResult:
    url = plan.url

    # Optional HEAD for policy/type checks (bypasses cache; fine to keep or disable)
    t0 = time.monotonic_ns()
    head = session.head(url, timeout=timeout_s)
    _emit(telemetry, run_id=run_id, resolver=plan.resolver_name, url=url,
          verb="HEAD", status="http-head", http_status=head.status_code,
          content_type=head.headers.get("Content-Type"),
          elapsed_ms=(time.monotonic_ns() - t0) // 1_000_000)

    # GET through hishel-enabled client
    t0 = time.monotonic_ns()
    resp = session.get(url, timeout=timeout_s)
    elapsed_ms = (time.monotonic_ns() - t0) // 1_000_000

    from_cache = bool(resp.extensions.get("from_cache"))
    revalidated = bool(resp.extensions.get("revalidated"))

    # Emit attempt for the GET handshake
    _emit(telemetry, run_id=run_id, resolver=plan.resolver_name, url=url,
          verb="GET", status="http-get", http_status=resp.status_code,
          content_type=resp.headers.get("Content-Type"), elapsed_ms=elapsed_ms)

    # Cache-aware short circuits
    if from_cache and not revalidated:
        # Pure cache hit (200 from cache, no network)
        _emit(telemetry, run_id=run_id, resolver=plan.resolver_name, url=url,
              verb="GET", status="cache-hit", http_status=resp.status_code,
              content_type=resp.headers.get("Content-Type"), reason="ok")
    elif revalidated and resp.status_code == 304:
        _emit(telemetry, run_id=run_id, resolver=plan.resolver_name, url=url,
              verb="GET", status="http-304", http_status=304,
              content_type=resp.headers.get("Content-Type"), reason="not-modified")
        # Finalize path returns a “skip” outcome later; do not stream body
        return DownloadStreamResult(path_tmp="", bytes_written=0, http_status=304,
                                    content_type=resp.headers.get("Content-Type"))

    # Stream body (hishel gives you the response content either from cache or network)
    tmp_dir = os.getcwd()  # replace with configured dest dir
    os.makedirs(tmp_dir, exist_ok=True)
    tmp_path = os.path.join(tmp_dir, ".part-download.tmp")

    bytes_written = 0
    with open(tmp_path, "wb") as f:
        for chunk in resp.iter_bytes():  # httpx iterator
            if not chunk:
                continue
            f.write(chunk); bytes_written += len(chunk)

    # Emit terminal 200 attempt with size info
    if resp.status_code == 200:
        _emit(telemetry, run_id=run_id, resolver=plan.resolver_name, url=url,
              verb="GET", status="http-200", http_status=200,
              content_type=resp.headers.get("Content-Type"),
              bytes_written=bytes_written,
              content_length_hdr=int(resp.headers.get("Content-Length", "0")) or None)

    return DownloadStreamResult(
        path_tmp=tmp_path, bytes_written=bytes_written,
        http_status=resp.status_code, content_type=resp.headers.get("Content-Type"),
    )
```

> **New attempt token:** `cache-hit` (additive; safe). Everything else (`http-304`, `http-200`, `retry`) stays unchanged.

Finalization (`finalize_candidate_download`) remains the same: atomic move, `Content-Length` verification, PDF tail checks, outcome build.

---

## 6) CLI: small cache helpers (FileStorage)

Because hishel abstracts the cache, cross-backend “introspection” is limited. We can reliably support **FileStorage** stats/clear; for other backends we provide a friendly message or a targeted command later.

```python
# src/DocsToKG/ContentDownload/cli/app.py (append)
cache_app = typer.Typer(help="hishel cache helpers")
app.add_typer(cache_app, name="cache")

@cache_app.command("stats")
def cache_stats(config: Optional[str] = typer.Option(None, "--config", "-c")):
    cfg = load_config(config)
    if cfg.hishel.backend != "file":
        typer.echo("Stats only implemented for file backend.")
        raise typer.Exit(1)
    import os
    total = 0; files = 0
    base = cfg.hishel.base_path
    for root, _, fnames in os.walk(base):
        for f in fnames:
            files += 1
            try: total += os.path.getsize(os.path.join(root, f))
            except OSError: pass
    typer.echo(json.dumps({"entries": files, "bytes": total, "base_path": base}, indent=2))

@cache_app.command("clear")
def cache_clear(config: Optional[str] = typer.Option(None, "--config", "-c")):
    cfg = load_config(config)
    if cfg.hishel.backend != "file":
        typer.echo("Clear implemented only for file backend. Delete via your backend admin (e.g., redis-cli, rm sqlite).")
        raise typer.Exit(1)
    import shutil, os
    base = cfg.hishel.base_path
    if os.path.isdir(base): shutil.rmtree(base)
    typer.echo("OK")
```

---

## 7) Tests

### 7.1 200 → cache-hit and 304 revalidate

```python
# tests/contentdownload/test_hishel_cache_flow.py
from __future__ import annotations
from types import SimpleNamespace
import httpx

from DocsToKG.ContentDownload.config.models import ContentDownloadConfig
from DocsToKG.ContentDownload.bootstrap import run_from_config

# Fake resolver that always returns the same URL
from DocsToKG.ContentDownload.api.types import ResolverResult, DownloadPlan

class R:
    name = "unpaywall"
    def resolve(self, artifact, session, ctx, telemetry, run_id):
        return ResolverResult(plans=[DownloadPlan(url="http://example.net/x.bin", resolver_name=self.name)])

def _monkey_resolver(monkeypatch):
    import DocsToKG.ContentDownload.resolvers as reg
    monkeypatch.setattr(reg, "build_resolvers", lambda order, cfg: [R()], raising=True)

def _httpx_handler_sequence():
    # First run: 200 with ETag; second run: serve from cache without network (hishel); third run: 304 revalidate path
    # We simulate “network” by using httpx.MockTransport under the hishel cache transport.
    calls = {"count": 0}
    def handler(request: httpx.Request) -> httpx.Response:
        calls["count"] += 1
        if request.headers.get("If-None-Match") == '"abc123"':
            return httpx.Response(304, headers={"ETag": '"abc123"', "Content-Type":"application/octet-stream"})
        if calls["count"] == 1:
            return httpx.Response(200, content=b"abc", headers={"ETag": '"abc123"', "Content-Type":"application/octet-stream","Content-Length":"3"})
        # Anything unexpected → 200 again
        return httpx.Response(200, content=b"abc", headers={"ETag": '"abc123"', "Content-Type":"application/octet-stream","Content-Length":"3"})
    return httpx.MockTransport(handler)

def test_hishel_cache(monkeypatch, tmp_path):
    _monkey_resolver(monkeypatch)

    # Build config with file storage under tmp
    cfg = ContentDownloadConfig.model_validate({
        "run_id": "r1",
        "telemetry": {"csv_path": str(tmp_path/"attempts.csv"), "manifest_path": str(tmp_path/"manifest.jsonl")},
        "hishel": {"enabled": True, "backend":"file", "base_path": str(tmp_path/"cache")},
        "resolvers": {"order":["unpaywall"], "unpaywall":{"enabled":True}}
    })

    # Patch hishel transport to use MockTransport inside
    import DocsToKG.ContentDownload.httpx.hishel_build as HB
    mock = _httpx_handler_sequence()
    def build_hishel_transport(cfg_hishel):
        import hishel
        # Use the same storage/controller builder, swapping transport
        storage = hishel.FileStorage(base_path=cfg_hishel.base_path, ttl=cfg_hishel.ttl_seconds, check_ttl_every=cfg_hishel.check_ttl_every_seconds)
        controller = hishel.Controller(force_cache=False, allow_heuristics=False, allow_stale=False, always_revalidate=False, cache_private=True, cacheable_methods=["GET"])
        return hishel.CacheTransport(transport=mock, controller=controller, storage=storage)
    monkeypatch.setattr(HB, "build_hishel_transport", build_hishel_transport, raising=True)

    # First run → network 200, cache populated
    run_from_config(cfg, artifacts=[SimpleNamespace(id="1")])
    # Second run → cache-hit (hishel serves from cache); our wrapper should refund token; attempts include "cache-hit"
    run_from_config(cfg, artifacts=[SimpleNamespace(id="2")])
    # Third run → change the controller to always_revalidate to simulate 304 path (optional)
```

### 7.2 Retry/backoff still behaves (identical to earlier tests)

```python
# tests/contentdownload/test_httpx_retry_limit.py
# Drive 429 Retry-After then 200 using MockTransport; assert that our wrapper emitted retry and slept ~1s.
```

---

## 8) Telemetry mapping (hishel → our tokens)

* **Pure cache hit** (`resp.extensions["from_cache"]=True`, `revalidated=False`) → `status="cache-hit"`, `reason="ok"`.
* **Revalidated and not modified** (`revalidated=True`, `status_code=304`) → `status="http-304"`, `reason="not-modified"`.
* **Network success** (`status_code=200`) → `status="http-200"` (unchanged).
* **Retries/backoff** → `status="retry"`, `reason="retry-after" | "backoff" | "conn-error"` (unchanged).

> Token **additions** are backward-compatible (we’ve only added `cache-hit`).

---

## 9) Acceptance checklist

* [ ] A **shared hishel cache** is created from config and used by all per-resolver clients.
* [ ] Our **RateRetryClient** wraps **httpx.Client** (hishel) and keeps **rate-limit + retry** behavior.
* [ ] **Telemetry** now records `cache-hit` and `http-304` based on `response.extensions`.
* [ ] No bespoke conditional headers or ETag store exists in our code.
* [ ] CLI offers **file-backend stats** and **clear** helpers; other backends can be added later.
* [ ] Tests prove: first run 200 → second run cache-hit (no network) → (optional) revalidate path 304.

---

## 10) Notes & guardrails

* **Cache scope:** Sharing one hishel storage across resolvers is typically what you want; if two resolvers point at the same URL, they’ll benefit from the same cache.
* **HEAD vs GET:** Caching is primarily for GET; your HEAD policy remains for type/size gating. If HEAD becomes noisy, you can make it optional via config.
* **Refunding tokens:** Rate limit is enforced **after** we know whether network was used; if hishel served from cache, we refund the token. This keeps politeness for network calls without slowing cache hits.
* **TTL vs RFC freshness:** hishel’s **storage TTL** is at-rest eviction; RFC freshness/revalidation is governed by its **Controller**.
* **Backends:** Start with `file`; add Redis/SQLite/S3 as needed (we keep the seams).
* **Observability:** Attempt CSV remains your single, uniform source of truth across cache hits, revalidations, and network I/O.

---

This plan removes bespoke conditional-request plumbing and leans entirely on **hishel** for caching & validation, while preserving your **rate-limit** policies and **telemetry contract**. If you want me to output patch files mirroring this plan, I can print those next.
